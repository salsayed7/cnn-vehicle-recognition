{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "297f060f-edd1-407f-afad-e08e41b8e5b9",
   "metadata": {},
   "source": [
    "# Install the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342fe1e3-3352-4858-ba74-bc7859dcbebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install opencv-python torch torchvision ultralytics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8a4079-da31-4f1a-ac33-7f25b040b062",
   "metadata": {},
   "source": [
    "# Import the most important libraries used for the implementation through the whole code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c3edbd-44c3-449c-a862-b18b73b1cd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the library that allows us to read and save graphics files and apply filters and effects to them, including resize, grayscale, rotation and many other variations. (Installation: pip install opencv-python)\n",
    "import cv2\n",
    "\n",
    "# A Jupyter kernel to work with Python code in Jupyter notebooks and other interactive frontends\n",
    "import IPython\n",
    "\n",
    "# For more easier implementation\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# This is an image formatting library that deals with images as matrices and converts them from and back to images\n",
    "from PIL import Image\n",
    "\n",
    "# This library prints errors as trees\n",
    "import traceback\n",
    "\n",
    "# This is the Pytorch import (Installation: pip install torch)\n",
    "import torch\n",
    "\n",
    "# For common mathematical functions like \"floor\"\n",
    "import math\n",
    "\n",
    "# For measuring inference time\n",
    "import time\n",
    "\n",
    "# Imports all other pre-trained detection models needed along with their weights (COCO dataset with 80 classes); they will be automatically downloaded if they are not available (Installation: pip install torchvision)\n",
    "from torchvision.models.detection import *\n",
    "\n",
    "# For processing Pytorch detection models\n",
    "from torchvision.io.image import read_image\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Imports all pre-trained YOLOv5, YOLOv8, YOLOv9 and in the future YOLOv10 models (COCO dataset with 80 classes); they will be automatically downloaded if not available (Installation: pip install ultralytics)\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6d80b6-a4ba-4f6e-8e0f-35862c51f18c",
   "metadata": {},
   "source": [
    "# Determining first priority for GPU for better performance if available; otherwise, settle for CPU. You can import the models and their weights from either Ultralytics for YOLO or from Pytorch for other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa7120e-c465-428a-845e-59ac3958ef37",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b317f15-e74f-43da-b43e-8fcc6cc710c9",
   "metadata": {},
   "source": [
    "# Use this only when you are using any model from Pytorch, it transforms input image to tensor, there is a commented line that resizes the input image into 300x300 before processing but it is not needed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7385e23-2fbd-4fbd-b2f0-d4a5cc0016af",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((300, 300)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4a908d-a344-4be8-9f26-73eeedc3556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we can limit the classes we test to vehicle related ones\n",
    "target_classes = [\"truck\", \"bus\", \"car\", \"train\", \"bicycle\"]\n",
    "\n",
    "# Here we can import and initialize a detection model from \"torchvision.models.detection\"\n",
    "WEIGHTS = FasterRCNN_ResNet50_FPN_Weights\n",
    "MODEL = fasterrcnn_resnet50_fpn(weights=WEIGHTS.DEFAULT)\n",
    "\n",
    "# List of all available detection models and their weights\n",
    " # fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights\n",
    " # retinanet_resnet50_fpn, RetinaNet_ResNet50_FPN_Weights\n",
    " # retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights\n",
    " # fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n",
    " # fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
    " # fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights\n",
    " # fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights\n",
    " # maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights\n",
    " # maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights\n",
    " # ssd300_vgg16, SSD300_VGG16_Weights\n",
    " # ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights\n",
    "\n",
    "# For YOLO\n",
    "\n",
    "# Here we can import and initialize a detection model from \"ultralytics.YOLO\"\n",
    "WEIGHTS = \"yolov5nu.pt\" # or \"yolov8n.pt\"\n",
    "MODEL = YOLO(WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad37da0-f4c2-4706-abfe-550b695895d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use MODEL.eval() to see all layers in the network\n",
    "MODEL.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba68e19-86bb-4012-a34b-3682181b5339",
   "metadata": {},
   "source": [
    "# For one image input, we can simply use this code with \"source\" as the string path to the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed16773d-6a68-4f23-91ea-df31fbb93c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"samples/truck.jpg\"\n",
    "frame = cv2.imread(source, cv2.IMREAD_COLOR)\n",
    "handleFrame(frame, 1, source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2d11ff-8742-401e-9b7f-395883e2574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The $handleFrame$ function takes care of the pre- and post-processing of the frames in case they have to be resized, grayscaled, flipped or enhanced for the detection model to deliver better results, and calls the processing method in between. This function has the following parameters:\n",
    "# frame: the input image\n",
    "# frameIndex: the index of this frame in the video if the source is a video, otherwise -1\n",
    "# source: the source name; if camera, then it will be a positive integer like 0, or the video path as string\n",
    "def handleFrame(frame, frameIndex=-1, source=None):\n",
    "    # Pre-processing can be implemented here in case the frames have to be resized, grayscaled or flipped\n",
    "    \n",
    "    processed_frame = process_frame(frame, frameIndex, source)\n",
    "    \n",
    "    # Post-processing can be implemented here in case any modifications have to be shown on the displayed frame\n",
    "\n",
    "    # Restores the RGB channels and displays the frames on top of each other like a smooth video\n",
    "    _, processed_frame = cv2.imencode('.jpeg', processed_frame)\n",
    "    display_handle.update(IPython.display.Image(data=processed_frame.tobytes()))\n",
    "    IPython.display.clear_output(wait=True)\n",
    "\n",
    "    # Clears the output to prepare for the next frame display in-place; this allows printing text above the image for debugging, but there will be blanks between frames\n",
    "    # img = Image.fromarray(processed_frame, 'RGB')\n",
    "    # display(img)\n",
    "    # clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89228bad-37be-44c0-87c1-f099abc16b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The process_frame function includes the detection head in the implementation and processes the frame accordingly with the same parameters as $handleFrame$. The iterations through the results objects can be obtained from the official documentation; as for YOLO models, it is in the Ultralytics Documents \\cite{ultralyticsmodels} and for all other models, it is documented in Pytorch \\cite{pytorchmodels}.\n",
    "def process_frame(frame, frameIndex = -1, source = None):\n",
    "\n",
    "    confidence_threshold = 0.5\n",
    "\n",
    "    # Here we call the model inference on \"frame\"\n",
    "    frame = detectAndHandle(frame, confidence_threshold, frameIndex, source)\n",
    "    \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c35f24c-4a8a-4b27-8b7b-c7f09b55d3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The detectAndHandle function is a model specific code, here we can implement the detection model in the pipeline, this code works for detection models from torchvision by Pytorch. The confidence_threshold is injected through the process_frame method.\n",
    "def detectAndHandle(frame, confidence_threshold, frameIndex = -1, source = None):\n",
    "    # Convert frame to RGB and PIL Image, then apply transformation\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    pil_image = Image.fromarray(frame_rgb)\n",
    "    image = transform(pil_image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Object detection\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        predictions = MODEL(image)\n",
    "\n",
    "    # Processing (inference) time\n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    # Visualization\n",
    "    scores = predictions[0]['scores']\n",
    "    boxes = predictions[0]['boxes']\n",
    "    labels = predictions[0]['labels']\n",
    "\n",
    "    # For each detection\n",
    "    for confidence, bounding_box, label in zip(scores, boxes, labels):\n",
    "        name = WEIGHTS.DEFAULT.meta[\"categories\"][label.item()]\n",
    "        if name in target_classes and confidence>confidence_threshold:\n",
    "            # Extract bounding boxes\n",
    "            x1, y1, x2, y2 = map(int, bounding_box)\n",
    "\n",
    "            # Ensure x1 < x2 and y1 < y2\n",
    "            x1, x2 = min(x1, x2), max(x1, x2)\n",
    "            y1, y2 = min(y1, y2), max(y1, y2)\n",
    "\n",
    "            # Build up the label text out of class name, confidence score and inference speed\n",
    "            label_text = f'{name} {confidence*100:.2f}% ({processing_time:.3f}s)'\n",
    "\n",
    "            # Draw bounding boxes and label on the frame\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0,255,255), 2)\n",
    "            cv2.putText(frame, label_text, (x1, y1+20), 0, 0.7, (0,255,255), 2)\n",
    "\n",
    "            # For further processing of a detected vehicle object (This method will be used in the next chapter)\n",
    "            process_vehicle(frame, bounding_box, name, confidence)\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de05f3f5-af97-4130-a9db-c96b0c766480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The detectAndHandle function is a model specific code, here we can implement the detection model in the pipeline. This variant of the code works for YOLO detection models from Ultralytics.\n",
    "def detectAndHandle(frame, confidence_threshold, frameIndex = -1, source = None):\n",
    "    # For YOLO Models from Ultralytics:\n",
    "    \n",
    "    start_time = time.time()\n",
    "    results = MODEL(frame, device=device)[0]\n",
    "    processing_time = time.time() - start_time\n",
    "\n",
    "    # Iterate through results\n",
    "    for result in results:\n",
    "        detection_count = result.boxes.shape[0]\n",
    "\n",
    "        # Iterate through all detections\n",
    "        for i in range(detection_count):\n",
    "            cls = int(result.boxes.cls[i].item())\n",
    "\n",
    "            # Extract class name\n",
    "            name = result.names[cls]\n",
    "\n",
    "            # Extract confidence score\n",
    "            confidence = float(result.boxes.conf[i].item())\n",
    "            \n",
    "            # Vehicle detected, proceed\n",
    "            if name in target_classes and confidence>confidence_threshold:\n",
    "                # Extract bounding boxes\n",
    "                bounding_box = result.boxes.xyxy[i].cpu().numpy()\n",
    "                x1, y1, x2, y2 = [int(x) for x in bounding_box]\n",
    "                \n",
    "                # Ensure x1 < x2 and y1 < y2\n",
    "                x1, x2 = min(x1, x2), max(x1, x2)\n",
    "                y1, y2 = min(y1, y2), max(y1, y2)\n",
    "\n",
    "                # Build up the label text out of class name, confidence score and inference speed\n",
    "                label_text = '{} {:.2f}% ({:.3f}s)'.format(name.upper(), confidence*100, processing_time)\n",
    "\n",
    "                # Draw bounding boxes and label on the frame\n",
    "                cv2.rectangle(frame, (x1,y1), (x2,y2), (0,255,255), 2)\n",
    "                cv2.putText(frame, label_text, (x1, y1+20), 0, 0.7, (0,255,255), 2)\n",
    "\n",
    "                # For further processing of a detected vehicle object (This will be discussed in the next chapter)\n",
    "                process_vehicle(frame, bounding_box, name, confidence)\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d9ba95-57f2-47e6-a258-17d1e20e2fd8",
   "metadata": {},
   "source": [
    "# For video or live stream inputs, we have to use the code below, starting by initializing the parameters and opening the stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affa2315-eb1c-49ca-bf15-9cca0288bc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 0 means the first available camera of this device, or \"data/video/sample.MP4\" for a specific video\n",
    "source = \"data/video/sample.MP4\"\n",
    "\n",
    "# This variable determines how many frames do we skip, e.g. every = 60 means handle only every 60th frame from the source\n",
    "every = 60\n",
    "\n",
    "# This initializes the display method with default configuration\n",
    "display_handle=display(None, display_id=True)\n",
    "\n",
    "# This initializes the input feed from a video or camera and opens the source\n",
    "cam = cv2.VideoCapture(source)\n",
    "\n",
    "# These parameters are only used for video processing to determine the start and end count of frames\n",
    "start = -1 if isinstance(source, int) else 0 # Or \"if source==0\" in case you only have one camera input source\n",
    "end = -1 if isinstance(source, int) else int(cam.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "if not isinstance(source, int): cam.set(1, start)\n",
    "\n",
    "# This initializes a loop safety for allowing reading upto safety_limit = 2000 empty frames in sequence before terminating the while loop as a result of an error; otherwise, end it normally when input video ends or input stream gets interrupted\n",
    "frameCount=start\n",
    "while_safety=0\n",
    "safety_limit = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6c89b3-d931-40b3-9b7d-b3cd431cf3d8",
   "metadata": {},
   "source": [
    "# Checks if the source has not yet been opened, then throws an error, otherwise, it starts handling the loop with a safety try except block, which makes sure the source is closed correctly even in the case of an error, otherwise, it could result in leaving the camera running in background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5328d8-a159-4814-8b2d-f13fda7375ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cam.isOpened(): print(\"Error: Could not open source.\")\n",
    "else:\n",
    "    try:\n",
    "        # If source is opened, it enters an infinite while loop for camera feed input or a loop determined by the end frame count of the video, as frameCount counts up to the end of the video\n",
    "        while True if end<0 else frameCount<end:\n",
    "            # Read next frame\n",
    "            _, frame = cam.read()\n",
    "\n",
    "            # If frame is empty and the source is a video, allow the safety_limit because there will be frames later; otherwise, release the camera\n",
    "            if frame is None:\n",
    "                if not isinstance(source, int):\n",
    "                    if while_safety > safety_limit: break\n",
    "                    while_safety += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    print(\"Error: Could not capture frame.\")\n",
    "                    cam.release()\n",
    "                    break\n",
    "\n",
    "            # If there is a frame skip rate then apply it and handle frame only when it is valid, otherwise handle every frame\n",
    "            if every>0:\n",
    "                if (frameCount+1)%math.floor(every) == 0:\n",
    "                    while_safety = 0\n",
    "                    handleFrame(frame, frameCount, source)\n",
    "            else:\n",
    "                while_safety=0\n",
    "                handleFrame(frame)\n",
    "\n",
    "            # Increase the current frameCount\n",
    "            frameCount += 1\n",
    "\n",
    "        # After the loop handle the source release accordingly\n",
    "        handleRelease()\n",
    "    except KeyboardInterrupt:\n",
    "        # If stream or video read is interrupted by user, handle source release accordingly as well\n",
    "        handleRelease()\n",
    "    except:\n",
    "        # Some error occurred to reach here; handle it and print the error tree\n",
    "        print(\"Unknown error.\")\n",
    "        try:\n",
    "            cam.release()\n",
    "            raise TypeError(\"Error: Source could not be released.\")\n",
    "        except:\n",
    "            pass\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a720a43-e60a-4c3b-8b14-56168deb060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The $handleRelease$ function handles the program flow in case the video ends or the camera source is released on user interruption or end of video file.\n",
    "def handleRelease():\n",
    "    cam.release()\n",
    "    print(\"Source released.\")\n",
    "\n",
    "    # We could output anything we want here after the video ends or after the stream closes; for example, we can add arrays with statistics and print them here to show results with numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77acf2b7-2f9e-4a12-adc9-2f457b403e54",
   "metadata": {},
   "source": [
    "# This code builds upon the codes used in Vehicle Detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e897b7-7385-4171-92a2-8f2cf69e8a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_vehicle(frame, parent_bounding_box, parent_name, parent_confidence):\n",
    "    x1, y1, x2, y2 = [int(x) for x in parent_bounding_box]\n",
    "    vehicle_frame = frame[y1:y2,x1:x2]\n",
    "\n",
    "    # Using an ultralytics YOLO model for license plate detection\n",
    "    results = plate_model(vehicle_frame, agnostic_nms=True, verbose=False)[0]\n",
    "    for result in results:\n",
    "        detection_count = result.boxes.shape[0]\n",
    "        for i in range(detection_count):\n",
    "            cls = int(result.boxes.cls[i].item())\n",
    "            name = result.names[cls]\n",
    "            confidence = float(result.boxes.conf[i].item())\n",
    "            bounding_box = result.boxes.xyxy[i].cpu().numpy()\n",
    "            a1, b1, a2, b2 = [int(x) for x in bounding_box2]\n",
    "            # Ensure a1 < a2 and b1 < b2\n",
    "            a1, a2 = min(a1, a2), max(a1, a2)\n",
    "            b1, b2 = min(b1, b2), max(b1, b2)\n",
    "\n",
    "            # Draw bounding boxes and write labels\n",
    "            cv2.rectangle(frame,(x1+a1,y1+b1),(x1+a2,y1+b2),(255,0,255),2)\n",
    "            cv2.putText(frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+a1+10,y1+b1-15),0,0.9,(255,0,255),2)\n",
    "\n",
    "            # This method tries to recognize the license plate characters\n",
    "            process_license(frame, [x1+a1, y1+b1, x1+a2, y1+b2], name, confidence, parent_bounding_box, parent_name, parent_confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fa494f-1c58-4e0c-9797-e09c044495ac",
   "metadata": {},
   "source": [
    "# This code shows an example of handling a specialist detection model with all needed classes for a specific use case, e.g. vehicle recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f016d84e-63db-4daa-bbd3-e846e6532203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grayscale(image):\n",
    "    return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "def thresholding(image):\n",
    "    return cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "\n",
    "def process_license(processed_frame, bounding_box, name, confidence, parentName='test', parentConfidence=0.0):\n",
    "    x1, y1, x2, y2 = [int(x) for x in bounding_box]\n",
    "    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(255,0,255),2)\n",
    "    cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1-15),0,0.7,(255,0,255))\n",
    "\n",
    "    license_frame = processed_frame[y1:y2,x1:x2]\n",
    "    process_recognition(...)\n",
    "\n",
    "# NN Layer Yolov8 Detection\n",
    "def process_frame(frame):\n",
    "    processed_frame = frame\n",
    "    results = specialist_model(processed_frame, agnostic_nms=True, verbose=False)[0]\n",
    "\n",
    "    # For every detection we check class names and implement the logic based on that\n",
    "    for result in results:\n",
    "        detection_count = result.boxes.shape[0]\n",
    "        for i in range(detection_count):\n",
    "            cls = int(result.boxes.cls[i].item())\n",
    "            name = result.names[cls]\n",
    "            confidence = float(result.boxes.conf[i].item())\n",
    "            bounding_box = result.boxes.xyxy[i].cpu().numpy()\n",
    "            x1, y1, x2, y2 = [int(x) for x in bounding_box]\n",
    "            # Ensure x1 < x2 and y1 < y2\n",
    "            x1, x2 = min(x1, x2), max(x1, x2)\n",
    "            y1, y2 = min(y1, y2), max(y1, y2)\n",
    "            cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(0,255,255),2)\n",
    "            cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1-15),0,0.7,(0,255,255))\n",
    "            if name==\"license_plate\": process_license(processed_frame, bounding_box, name, confidence)\n",
    "    return processed_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1527bedc-4f5d-4c42-9f6b-f38ff9bbfc3f",
   "metadata": {},
   "source": [
    "# This is the implementation for recognizing text using pytesseract OCR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bb257f-f2f9-42f9-ae33-3f544864cf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract   # pip install pytesseract\n",
    "\n",
    "detected_plates = []\n",
    "\n",
    "# Install this from \"https://github.com/UB-Mannheim/tesseract/wiki\"\n",
    "pytesseract.pytesseract.tesseract_cmd = 'C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe'\n",
    "\n",
    "# get grayscale image\n",
    "def get_grayscale(image):\n",
    "    return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#thresholding\n",
    "def thresholding(image):\n",
    "    return cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "\n",
    "def process_license(frame, parent_bounding_box, parent_name, parent_confidence, grandparent_bounding_box, grandparent_name, grandparent_confidence):\n",
    "    x1, y1, x2, y2 = [int(x) for x in parent_bounding_box]\n",
    "    license_plate_frame = frame[y1:y2,x1:x2]\n",
    "\n",
    "    grayscaled = get_grayscale(license_plate_frame)\n",
    "    threshholded = thresholding(grayscaled)\n",
    "\n",
    "    max_conf = 0.0\n",
    "    best_text = \"\"\n",
    "\n",
    "    results = pytesseract.image_to_data(threshholded, config='-c tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n",
    "    # NN Layer License Plate Recognition \"Pytesseract OCR\"\n",
    "\n",
    "    parsedLines = results.split('\\n')\n",
    "    best_depth = len(parsedLines)-2\n",
    "    for line in parsedLines:\n",
    "        params = line.split()\n",
    "        if len(params)==12 and float(params[10].replace('conf','0.0'))>=max_conf:\n",
    "            max_conf = float(params[10].replace('conf','0.0'))\n",
    "            best_text = params[11] if params[11]!=\"text\" else \"-\"\n",
    "            if max_conf>=0.0 and len(best_text)>0:\n",
    "                detected_plates.append([grandparent_confidence*100, grandparent_name.upper(), parent_confidence*100, parent_name.upper(), confidence*100, name.upper(), max_conf, best_text, \"tesse\"])\n",
    "    cv2.putText(frame, '{} {:.2f}%'.format(\"[tesse]: \"+best_text, max_conf),(x1+10,y2+25),0,0.9,(0,0,255),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf1c37d-36d5-4a43-a0f3-5f8edb7f9cbd",
   "metadata": {},
   "source": [
    "# This is the implementation for recognizing text using EasyOCR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fa8fd7-a9b0-4245-8d1d-faf8b5881889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import easyocr  # pip install easyocr\n",
    "reader = easyocr.Reader(['en', 'de'])\n",
    "\n",
    "detected_plates = []\n",
    "\n",
    "# get grayscale image\n",
    "def get_grayscale(image):\n",
    "    return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#thresholding\n",
    "def thresholding(image):\n",
    "    return cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "\n",
    "def process_license(frame, parent_bounding_box, parent_name, parent_confidence, grandparent_bounding_box, grandparent_name, grandparent_confidence):\n",
    "    x1, y1, x2, y2 = [int(x) for x in parent_bounding_box]\n",
    "    license_plate_frame = frame[y1:y2,x1:x2]\n",
    "\n",
    "    grayscaled = get_grayscale(license_plate_frame)\n",
    "    threshholded = thresholding(grayscaled)\n",
    "\n",
    "    max_conf = 0.0\n",
    "    best_text = \"\"\n",
    "\n",
    "    result = reader.readtext(threshholded)\n",
    "    text = \"\"\n",
    "    conf = 0.0\n",
    "    for res in result:\n",
    "        if res[2]>conf:\n",
    "            conf=res[2]\n",
    "            text=res[1]\n",
    "            if res[2]*100>max_conf:\n",
    "                max_conf = res[2]*100\n",
    "                best_text = res[1]\n",
    "                if max_conf>=0.0 and len(best_text)>0:\n",
    "                    detected_plates.append([grandparent_confidence*100, grandparent_name.upper(), parent_confidence*100, parent_name.upper(), confidence*100, name.upper(), max_conf, best_text, \"easyocr\"])\n",
    "    cv2.putText(frame, '{} {:.2f}%'.format(\"[easy]: \"+str(text), conf*100),(x1+10,y2+57),0,0.9,(200,200,200),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6039fa-e4bc-4055-b96e-144bf11a53b0",
   "metadata": {},
   "source": [
    "# This is the implementation for recognizing text using EasyOCR. detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf42f64-9ce8-493f-b204-8cffffc53524",
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_plates = []\n",
    "\n",
    "def process_license(frame, parent_bounding_box, parent_name, parent_confidence, grandparent_bounding_box, grandparent_name, grandparent_confidence):\n",
    "    x1, y1, x2, y2 = [int(x) for x in parent_bounding_box]\n",
    "    license_plate_frame = frame[y1:y2,x1:x2]\n",
    "\n",
    "    # Using an ultralytics YOLO model for license plate recognition\n",
    "    results = characters_model(license_plate_frame, agnostic_nms=True, verbose=False)[0]\n",
    "    for result in results:\n",
    "        detection_count = result.boxes.shape[0]\n",
    "        for i in range(detection_count):\n",
    "            cls = int(result.boxes.cls[i].item())\n",
    "            name = result.names[cls]\n",
    "            confidence = float(result.boxes.conf[i].item())\n",
    "            bounding_box = result.boxes.xyxy[i].cpu().numpy()\n",
    "            a1, b1, a2, b2 = [int(x) for x in bounding_box2]\n",
    "            # Ensure a1 < a2 and b1 < b2\n",
    "            a1, a2 = min(a1, a2), max(a1, a2)\n",
    "            b1, b2 = min(b1, b2), max(b1, b2)\n",
    "\n",
    "            # Draw bounding boxes and write labels\n",
    "            cv2.rectangle(frame,(x1+a1,y1+b1),(x1+a2,y1+b2),(255,0,255),2)\n",
    "            cv2.putText(frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+a1+10,y1+b1-15),0,0.9,(255,0,255),2)\n",
    "            plate_parts.append([a1,b1,name2])\n",
    "\n",
    "    sorted_list = sorted(plate_parts,key=lambda l:l[0])\n",
    "    for part in range(len(sorted_list)):\n",
    "        if sorted_list[part][2]!=\"undefined\":\n",
    "            plate_text+=sorted_list[part][2]\n",
    "    detected_plates.append(plate_text)  # We could also add parent and grandparent object information here too\n",
    "\n",
    "print(detected_plates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fbc19d-ebb7-4ba4-b43a-6cb8efd5f48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code displays every tested frame with every model name, accuracy, speed, class names and bounding boxes on the displayed frame. It also outputs a code that can be used in plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c1f5f9-59a8-4ae4-a633-706f6aaf9191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "\n",
    "# Imports all pre-trained YOLOv5, YOLOv8, YOLOv9 and in the future YOLOv10 models (COCO dataset with 80 classes), they will be automatically downloaded if not available\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Imports all other pre-trained detection models needed (COCO dataset with 80 classes), they will be automatically downloaded if they are not available\n",
    "from torchvision.models.detection import *\n",
    "\n",
    "from torchvision.io.image import read_image\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import IPython\n",
    "from IPython.display import display, clear_output\n",
    "import traceback\n",
    "import math\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Transforms input image to tensor, there is a commented line that resizes the input image into 300x300 before processing but it was not used in the analysis, it is kept so you can have the option to use it, but the models usually automatically resize inputs based on the trained weights resolution.\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((300, 300)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# This is a new class definition that describes a Detection Model object, where detections can be stored and averages can be calculated, this workflow is based on the RAM size because it stores data temporarily, I wanted to rebuild the code so it works with file save and file read and that every model inferences the whole video then the next model comes but I still wanted to test with camera on a live feed so I kept it as is in the end. This class downloads, loads and initializes all models before the beginning of tests.\n",
    "class DetectionModel():\n",
    "    def __init__(self, concept, backbone, model, weights, reference, color=None):\n",
    "        self.concept = concept\n",
    "        self.backbone = backbone\n",
    "        self.model = model(weights) if concept==\"YOLO\" else model(weights=weights.DEFAULT)\n",
    "        if concept!=\"YOLO\":\n",
    "            self.model.to(device)\n",
    "            self.model.eval()\n",
    "        self.weights = weights if concept==\"YOLO\" else weights.DEFAULT\n",
    "        self.reference = reference\n",
    "        self.color = color\n",
    "        self.detections = []\n",
    "\n",
    "    def addDetection(self, detection):\n",
    "        self.detections.append(detection)\n",
    "\n",
    "    def getDetections(self):\n",
    "        return self.detections\n",
    "\n",
    "    def clearDetections(self):\n",
    "        self.detections = []\n",
    "\n",
    "    def detectionsCount(self):\n",
    "        return len(self.detections)\n",
    "\n",
    "    def inferencesCount(self):\n",
    "        length = 0\n",
    "        addedFrames = []\n",
    "        if len(self.detections)>0:\n",
    "            for i in range(len(self.detections)):\n",
    "                if self.detections[i].frame not in addedFrames:\n",
    "                    addedFrames.append(self.detections[i].frame)\n",
    "        return len(addedFrames)\n",
    "\n",
    "    def getAveragePrecision(self):\n",
    "        sum=0.0\n",
    "        avg=0.0\n",
    "        if len(self.detections)>0:\n",
    "            for i in range(len(self.detections)):\n",
    "                sum+=self.detections[i].precision\n",
    "            avg = sum/len(self.detections)\n",
    "        return  avg.item() if torch.is_tensor(avg) else avg\n",
    "\n",
    "    def getMaxPrecision(self):\n",
    "        max=0.0\n",
    "        for i in range(len(self.detections)):\n",
    "            if max<self.detections[i].precision:\n",
    "                max=self.detections[i].precision\n",
    "        return max.item() if torch.is_tensor(max) else max\n",
    "\n",
    "    def getMinPrecision(self):\n",
    "        min=9999999.99\n",
    "        for i in range(len(self.detections)):\n",
    "            if min>self.detections[i].precision:\n",
    "                min=self.detections[i].precision\n",
    "        return 0.0 if min==9999999.99 else min.item() if torch.is_tensor(min) else min\n",
    "\n",
    "    def getAverageSpeed(self):\n",
    "        sum=0.0\n",
    "        avg=0.0\n",
    "        if len(self.detections)>0:\n",
    "            for i in range(len(self.detections)):\n",
    "                sum+=self.detections[i].speed\n",
    "            avg = sum/len(self.detections)\n",
    "        return avg\n",
    "\n",
    "    def getMaxSpeed(self):\n",
    "        max=0.0\n",
    "        for i in range(len(self.detections)):\n",
    "            if max<self.detections[i].speed:\n",
    "                max=self.detections[i].speed\n",
    "        return max\n",
    "\n",
    "    def getMinSpeed(self):\n",
    "        min=9999999.99\n",
    "        for i in range(len(self.detections)):\n",
    "            if min>self.detections[i].speed:\n",
    "                min=self.detections[i].speed\n",
    "        return 0.0 if min==9999999.99 else min\n",
    "\n",
    "    def getAverageInferenceSpeed(self):\n",
    "        sum=0.0\n",
    "        avg=0.0\n",
    "        addedFrames=[]\n",
    "        if len(self.detections)>0:\n",
    "            for i in range(len(self.detections)):\n",
    "                if self.detections[i].frame not in addedFrames:\n",
    "                    sum+=self.detections[i].speed\n",
    "                    addedFrames.append(self.detections[i].frame)\n",
    "            avg = sum/len(addedFrames) if len(addedFrames)>0 else 0.0\n",
    "        return avg\n",
    "\n",
    "    def getMaxInferenceSpeed(self):\n",
    "        max=0.0\n",
    "        addedFrames=[]\n",
    "        for i in range(len(self.detections)):\n",
    "            if max<self.detections[i].speed and self.detections[i].frame not in addedFrames:\n",
    "                max=self.detections[i].speed\n",
    "                addedFrames.append(self.detections[i].frame)\n",
    "        return max\n",
    "\n",
    "    def getMinInferenceSpeed(self):\n",
    "        min=9999999.99\n",
    "        addedFrames=[]\n",
    "        for i in range(len(self.detections)):\n",
    "            if min>self.detections[i].speed and self.detections[i].frame not in addedFrames:\n",
    "                min=self.detections[i].speed\n",
    "                addedFrames.append(self.detections[i].frame)\n",
    "        return 0.0 if min==9999999.99 else min\n",
    "\n",
    "    def filterPrecisionsBelow(self, minPrecision):\n",
    "        newDetections = []\n",
    "        for i in range(len(self.detections)):\n",
    "            if self.detections[i].precision>=minPrecision:\n",
    "                newDetections.append(self.detections[i])\n",
    "        self.detections = newDetections\n",
    "\n",
    "    def printResults(self):\n",
    "        print(\"Model:\", self.reference, \n",
    "              \"\\nBest Precision:\", self.getMaxPrecision(), \n",
    "              \"%\\nAverage Precision:\", self.getAveragePrecision(), \n",
    "              \"%\\nWorst Precision:\", self.getMinPrecision(), \n",
    "              \"%\\nBest Speed:\", self.getMinSpeed(), \n",
    "              \"seconds\\nAverage Speed:\", self.getAverageSpeed(), \n",
    "              \"seconds\\nWorst Speed:\", self.getMaxSpeed(), \n",
    "              \"seconds\\nBest Inference Speed:\", self.getMinInferenceSpeed(), \n",
    "              \"seconds\\nAverage Inference Speed:\", self.getAverageInferenceSpeed(), \n",
    "              \"seconds\\nWorst Inference Speed:\", self.getMaxInferenceSpeed(), \n",
    "              \"seconds\\nTotal Detections:\", self.detectionsCount(), \n",
    "              \"\\nTotal Inferences:\", self.inferencesCount(), \n",
    "              \"\\n\\n\")\n",
    "\n",
    "# This class represents a Detection Result, it is saved into the detections array in the Detection Model object\n",
    "class DetectionResult():\n",
    "    def __init__(self, model, label, precision, speed, device=None, source=None, frame=None, boundingBox=None, details=None):\n",
    "        self.model = model\n",
    "        self.label = label\n",
    "        self.precision = precision.item() if torch.is_tensor(precision) else precision\n",
    "        self.speed = speed.item() if torch.is_tensor(speed) else speed\n",
    "        self.device = device\n",
    "        self.source = source\n",
    "        self.frame = frame\n",
    "        self.boundingBox = boundingBox\n",
    "        self.details = details\n",
    "\n",
    "target_classes = [\"truck\", \"bus\", \"car\", \"train\", \"bicycle\"]\n",
    "\n",
    "# This defines all the models we want to work with, we can import any model we want above and add it in this array eventually. Defining a new model takes the concept name as the first parameter, the backbone name as the second parameter, the model object in the third, the pretrained weights name in fourth, a custom given name for the model as fifth parameter and a color tupel in the last parameter.\n",
    "VEHICLE_DETECTION_MODELS = [\n",
    "    DetectionModel(\"FCOS\", \"resnet50_fpn\", fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights, \"fcos_resnet\", (255,0,0)),\n",
    "    DetectionModel(\"RetinaNet\", \"resnet50_fpn\", retinanet_resnet50_fpn, RetinaNet_ResNet50_FPN_Weights, \"retinanet_resnet\", (255,255,0)),\n",
    "    DetectionModel(\"RetinaNet\", \"resnet50_fpn_v2\", retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights, \"retinanet_resnet_v2\", (0,234,255)),\n",
    "    DetectionModel(\"FasterRCNN\", \"resnet50_fpn\", fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights, \"faster_rcnn_resnet\", (170,0,255)),\n",
    "    DetectionModel(\"FasterRCNN\", \"resnet50_fpn_v2\", fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights, \"faster_rcnn_resnet_v2\", (255,127,0)),\n",
    "    DetectionModel(\"FasterRCNN\", \"mobilenet_v3_large_fpn\", fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights, \"faster_rcnn_mobilenet_v3\", (191,255,0)),\n",
    "    DetectionModel(\"FasterRCNN\", \"mobilenet_v3_large_320_fpn\", fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights, \"faster_rcnn_mobilenet_v3_320\", (0,149,255)),\n",
    "    DetectionModel(\"MaskRCNN\", \"resnet50_fpn\", maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights, \"mask_rcnn_resnet\", (255,0,170)),\n",
    "    DetectionModel(\"MaskRCNN\", \"resnet50_fpn_v2\", maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights, \"mask_rcnn_resnet_v2\", (255,212,0)),\n",
    "    DetectionModel(\"SSD300\", \"vgg16\", ssd300_vgg16, SSD300_VGG16_Weights, \"ssd_vgg16\", (106,255,0)),\n",
    "    DetectionModel(\"SSDLite320\", \"mobilenet_v3_large\", ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights, \"ssd_mobilenet_v3\", (0,64,255)),\n",
    "    DetectionModel(\"YOLO\", \"csp_darknet53\", YOLO, 'yolov5nu.pt', \"yolov5nu\", (237,185,185)),\n",
    "    DetectionModel(\"YOLO\", \"csp_darknet53\", YOLO, 'yolov5su.pt', \"yolov5su\", (185,215,237)),\n",
    "    DetectionModel(\"YOLO\", \"csp_darknet53\", YOLO, 'yolov5mu.pt', \"yolov5mu\", (231,233,185)),\n",
    "    DetectionModel(\"YOLO\", \"csp_darknet53\", YOLO, 'yolov5lu.pt', \"yolov5lu\", (220,185,237)),\n",
    "    DetectionModel(\"YOLO\", \"csp_darknet53\", YOLO, 'yolov5xu.pt', \"yolov5xu\", (185,237,224)),\n",
    "    DetectionModel(\"YOLO\", \"csp_darknet53\", YOLO, 'yolov5n6u.pt', \"yolov5n6u\", (143,35,35)),\n",
    "    DetectionModel(\"YOLO\", \"csp_darknet53\", YOLO, 'yolov5s6u.pt', \"yolov5s6u\", (35,98,143)),\n",
    "    DetectionModel(\"YOLO\", \"csp_darknet53\", YOLO, 'yolov5m6u.pt', \"yolov5m6u\", (143,106,35)),\n",
    "    DetectionModel(\"YOLO\", \"csp_darknet53\", YOLO, 'yolov5l6u.pt', \"yolov5l6u\", (107,35,143)),\n",
    "    DetectionModel(\"YOLO\", \"csp_darknet53\", YOLO, 'yolov5x6u.pt', \"yolov5x6u\", (79,143,35)),\n",
    "    DetectionModel(\"YOLO\", \"csp_darknet53\", YOLO, 'yolov8n.pt', \"yolov8n\", (185,237,224)),\n",
    "    DetectionModel(\"YOLO\", \"csp_darknet53\", YOLO, 'yolov8s.pt', \"yolov8s\", (115,115,115)),\n",
    "    DetectionModel(\"YOLO\", \"csp_darknet53\", YOLO, 'yolov8m.pt', \"yolov8m\", (204,204,204)),\n",
    "    DetectionModel(\"YOLO\", \"csp_darknet53\", YOLO, 'yolov8l.pt', \"yolov8l\", (255,0,0)),\n",
    "    DetectionModel(\"YOLO\", \"csp_darknet53\", YOLO, 'yolov8x.pt', \"yolov8x\", (255,255,0)),\n",
    "    DetectionModel(\"YOLO\", \"csp_darknet53\", YOLO, 'yolov9c.pt', \"yolov9c\", (0,234,255)),\n",
    "    DetectionModel(\"YOLO\", \"csp_darknet53\", YOLO, 'yolov9e.pt', \"yolov9e\", (170,0,255))#,\n",
    "    # DetectionModel(\"YOLO\", \"csp_darknet53\", YOLO, 'yolov10n.pt', \"yolov10n\", (185,237,224)),\n",
    "    # DetectionModel(\"YOLO\", \"csp_darknet53\", YOLO, 'yolov10s.pt', \"yolov10s\", (115,115,115)),\n",
    "    # DetectionModel(\"YOLO\", \"csp_darknet53\", YOLO, 'yolov10m.pt', \"yolov10m\", (204,204,204)),\n",
    "    # DetectionModel(\"YOLO\", \"csp_darknet53\", YOLO, 'yolov10b.pt', \"yolov10b\", (204,204,204)),\n",
    "    # DetectionModel(\"YOLO\", \"csp_darknet53\", YOLO, 'yolov10l.pt', \"yolov10l\", (255,0,0)),\n",
    "    # DetectionModel(\"YOLO\", \"csp_darknet53\", YOLO, 'yolov10x.pt', \"yolov10x\", (255,255,0))\n",
    "]\n",
    "\n",
    "def process(frame, frameIndex=-1, source=None):\n",
    "    for i in range(len(VEHICLE_DETECTION_MODELS)):\n",
    "        currentModel = VEHICLE_DETECTION_MODELS[i]\n",
    "        processed_frame = frame.copy()\n",
    "        confidence_threshold = 0.5\n",
    "        \n",
    "        if currentModel.concept==\"YOLO\":\n",
    "            start_time = time.time()\n",
    "            results = currentModel.model(processed_frame, agnostic_nms=True, verbose=False, device=device)[0]\n",
    "            processing_time = time.time() - start_time\n",
    "            for result in results:\n",
    "                detection_count = result.boxes.shape[0]\n",
    "                for j in range(detection_count):\n",
    "                    cls = int(result.boxes.cls[j].item())\n",
    "                    name = result.names[cls]\n",
    "                    confidence = float(result.boxes.conf[j].item())\n",
    "                    bounding_box = result.boxes.xyxy[j].cpu().numpy()\n",
    "                    if name in target_classes and confidence>confidence_threshold:\n",
    "                        x1, y1, x2, y2 = [int(x) for x in bounding_box]\n",
    "                        # Ensure x1 < x2 and y1 < y2\n",
    "                        x1, x2 = min(x1, x2), max(x1, x2)\n",
    "                        y1, y2 = min(y1, y2), max(y1, y2)\n",
    "                        label = '{} ({:.2f}%) ({:.3f}s) {}'.format(name.upper(), confidence*100, processing_time, currentModel.reference)\n",
    "                        print(label)\n",
    "                        VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, name, confidence*100, processing_time, device, source, frameIndex, bounding_box, label))\n",
    "                        cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)\n",
    "                        cv2.putText(processed_frame, label, (x1+10, y1+25), 0, 0.8, currentModel.color, 2)\n",
    "        else:\n",
    "            # Convert frame to RGB and PIL Image, then apply transformation\n",
    "            frame_rgb = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)\n",
    "            pil_image = Image.fromarray(frame_rgb)\n",
    "            image = transform(pil_image).unsqueeze(0).to(device)\n",
    "        \n",
    "            # Object detection\n",
    "            start_time = time.time()\n",
    "            with torch.no_grad():\n",
    "                predictions = currentModel.model(image)\n",
    "        \n",
    "            # Processing time\n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            # Visualization\n",
    "            scores = predictions[0]['scores']\n",
    "            boxes = predictions[0]['boxes']\n",
    "            labels = predictions[0]['labels']\n",
    "            \n",
    "            for score, box, label in zip(scores, boxes, labels):\n",
    "                if currentModel.weights.meta[\"categories\"][label.item()] in target_classes and score>confidence_threshold:\n",
    "                    x1, y1, x2, y2 = map(int, box)\n",
    "                    label_text = f'{currentModel.weights.meta[\"categories\"][label.item()].upper()} ({score*100:.2f}%) ({processing_time:.3f}s) {currentModel.reference}'\n",
    "                    print(label_text)\n",
    "                    VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, currentModel.weights.meta[\"categories\"][label.item()], float(score.item())*100, processing_time, device, source, frameIndex, box, label_text))\n",
    "                    cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)\n",
    "                    cv2.putText(processed_frame, label_text, (x1, y1+20), 0, 0.8, currentModel.color, 2)\n",
    "\n",
    "        img = Image.fromarray(processed_frame, 'RGB')\n",
    "        display(img)\n",
    "        clear_output(wait=True)\n",
    "    return frame\n",
    "\n",
    "def handleFrame(frame, frameIndex=-1, source=None):\n",
    "    processed_frame = process(frame,frameIndex,source)\n",
    "\n",
    "def handleRelease():\n",
    "    cam.release()\n",
    "    print(\"Source released.\")\n",
    "\n",
    "    # The output of the experiment result is processed below\n",
    "    \n",
    "    print()\n",
    "    print(device)\n",
    "    print(source)\n",
    "    print()\n",
    "\n",
    "    BestPrecision=None\n",
    "    BestAvgPrecision=None\n",
    "    WorstPrecision=None\n",
    "    BestSpeed=None\n",
    "    BestAvgSpeed=None\n",
    "    WorstSpeed=None\n",
    "\n",
    "    Models=[]\n",
    "    BestPrecisions=[]\n",
    "    AvgPrecisions=[]\n",
    "    WorstPrecisions=[]\n",
    "    BestSpeeds=[]\n",
    "    AvgSpeeds=[]\n",
    "    WorstSpeeds=[]\n",
    "    BestInferenceSpeeds=[]\n",
    "    AvgInferenceSpeeds=[]\n",
    "    WorstInferenceSpeeds=[]\n",
    "    TotalDetectionsArr=[]\n",
    "    TotalInferencesArr=[]\n",
    "    \n",
    "    for i in range(len(VEHICLE_DETECTION_MODELS)):\n",
    "        currentModel = VEHICLE_DETECTION_MODELS[i]\n",
    "        \n",
    "        # You can uncomment this to see details of every model\n",
    "        # currentModel.printResults()\n",
    "\n",
    "        Models.append(currentModel.reference)\n",
    "        BestPrecisions.append(currentModel.getMaxPrecision())\n",
    "        AvgPrecisions.append(currentModel.getAveragePrecision())\n",
    "        WorstPrecisions.append(currentModel.getMinPrecision())\n",
    "        BestSpeeds.append(currentModel.getMinSpeed())\n",
    "        AvgSpeeds.append(currentModel.getAverageSpeed())\n",
    "        WorstSpeeds.append(currentModel.getMaxSpeed())\n",
    "        BestInferenceSpeeds.append(currentModel.getMinInferenceSpeed())\n",
    "        AvgInferenceSpeeds.append(currentModel.getAverageInferenceSpeed())\n",
    "        WorstInferenceSpeeds.append(currentModel.getMaxInferenceSpeed())\n",
    "        TotalDetectionsArr.append(currentModel.detectionsCount())\n",
    "        TotalInferencesArr.append(currentModel.inferencesCount())\n",
    "        \n",
    "        if BestPrecision is None or BestPrecision.getMaxPrecision()<=currentModel.getMaxPrecision():\n",
    "            BestPrecision = currentModel\n",
    "        if BestAvgPrecision is None or BestAvgPrecision.getAveragePrecision()<=currentModel.getAveragePrecision():\n",
    "            BestAvgPrecision = currentModel\n",
    "        if WorstPrecision is None or WorstPrecision.getMinPrecision()>=currentModel.getMinPrecision():\n",
    "            WorstPrecision = currentModel\n",
    "        if BestSpeed is None or BestSpeed.getMinSpeed()>=currentModel.getMinSpeed():\n",
    "            BestSpeed = currentModel\n",
    "        if BestAvgSpeed is None or BestAvgSpeed.getAverageSpeed()>=currentModel.getAverageSpeed():\n",
    "            BestAvgSpeed = currentModel\n",
    "        if WorstSpeed is None or WorstSpeed.getMaxSpeed()<=currentModel.getMaxSpeed():\n",
    "            WorstSpeed = currentModel\n",
    "            \n",
    "    if BestPrecision is not None: \n",
    "        print(\"Best Precision:\")\n",
    "        BestPrecision.printResults()\n",
    "    if BestAvgPrecision is not None: \n",
    "        print(\"Best Average Precision:\")\n",
    "        BestAvgPrecision.printResults()\n",
    "    if WorstPrecision is not None: \n",
    "        print(\"Worst Precision:\")\n",
    "        WorstPrecision.printResults()\n",
    "    if BestSpeed is not None: \n",
    "        print(\"Best Speed:\")\n",
    "        BestSpeed.printResults()\n",
    "    if BestAvgSpeed is not None: \n",
    "        print(\"Best Average Speed:\")\n",
    "        BestAvgSpeed.printResults()\n",
    "    if WorstSpeed is not None: \n",
    "        print(\"Worst Speed:\")\n",
    "        WorstSpeed.printResults()\n",
    "\n",
    "    # This can be copied and used to represent the data as diagrams and tables\n",
    "    print(\"models=\",Models)\n",
    "    print(\"best_precisions=\",BestPrecisions)\n",
    "    print(\"avg_precisions=\",AvgPrecisions)\n",
    "    print(\"worst_precisions=\",WorstPrecisions)\n",
    "    print(\"best_speeds=\",BestSpeeds)\n",
    "    print(\"avg_speeds=\",AvgSpeeds)\n",
    "    print(\"worst_speeds=\",WorstSpeeds)\n",
    "    print(\"best_inference_speeds=\",BestInferenceSpeeds)\n",
    "    print(\"avg_inference_speeds=\",AvgInferenceSpeeds)\n",
    "    print(\"worst_inference_speeds=\",WorstInferenceSpeeds)\n",
    "    print(\"total_detections=\",TotalDetectionsArr)\n",
    "    print(\"total_inferences=\",TotalInferencesArr)\n",
    "\n",
    "inputType=\"video\"\n",
    "source = \"data/video/sample.MP4\" if inputType==\"video\" else 0\n",
    "cam = cv2.VideoCapture(source)\n",
    "display_handle=display(None, display_id=True)\n",
    "every = 30\n",
    "\n",
    "start = 0 if source!=0 else -1\n",
    "end = int(cam.get(cv2.CAP_PROP_FRAME_COUNT)) if source!=0 else -1\n",
    "if source!=0: cam.set(1, start)\n",
    "\n",
    "frameCount=start\n",
    "while_safety=0\n",
    "saved_count=0\n",
    "\n",
    "if not cam.isOpened(): print(\"Error: Could not open source.\")\n",
    "else:\n",
    "    try:\n",
    "        while True if end<0 else frameCount<end:\n",
    "            _, frame = cam.read()\n",
    "            if frame is None:\n",
    "                if source!=0:\n",
    "                    if while_safety > 2000: break\n",
    "                    while_safety += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    print(\"Error: Could not capture frame.\")\n",
    "                    cam.release()\n",
    "                    break\n",
    "                    \n",
    "            if every>0:\n",
    "                if (frameCount+1)%math.floor(every) == 0:\n",
    "                    while_safety = 0\n",
    "                    handleFrame(frame, frameCount, source)\n",
    "            else:\n",
    "                while_safety=0\n",
    "                handleFrame(frame)\n",
    "            frameCount += 1\n",
    "        handleRelease()\n",
    "    except KeyboardInterrupt:\n",
    "        handleRelease()\n",
    "    except:\n",
    "        print(\"Unknown error.\")\n",
    "        try:\n",
    "            cam.release()\n",
    "            raise TypeError(\"Error: Source could not be released.\")\n",
    "        except:\n",
    "            pass\n",
    "        traceback.print_exc() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7b2a7f-cd10-4220-ba23-d2a2b003bb03",
   "metadata": {},
   "source": [
    "# After we copy the output from the Detection Experiment Analysis, we can run it in another cell, then use the code below to save and display the data. I still wanted to make the primary data also part of the output from the previous code, like the device used, source input path and parameters like framerate, threshold and target classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5fba0c-5903-4c0e-acb6-09fc2d431d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can paste the array definitions from the output above in this cell and run it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1baac9-762d-4ed6-90c3-8db70f1749a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# Generate a color map\n",
    "num_models = len(models)\n",
    "cmap = matplotlib.colormaps.get_cmap('nipy_spectral')\n",
    "colors = [cmap(i / num_models) for i in range(num_models)]\n",
    "\n",
    "# Create a figure and plot with unique colors\n",
    "plt.figure(figsize=(14, 10))\n",
    "for i, model in enumerate(models):\n",
    "    plt.scatter(avg_inference_speeds[i], avg_precisions[i], color=colors[i], label=model, edgecolor='black')\n",
    "\n",
    "print(\"DEVICE: CPU (Intel Core i7-14700KF)\")\n",
    "print(\"SOURCE: data/video/sample.MP4 (1080p60fps Video)\")\n",
    "print(\"SKIP EVERY 60 FRAMES\")\n",
    "print(\"TOTALLY PROCESSED FRAMES: 84\")\n",
    "print(\"COCO Dataset, Classes truck, bus, car, train, bicycle\")\n",
    "print(\"ALL >50%\")\n",
    "# Axis labels and plot title\n",
    "plt.xlabel('Average Speed (seconds)')\n",
    "plt.ylabel('Average Precision (%)')\n",
    "plt.title('Model Performance: Average Inference Speed x Average Accuracy')\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1.05, 1), fontsize='small')\n",
    "plt.grid(True)\n",
    "plt.savefig(\"Analysis/Experiment1/i7_14700KF/Experiment1_Diagram.png\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Set up the figure and axis for the table\n",
    "fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "# The table data: transpose the array to make each column a different metric\n",
    "table_data = np.transpose([models, best_precisions, avg_precisions, worst_precisions])\n",
    "\n",
    "# Create the table in the plot\n",
    "table = ax.table(cellText=table_data, colLabels=[\"Model\", \"Best Precision (%)\", \"Average Precision (%)\", \"Worst Precision (%)\"],\n",
    "                 cellLoc='center', loc='center', colColours=[\"palegreen\"] * 4)\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.2)  # Scale table size\n",
    "\n",
    "plt.title(\"Model Precision Metrics\")\n",
    "plt.savefig(\"Analysis/Experiment1/i7_14700KF/Experiment1_Precision_Table.png\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# # Set up the figure and axis for the table\n",
    "# fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed\n",
    "# ax.axis('tight')\n",
    "# ax.axis('off')\n",
    "\n",
    "# # The table data: transpose the array to make each column a different metric\n",
    "# table_data = np.transpose([models, best_speeds, avg_speeds, worst_speeds])\n",
    "\n",
    "# # Create the table in the plot\n",
    "# table = ax.table(cellText=table_data, colLabels=[\"Model\", \"Best Speed (s)\", \"Average Speed (s)\", \"Worst Speed (s)\"],\n",
    "#                  cellLoc='center', loc='center', colColours=[\"palegreen\"] * 4)\n",
    "# table.auto_set_font_size(False)\n",
    "# table.set_fontsize(10)\n",
    "# table.scale(1.2, 1.2)  # Scale table size\n",
    "\n",
    "# plt.title(\"Model Speed Metrics\")\n",
    "# plt.show()\n",
    "\n",
    "# Set up the figure and axis for the table\n",
    "fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "# The table data: transpose the array to make each column a different metric\n",
    "table_data = np.transpose([models, best_inference_speeds, avg_inference_speeds, worst_inference_speeds])\n",
    "\n",
    "# Create the table in the plot\n",
    "table = ax.table(cellText=table_data, colLabels=[\"Model\", \"Best Inference Speed (s)\", \"Average Inference Speed (s)\", \"Worst Inference Speed (s)\"],\n",
    "                 cellLoc='center', loc='center', colColours=[\"palegreen\"] * 4)\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.2)  # Scale table size\n",
    "\n",
    "plt.title(\"Model Inference Speed Metrics\")\n",
    "plt.savefig(\"Analysis/Experiment1/i7_14700KF/Experiment1_Time_Table.png\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# This code is used to remap the data as a latex table sorted by average precision in descending order.\n",
    "data = []\n",
    "for i in range(len(models)):\n",
    "    data.append([models[i], f'{avg_precisions[i]:.2f}', f'{avg_inference_speeds[i]:.5f}'])\n",
    "\n",
    "# LaTeX model names mapping\n",
    "latex_model_names = {\n",
    "    'fcos_resnet': 'FCOS ResNet50 FPN',\n",
    "    'retinanet_resnet': 'RetinaNet ResNet50 FPN',\n",
    "    'retinanet_resnet_v2': 'RetinaNet ResNet50 FPN V2',\n",
    "    'faster_rcnn_resnet': 'Faster R-CNN ResNet50 FPN',\n",
    "    'faster_rcnn_resnet_v2': 'Faster R-CNN ResNet50 FPN V2',\n",
    "    'faster_rcnn_mobilenet_v3': 'Faster R-CNN MobileNet V3 L',\n",
    "    'faster_rcnn_mobilenet_v3_320': 'Faster R-CNN MobileNet V3 L 320',\n",
    "    'mask_rcnn_resnet': 'Mask R-CNN ResNet50',\n",
    "    'mask_rcnn_resnet_v2': 'Mask R-CNN ResNet50 FPN V2',\n",
    "    'ssd_vgg16': 'SSD VGG16',\n",
    "    'ssd_mobilenet_v3': 'SSDLite MobileNet V3 Large',\n",
    "    'yolov5nu': 'YOLOv5nu',\n",
    "    'yolov5su': 'YOLOv5su',\n",
    "    'yolov5mu': 'YOLOv5mu',\n",
    "    'yolov5lu': 'YOLOv5lu',\n",
    "    'yolov5xu': 'YOLOv5xu',\n",
    "    'yolov5n6u': 'YOLOv5n6u',\n",
    "    'yolov5s6u': 'YOLOv5s6u',\n",
    "    'yolov5m6u': 'YOLOv5m6u',\n",
    "    'yolov5l6u': 'YOLOv5l6u',\n",
    "    'yolov5x6u': 'YOLOv5x6u',\n",
    "    'yolov8n': 'YOLOv8n',\n",
    "    'yolov8s': 'YOLOv8s',\n",
    "    'yolov8m': 'YOLOv8m',\n",
    "    'yolov8l': 'YOLOv8l',\n",
    "    'yolov8x': 'YOLOv8x',\n",
    "    'yolov9c': 'YOLOv9c',\n",
    "    'yolov9e': 'YOLOv9e'\n",
    "}\n",
    "\n",
    "# Sort data by average precision in descending order\n",
    "data_sorted = sorted(data, key=lambda x: float(x[1]), reverse=True)\n",
    "\n",
    "# Generate LaTeX table\n",
    "latex_table = \"\\\\begin{table}[H]\\n\\\\centering\\n\\\\begin{tabular}{lrr}\\n\\\\toprule\\n\"\n",
    "latex_table += \"\\\\multicolumn{3}{c}{A100 Experiment 1a: Vehicle Detection Models Performance}\\\\\\\\ \\\\cmidrule{1-3}\\n\"\n",
    "latex_table += \"Model & Average Precision (\\\\%) & Average Inference Speed (s)\\\\\\\\\\n\\\\midrule\\n\"\n",
    "\n",
    "best_prec_index = 0\n",
    "best_speed = [99999.99, 0]\n",
    "for i in range(len(data_sorted)):\n",
    "    if float(data_sorted[i][2])<best_speed[0]:\n",
    "        best_speed = [float(data_sorted[i][2]), i]\n",
    "\n",
    "count = 0\n",
    "for entry in data_sorted:\n",
    "    model = latex_model_names[entry[0]]\n",
    "    avg_precision = float(entry[1])\n",
    "    avg_speed = float(entry[2])\n",
    "\n",
    "    if count==best_prec_index:\n",
    "        if count==best_speed[1]:\n",
    "            latex_table += f\"\\\\textbf{{{model}}} & \\\\textbf{{{avg_precision:.2f}}} & \\\\textbf{{{avg_speed:.5f}}} \\\\\\\\ \\\\addlinespace\\n\"\n",
    "        else:\n",
    "            latex_table += f\"\\\\textbf{{{model}}} & \\\\textbf{{{avg_precision:.2f}}} & {avg_speed:.5f} \\\\\\\\ \\\\addlinespace\\n\"\n",
    "    else:\n",
    "        if count==best_speed[1]:\n",
    "            latex_table += f\"\\\\textbf{{{model}}} & {avg_precision:.2f} & \\\\textbf{{{avg_speed:.5f}}} \\\\\\\\ \\\\addlinespace\\n\"\n",
    "        else:\n",
    "            latex_table += f\"{model} & {avg_precision:.2f} & {avg_speed:.5f} \\\\\\\\ \\\\addlinespace\\n\"\n",
    "\n",
    "    count+=1\n",
    "\n",
    "latex_table += \"\\\\bottomrule\\n\\\\end{tabular}\\n\\\\caption{A100 Experiment 1a: Vehicle Detection Models Performance}\\n\\\\label{table:a100_experiment_1a_vehicle_detection_performance}\\n\\\\end{table}\"\n",
    "\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5492d9e8-bd3f-4842-85cc-5ffc09681dc4",
   "metadata": {},
   "source": [
    "# This is the source code for OCR implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ea7c76-0198-4337-a115-40b221a95d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import IPython\n",
    "import numpy as np\n",
    "from typing import Tuple, Union\n",
    "import math\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import traceback\n",
    "import re\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from deskew import determine_skew\n",
    "\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import easyocr\n",
    "reader = easyocr.Reader(['en', 'de'])\n",
    "\n",
    "from ultralytics import YOLO\n",
    "yolo_model = YOLO('yolov8x.pt')\n",
    "# plate_model = YOLO('best_plate_model.pt')\n",
    "plate_model = YOLO('best_3.pt')\n",
    "\n",
    "detected_plates = []\n",
    "\n",
    "# Install this from \"https://github.com/UB-Mannheim/tesseract/wiki\"\n",
    "pytesseract.pytesseract.tesseract_cmd = 'C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe'\n",
    "\n",
    "target_classes = [\"truck\", \"bus\", \"car\", \"train\", \"bicycle\"]\n",
    "\n",
    "characters = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "# You can also clone this and try it like I did too: https://github.com/clovaai/deep-text-recognition-benchmark\n",
    "\n",
    "# get grayscale image\n",
    "def get_grayscale(image):\n",
    "    return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# noise removal\n",
    "def remove_noise(image):\n",
    "    return cv2.medianBlur(image,5)\n",
    "\n",
    "#thresholding\n",
    "def thresholding(image):\n",
    "    return cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "\n",
    "#dilation\n",
    "def dilate(image):\n",
    "    kernel = np.ones((5,5),np.uint8)\n",
    "    return cv2.dilate(image, kernel, iterations = 1)\n",
    "\n",
    "#erosion\n",
    "def erode(image):\n",
    "    kernel = np.ones((5,5),np.uint8)\n",
    "    return cv2.erode(image, kernel, iterations = 1)\n",
    "\n",
    "#opening - erosion followed by dilation\n",
    "def opening(image):\n",
    "    kernel = np.ones((5,5),np.uint8)\n",
    "    return cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "#canny edge detection\n",
    "def canny(image):\n",
    "    return cv2.Canny(image, 100, 200)\n",
    "\n",
    "#skew correction\n",
    "def deskew(image):\n",
    "    coords = np.column_stack(np.where(image > 0))\n",
    "    angle = cv2.minAreaRect(coords)[-1]\n",
    "    if angle < -45:\n",
    "        angle = -(90 + angle)\n",
    "    else:\n",
    "        angle = -angle\n",
    "    (h, w) = image.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    rotated = cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
    "    return rotated\n",
    "\n",
    "def compute_skew(src_img):\n",
    "    if len(src_img.shape) == 3:\n",
    "        h, w, _ = src_img.shape\n",
    "    elif len(src_img.shape) == 2:\n",
    "        h, w = src_img.shape\n",
    "    else:\n",
    "        print('unsupported image type')\n",
    "    img = cv2.medianBlur(src_img, 3)\n",
    "    edges = cv2.Canny(img,  threshold1 = 30,  threshold2 = 100, apertureSize = 3, L2gradient = True)\n",
    "    lines = cv2.HoughLinesP(edges, 1, math.pi/180, 30, minLineLength=w / 4.0, maxLineGap=h/4.0)\n",
    "    angle = 0.0\n",
    "    nlines = lines.size\n",
    "    #print(nlines)\n",
    "    cnt = 0\n",
    "    for x1, y1, x2, y2 in lines[0]:\n",
    "        ang = np.arctan2(y2 - y1, x2 - x1)\n",
    "        #print(ang)\n",
    "        if math.fabs(ang) <= 30: # excluding extreme rotations\n",
    "            angle += ang\n",
    "            cnt += 1\n",
    "    if cnt == 0:\n",
    "        return 0.0\n",
    "    return (angle / cnt)*180/math.pi\n",
    "\n",
    "def rotate_image(image, angle):\n",
    "    image_center = tuple(np.array(image.shape[1::-1]) / 2)\n",
    "    rot_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n",
    "    result = cv2.warpAffine(image, rot_mat, image.shape[1::-1], flags=cv2.INTER_LINEAR)\n",
    "    return result\n",
    "\n",
    "def get_deskew_angle(image):\n",
    "    coords = np.column_stack(np.where(image > 0))\n",
    "    angle = cv2.minAreaRect(coords)[-1]\n",
    "    if angle<-45: angle=-angle\n",
    "    return angle\n",
    "\n",
    "def deskew_image(image, angle):\n",
    "    (h, w) = image.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    return cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
    "\n",
    "#template matching\n",
    "def match_template(image, template):\n",
    "    return cv2.matchTemplate(image, template, cv2.TM_CCOEFF_NORMED)\n",
    "\n",
    "def rotate(\n",
    "        image: np.ndarray, angle: float, background: Union[int, Tuple[int, int, int]]\n",
    ") -> np.ndarray:\n",
    "    old_width, old_height = image.shape[:2]\n",
    "    angle_radian = math.radians(angle)\n",
    "    width = abs(np.sin(angle_radian) * old_height) + abs(np.cos(angle_radian) * old_width)\n",
    "    height = abs(np.sin(angle_radian) * old_width) + abs(np.cos(angle_radian) * old_height)\n",
    "\n",
    "    image_center = tuple(np.array(image.shape[1::-1]) / 2)\n",
    "    rot_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n",
    "    rot_mat[1, 2] += (width - old_width) / 2\n",
    "    rot_mat[0, 2] += (height - old_height) / 2\n",
    "    return cv2.warpAffine(image, rot_mat, (int(round(height)), int(round(width))), borderValue=background)\n",
    "\n",
    "# NN Layer License Plate Recognition \"OCR\"\n",
    "def process_license(processed_frame, bounding_box, name, confidence, parentName, parentConfidence):\n",
    "    x1, y1, x2, y2 = [int(x) for x in bounding_box]\n",
    "    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(255,0,255),2)\n",
    "    cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1-15),0,0.9,(255,0,255),2)\n",
    "\n",
    "    # Rotation\n",
    "    # license_frame = deskew(processed_frame[y1:y2,x1:x2])\n",
    "    \n",
    "    license_frame = processed_frame[y1:y2,x1:x2]\n",
    "\n",
    "    grayscaled = get_grayscale(license_frame) # get_grayscale(np.array(license_frame))\n",
    "    # angle = determine_skew(grayscaled)\n",
    "    threshholded = thresholding(grayscaled) # deskew_image(thresholding(grayscaled), angle)\n",
    "\n",
    "    # newdata=pytesseract.image_to_osd(threshholded)\n",
    "    # angle=float(re.search('(?<=Rotate: )\\d+', newdata).group(0))\n",
    "    # print('osd angle:',angle)\n",
    "\n",
    "    # angle = compute_skew(thresholding(grayscaled))\n",
    "    # threshholded = rotate_image(thresholding(grayscaled), angle)\n",
    "    # dilated = dilate(threshholded)\n",
    "    # plt.imshow(deskew(threshholded))\n",
    "\n",
    "    # print(angle)\n",
    "    # plt.imshow(threshholded)\n",
    "    # plt.show()\n",
    "\n",
    "    max_conf = 0.0\n",
    "    best_text = \"\"\n",
    "\n",
    "    results = pytesseract.image_to_data(threshholded, config='-c tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n",
    "    # NN Layer License Plate Recognition \"Pytesseract OCR\"\n",
    "\n",
    "    parsedLines = results.split('\\n')\n",
    "    best_depth = len(parsedLines)-2\n",
    "    for line in parsedLines:\n",
    "        params = line.split()\n",
    "        if len(params)==12 and float(params[10].replace('conf','0.0'))>=max_conf:\n",
    "            max_conf = float(params[10].replace('conf','0.0'))\n",
    "            best_text = params[11] if params[11]!=\"text\" else \"-\"\n",
    "            if max_conf>=0.0 and len(best_text)>0:\n",
    "                detected_plates.append([parentConfidence*100, parentName.upper(), confidence*100, name.upper(), max_conf, best_text, \"pytesseract\"])\n",
    "    cv2.putText(processed_frame, '{} {:.2f}%'.format(\"[tesse]: \"+best_text, max_conf),(x1+10,y2+25),0,0.9,(0,0,255),3)\n",
    "\n",
    "    result = reader.readtext(threshholded)\n",
    "    text = \"\"\n",
    "    conf = 0.0\n",
    "    for res in result:\n",
    "        if res[2]>conf:\n",
    "            conf=res[2]\n",
    "            text=res[1]\n",
    "            if res[2]*100>max_conf:\n",
    "                max_conf = res[2]*100\n",
    "                best_text = res[1]\n",
    "                if max_conf>=0.0 and len(best_text)>0:\n",
    "                    detected_plates.append([parentConfidence*100, parentName.upper(), confidence*100, name.upper(), max_conf, best_text, \"easyocr\"])\n",
    "    cv2.putText(processed_frame, '{} {:.2f}%'.format(\"[easy]: \"+str(text), conf*100),(x1+10,y2+57),0,0.9,(200,200,200),3)\n",
    "\n",
    "# NN Layer Vehicle Found -> License Detection\n",
    "def process_vehicle(processed_frame, bounding_box, name, confidence):\n",
    "\n",
    "    # Yolov8n Bounding Boxes\n",
    "    x1, y1, x2, y2 = [int(x) for x in bounding_box]\n",
    "    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(0,255,255),2)\n",
    "\n",
    "    cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1+25),0,0.8,(0,255,255),2)\n",
    "    vehicle_frame = processed_frame[y1:y2,x1:x2]\n",
    "    results = plate_model(vehicle_frame, agnostic_nms=True, verbose=False)[0]\n",
    "    for result in results:\n",
    "        detection_count = result.boxes.shape[0]\n",
    "        for i in range(detection_count):\n",
    "            cls = int(result.boxes.cls[i].item())\n",
    "            name2 = result.names[cls]\n",
    "            confidence2 = float(result.boxes.conf[i].item())\n",
    "            bounding_box2 = result.boxes.xyxy[i].cpu().numpy()\n",
    "            a1, b1, a2, b2 = [int(x) for x in bounding_box2]\n",
    "            # Ensure a1 < a2 and b1 < b2\n",
    "            a1, a2 = min(a1, a2), max(a1, a2)\n",
    "            b1, b2 = min(b1, b2), max(b1, b2)\n",
    "            # if name2==\"License_Plate\": process_license(processed_frame, [x1+a1, y1+b1, x1+a2, y1+b2], name2, confidence2, name, confidence)\n",
    "            process_license(processed_frame, [x1+a1, y1+b1, x1+a2, y1+b2], name2, confidence2, name, confidence)\n",
    "\n",
    "# NN Layer Other Objects Detected [Not target classes]\n",
    "def process_detection(processed_frame, bounding_box, name, confidence):\n",
    "    x1, y1, x2, y2 = [int(x) for x in bounding_box]\n",
    "    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(255,255,0),2)\n",
    "    cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1+25),0,0.8,(255,255,0),2)\n",
    "\n",
    "# NN Layer Yolov8 Detection\n",
    "def yolov8n_objects(frame):\n",
    "    processed_frame = frame\n",
    "    start_time = time.time()\n",
    "    results = yolo_model(processed_frame, agnostic_nms=True, verbose=False)[0]\n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "    # print(f\"Processing time yolov8: {processing_time:.3f} seconds\")  # Output processing time\n",
    "    for result in results:\n",
    "        detection_count = result.boxes.shape[0]\n",
    "        for i in range(detection_count):\n",
    "            cls = int(result.boxes.cls[i].item())\n",
    "            name = result.names[cls]\n",
    "            confidence = float(result.boxes.conf[i].item())\n",
    "            bounding_box = result.boxes.xyxy[i].cpu().numpy()\n",
    "            x1, y1, x2, y2 = [int(x) for x in bounding_box]\n",
    "            # Ensure x1 < x2 and y1 < y2\n",
    "            x1, x2 = min(x1, x2), max(x1, x2)\n",
    "            y1, y2 = min(y1, y2), max(y1, y2)\n",
    "            if name in target_classes: process_vehicle(processed_frame, bounding_box, name, confidence)\n",
    "            else: process_detection(processed_frame, bounding_box, name, confidence)\n",
    "    return processed_frame\n",
    "\n",
    "def process_frame(frame):\n",
    "    processed_frame = yolov8n_objects(frame)\n",
    "    # processed_frame = cv2.flip(processed_frame, 1)\n",
    "    return processed_frame\n",
    "\n",
    "def handleFrame(frame):\n",
    "    processed_frame = process_frame(frame)\n",
    "    _, processed_frame = cv2.imencode('.jpeg', processed_frame)\n",
    "    display_handle.update(IPython.display.Image(data=processed_frame.tobytes()))\n",
    "    IPython.display.clear_output(wait=True)\n",
    "\n",
    "def handleRelease():\n",
    "    cam.release()\n",
    "    print(\"Source released.\")\n",
    "    correct_plates = []\n",
    "    better_plates = []\n",
    "    best_plate = []\n",
    "    most_repeated_plates = []\n",
    "    best_repeated_plates = []\n",
    "    highest_score = 0.0\n",
    "    if len(detected_plates)>0:\n",
    "        repetitions = {}\n",
    "        count, item = 0, ''\n",
    "        for plate in detected_plates:\n",
    "            if plate[0]>50.0 and plate[2]>50.0 and plate[4]>50.0: correct_plates.append(plate)\n",
    "            if plate[0]>70.0 and plate[2]>70.0 and plate[4]>70.0: better_plates.append(plate)\n",
    "            if plate[0] + plate[2] + plate[4] > highest_score:\n",
    "                highest_score = plate[0] + plate[2] + plate[4]\n",
    "                best_plate = plate\n",
    "            repetitions[plate[5]] = repetitions.get(plate[5], 0) + 1\n",
    "            if repetitions[plate[5]]>count: count, item = repetitions[plate[5]], plate[5]\n",
    "        if len(repetitions.keys())>0:\n",
    "            for itm in repetitions.keys():\n",
    "                if repetitions[itm]==count:\n",
    "                    repeated_detected = [plate for plate in detected_plates if plate[5]==itm]\n",
    "                    best_percentages = []\n",
    "                    for plate in repeated_detected:\n",
    "                        if len(best_percentages)==0: best_percentages=plate\n",
    "                        elif (plate[0] + plate[2] + plate[4])/3.0>(best_percentages[0] + best_percentages[2] + best_percentages[4])/3.0: best_percentages=plate\n",
    "                    most_repeated_plates.append(best_percentages)\n",
    "        if len(most_repeated_plates)>0:\n",
    "            for plate in most_repeated_plates:\n",
    "                best_repeated = []\n",
    "                if len(best_repeated)==0: best_repeated=plate\n",
    "                elif (plate[0] + plate[2] + plate[4])/3.0>(best_repeated[0] + best_repeated[2] + best_repeated[4])/3.0: best_repeated=plate\n",
    "            best_repeated_plates.append(best_repeated)\n",
    "    print(\"\\nDetected Plates: \\n\", np.matrix(detected_plates))\n",
    "    print(\"\\nCorrectly Detected Plates: \\n\", np.matrix(correct_plates))\n",
    "    print(\"\\nBetter Detected Plates: \\n\", np.matrix(better_plates))\n",
    "    print(\"\\nBest Detected Plate: \\n\", np.matrix(best_plate))\n",
    "    print(\"\\nMost Repeated Plates: \\n\", np.matrix(most_repeated_plates))\n",
    "    print(\"\\nBest Repeated Plate: \\n\", np.matrix(best_repeated_plates))\n",
    "\n",
    "inputType=\"video\"\n",
    "# cam = cv2.VideoCapture(0)\n",
    "source = \"data/video/sample.MP4\" if inputType==\"video\" else 0\n",
    "cam = cv2.VideoCapture(source)\n",
    "display_handle=display(None, display_id=True)\n",
    "every = 60\n",
    "\n",
    "start = 0 if source!=0 else -1\n",
    "end = int(cam.get(cv2.CAP_PROP_FRAME_COUNT)) if source!=0 else -1\n",
    "if source!=0: cam.set(1, start)\n",
    "\n",
    "frameCount=start\n",
    "while_safety=0\n",
    "saved_count=0\n",
    "\n",
    "if not cam.isOpened(): print(\"Error: Could not open source.\")\n",
    "else:\n",
    "    try:\n",
    "        while True if end<0 else frameCount<end:\n",
    "            _, frame = cam.read()\n",
    "            if frame is None:\n",
    "                if source!=0:\n",
    "                    if while_safety > 2000: break\n",
    "                    while_safety += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    print(\"Error: Could not capture frame.\")\n",
    "                    cam.release()\n",
    "                    break\n",
    "\n",
    "            if every>0:\n",
    "                if (frameCount+1)%math.floor(every) == 0:\n",
    "                    while_safety = 0\n",
    "                    handleFrame(frame)\n",
    "            else:\n",
    "                while_safety=0\n",
    "                handleFrame(frame)\n",
    "            frameCount += 1\n",
    "        handleRelease()\n",
    "    except KeyboardInterrupt:\n",
    "        handleRelease()\n",
    "    except:\n",
    "        print(\"Unknown error.\")\n",
    "        try:\n",
    "            cam.release()\n",
    "            raise TypeError(\"Error: Source could not be released.\")\n",
    "        except:\n",
    "            pass\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0295a24d-3b67-4980-9456-923b1ae08e92",
   "metadata": {},
   "source": [
    "# This code uses the model detection to recognize license plate characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33ee94b-802a-46d4-9ba0-f885db4b758c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import IPython\n",
    "import numpy as np\n",
    "from typing import Tuple, Union\n",
    "import math\n",
    "from PIL import Image\n",
    "import traceback\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "from ultralytics import YOLO\n",
    "model = YOLO('yolov8x.pt')\n",
    "plate_model = YOLO('vehicle_license_best.pt')\n",
    "characters_model = YOLO('plate_char/yolov8x/weights/best.pt')\n",
    "\n",
    "detected_plates = []\n",
    "\n",
    "target_classes = [\"truck\", \"bus\", \"car\", \"train\", \"bicycle\"]\n",
    "\n",
    "# NN Layer License Plate Recognition \"CNN\"\n",
    "def process_license(processed_frame, bounding_box, name, confidence, parentName, parentConfidence):\n",
    "    x1, y1, x2, y2 = [int(x) for x in bounding_box]\n",
    "    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(255,0,255),2)\n",
    "    cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1-15),0,0.7,(255,0,255))\n",
    "\n",
    "    license_frame = processed_frame[y1:y2,x1:x2]\n",
    "\n",
    "    plate_parts=[]\n",
    "    plate_text=\"\"\n",
    "\n",
    "    results = characters_model(license_frame, agnostic_nms=True, verbose=False, device=device)[0]\n",
    "    for result in results:\n",
    "        detection_count = result.boxes.shape[0]\n",
    "        for i in range(detection_count):\n",
    "            cls = int(result.boxes.cls[i].item())\n",
    "            name2 = result.names[cls]\n",
    "            confidence2 = float(result.boxes.conf[i].item())\n",
    "            bounding_box2 = result.boxes.xyxy[i].cpu().numpy()\n",
    "            a1, b1, a2, b2 = [int(x) for x in bounding_box2]\n",
    "            # Ensure a1 < a2 and b1 < b2\n",
    "            a1, a2 = min(a1, a2), max(a1, a2)\n",
    "            b1, b2 = min(b1, b2), max(b1, b2)\n",
    "            if confidence2>0.5:\n",
    "                cv2.rectangle(processed_frame,(x1+a1,y1+b1),(x1+a2,y1+b2),(0,0,255),1)\n",
    "                cv2.putText(processed_frame, '{}'.format(name2),(x1+a1,y1+b1+75),0,0.9,(0,0,255),2)\n",
    "                cv2.putText(processed_frame, '{:.2f}%'.format(confidence2*100),(x1+a1,y1+b1+100+25*i),0,0.7,(0,0,255),2)\n",
    "                plate_parts.append([a1,b1,name2])\n",
    "\n",
    "    sorted_list = sorted(plate_parts,key=lambda l:l[0])\n",
    "    for part in range(len(sorted_list)):\n",
    "        if sorted_list[part][2]!=\"undefined\":\n",
    "            plate_text+=sorted_list[part][2]\n",
    "    detected_plates.append(plate_text)\n",
    "\n",
    "# NN Layer Vehicle Found -> License Detection\n",
    "def process_vehicle(processed_frame, bounding_box, name, confidence):\n",
    "    x1, y1, x2, y2 = [int(x) for x in bounding_box]\n",
    "    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(0,255,255),2)\n",
    "    cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1-15),0,0.7,(0,255,255))\n",
    "    vehicle_frame = processed_frame[y1:y2,x1:x2]\n",
    "    results = plate_model(vehicle_frame, agnostic_nms=True, verbose=False, device=device)[0]\n",
    "    for result in results:\n",
    "        detection_count = result.boxes.shape[0]\n",
    "        for i in range(detection_count):\n",
    "            cls = int(result.boxes.cls[i].item())\n",
    "            name2 = result.names[cls]\n",
    "            confidence2 = float(result.boxes.conf[i].item())\n",
    "\n",
    "            if confidence2 > 0.5:\n",
    "                bounding_box2 = result.boxes.xyxy[i].cpu().numpy()\n",
    "                a1, b1, a2, b2 = [int(x) for x in bounding_box2]\n",
    "                # Ensure a1 < a2 and b1 < b2\n",
    "                a1, a2 = min(a1, a2), max(a1, a2)\n",
    "                b1, b2 = min(b1, b2), max(b1, b2)\n",
    "                # if name2==\"License_Plate\":\n",
    "                process_license(processed_frame, [x1+a1, y1+b1, x1+a2, y1+b2], name2, confidence2, name, confidence)\n",
    "\n",
    "# NN Layer Other Objects Detected\n",
    "def process_detection(processed_frame, bounding_box, name, confidence):\n",
    "    x1, y1, x2, y2 = [int(x) for x in bounding_box]\n",
    "    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(255,255,0),2)\n",
    "    cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1-15),0,0.7,(255,255,0))\n",
    "\n",
    "# NN Layer Yolov8 Detection\n",
    "def process_frame(frame):\n",
    "    processed_frame = frame\n",
    "    # processed_frame = cv2.flip(processed_frame, 1)\n",
    "    results = model(processed_frame, agnostic_nms=True, verbose=False, device=device)[0]\n",
    "    for result in results:\n",
    "        detection_count = result.boxes.shape[0]\n",
    "        for i in range(detection_count):\n",
    "            cls = int(result.boxes.cls[i].item())\n",
    "            name = result.names[cls]\n",
    "            confidence = float(result.boxes.conf[i].item())\n",
    "            bounding_box = result.boxes.xyxy[i].cpu().numpy()\n",
    "            x1, y1, x2, y2 = [int(x) for x in bounding_box]\n",
    "            # Ensure x1 < x2 and y1 < y2\n",
    "            x1, x2 = min(x1, x2), max(x1, x2)\n",
    "            y1, y2 = min(y1, y2), max(y1, y2)\n",
    "            if name in target_classes: process_vehicle(processed_frame, bounding_box, name, confidence)\n",
    "            else: process_detection(processed_frame, bounding_box, name, confidence)\n",
    "    return processed_frame\n",
    "\n",
    "def handleFrame(frame):\n",
    "    processed_frame = process_frame(frame)\n",
    "    _, processed_frame = cv2.imencode('.jpeg', processed_frame)\n",
    "    display_handle.update(IPython.display.Image(data=processed_frame.tobytes()))\n",
    "    IPython.display.clear_output(wait=True)\n",
    "\n",
    "def handleRelease():\n",
    "    cam.release()\n",
    "    print(\"Source released.\")\n",
    "    print()\n",
    "    print(detected_plates)\n",
    "    # These can be trimmed with empty detections and then compared by most repeated to get the correct one\n",
    "\n",
    "inputType=\"video\"\n",
    "# cam = cv2.VideoCapture(0)\n",
    "source = \"data/video/sample.MP4\" if inputType==\"video\" else 0\n",
    "cam = cv2.VideoCapture(source)\n",
    "display_handle=display(None, display_id=True)\n",
    "every = 60\n",
    "\n",
    "start = 0 if source!=0 else -1\n",
    "end = int(cam.get(cv2.CAP_PROP_FRAME_COUNT)) if source!=0 else -1\n",
    "if source!=0: cam.set(1, start)\n",
    "\n",
    "frameCount=start\n",
    "while_safety=0\n",
    "saved_count=0\n",
    "\n",
    "if not cam.isOpened(): print(\"Error: Could not open source.\")\n",
    "else:\n",
    "    try:\n",
    "        while True if end<0 else frameCount<end:\n",
    "            _, frame = cam.read()\n",
    "            if frame is None:\n",
    "                if source!=0:\n",
    "                    if while_safety > 2000: break\n",
    "                    while_safety += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    print(\"Error: Could not capture frame.\")\n",
    "                    cam.release()\n",
    "                    break\n",
    "\n",
    "            if every>0:\n",
    "                if (frameCount+1)%math.floor(every) == 0:\n",
    "                    while_safety = 0\n",
    "                    handleFrame(frame)\n",
    "            else:\n",
    "                while_safety=0\n",
    "                handleFrame(frame)\n",
    "            frameCount += 1\n",
    "        handleRelease()\n",
    "    except KeyboardInterrupt:\n",
    "        handleRelease()\n",
    "    except:\n",
    "        print(\"Unknown error.\")\n",
    "        try:\n",
    "            cam.release()\n",
    "            raise TypeError(\"Error: Source could not be released.\")\n",
    "        except:\n",
    "            pass\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7274ca97-4953-4f37-96c0-e9a875690534",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
