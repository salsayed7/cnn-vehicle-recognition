


# Magical reload for modules
%load_ext autoreload
%reload_ext autoreload
%autoreload 2


# Video Frames Extractor (First Try One-Threaded)
import OneThreaded
import importlib
importlib.reload(OneThreaded)

export_nth_frame = 300
video_file = "data/video/sample.MP4"
frames_folder = "data/frames"

OneThreaded.extractFrames(pathIn=video_file, pathOut=frames_folder, every=export_nth_frame)


# Faster Video Frames Extractor for Real-Time Performance (Multi-Threaded)
import MultiThreaded
import importlib
importlib.reload(MultiThreaded)

export_nth_frame = 300
video_file = "data/video/sample.MP4"
frames_folder = "data/frames"
overwrite = False

MultiThreaded.extractFrames(video_path=video_file, frames_dir=frames_folder, overwrite=overwrite, every=export_nth_frame)


# Open Camera and Show Feed (slow fps, picture for picture)

# pip install opencv-python
# pip install matplotlib
import cv2
import matplotlib
import IPython
print("CV2 Version", cv2.__version__)
print("MatPlotLib Version", matplotlib.__version__)
print("IPython Version", IPython.__version__)

def cv2_imshow(cv2image):
    matplotlib.pyplot.imshow(cv2.cvtColor(cv2.flip(cv2image, 1), cv2.COLOR_BGR2RGB))
    matplotlib.pyplot.show()

camera = cv2.VideoCapture(0)

try:
    if camera.isOpened():
        ret, frame = camera.read()
        if ret:
            while ret:
                IPython.display.clear_output(wait=True)
                cv2_imshow(frame)
                ret, frame = camera.read()
        else: print("Error: Failed to capture frame!")
    else: print("Error: Failed to open camera!")
except KeyboardInterrupt:
    pass
finally:
    camera.release()
    IPython.display.clear_output()


# Open Camera and Show Feed (better fps)
import cv2
from IPython.display import display, Image

video = cv2.VideoCapture(0)
display_handle=display(None, display_id=True)
try:
    while True:
        _, frame = video.read()
        frame = cv2.flip(frame, 1)
        _, frame = cv2.imencode('.jpeg', frame)
        display_handle.update(Image(data=frame.tobytes()))
except KeyboardInterrupt:
    pass
finally:
    video.release()
    display_handle.update(None)


# Neuronal Network Layered Stream with the better fps

# pip install opencv-python numpy pillow
import cv2
import IPython
# import PIL.Image

# NN Layer Dummy Grayscale and Flip
def process_frame(frame):
    processed_frame = frame
    processed_frame = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2GRAY)
    processed_frame = cv2.flip(processed_frame, 1)
    return processed_frame

cam = cv2.VideoCapture(0)
display_handle=display(None, display_id=True)

if not cam.isOpened(): print("Error: Could not open camera.")
else:
    try:
        while True:
            ret, frame = cam.read()
            if not ret:
                print("Error: Couldn't capture frame.")
                break
            processed_frame = process_frame(frame)
            processed_frame = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)
            _, processed_frame = cv2.imencode('.jpeg', processed_frame)
            display_handle.update(IPython.display.Image(data=processed_frame.tobytes()))
            # img = PIL.Image.fromarray(rgb_frame)
            # IPython.display.display(img)
            IPython.display.clear_output(wait=True)
    except KeyboardInterrupt:
        cam.release()
        print("Camera released.")


import cv2
import torch
from torchvision.models.detection import *
from torchvision.io.image import read_image
from torchvision.utils import draw_bounding_boxes
from torchvision.transforms.functional import to_pil_image
import torchvision.transforms as transforms
from PIL import Image
import IPython
from IPython.display import display, clear_output
import time
import traceback
import math
from ultralytics import YOLO
import matplotlib.pyplot as plt
import matplotlib

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

transform = transforms.Compose([
    #transforms.Resize((300, 300)),
    transforms.ToTensor(),
])

class DetectionModel():
    def __init__(self, concept, backbone, model, weights, reference, color=None):
        self.concept = concept
        self.backbone = backbone
        self.model = model(weights) if concept=="YOLO" else model(weights=weights.DEFAULT)
        self.weights = weights
        self.reference = reference
        self.color = color
        self.detections = []

    def addDetection(self, detection):
        self.detections.append(detection)

    def getDetections(self):
        return self.detections

    def clearDetections(self):
        self.detections = []

    def detectionsCount(self):
        return len(self.detections)

    def inferencesCount(self):
        length = 0
        addedFrames = []
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    addedFrames.append(self.detections[i].frame)
        return len(addedFrames)

    def getAveragePrecision(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].precision
            avg = sum/len(self.detections)
        return  avg.item() if torch.is_tensor(avg) else avg

    def getMaxPrecision(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].precision:
                max=self.detections[i].precision
        return max.item() if torch.is_tensor(max) else max

    def getMinPrecision(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].precision:
                min=self.detections[i].precision
        return 0.0 if min==9999999.99 else min.item() if torch.is_tensor(min) else min

    def getAverageSpeed(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].speed
            avg = sum/len(self.detections)
        return avg

    def getMaxSpeed(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].speed:
                max=self.detections[i].speed
        return max

    def getMinSpeed(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].speed:
                min=self.detections[i].speed
        return 0.0 if min==9999999.99 else min

    def getAverageInferenceSpeed(self):
        sum=0.0
        avg=0.0
        addedFrames=[]
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    sum+=self.detections[i].speed
                    addedFrames.append(self.detections[i].frame)
            avg = sum/len(addedFrames) if len(addedFrames)>0 else 0.0
        return avg

    def getMaxInferenceSpeed(self):
        max=0.0
        addedFrames=[]
        for i in range(len(self.detections)):
            if max<self.detections[i].speed and self.detections[i].frame not in addedFrames:
                max=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return max

    def getMinInferenceSpeed(self):
        min=9999999.99
        addedFrames=[]
        for i in range(len(self.detections)):
            if min>self.detections[i].speed and self.detections[i].frame not in addedFrames:
                min=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return 0.0 if min==9999999.99 else min

    def filterPrecisionsBelow(self, minPrecision):
        newDetections = []
        for i in range(len(self.detections)):
            if self.detections[i].precision>=minPrecision:
                newDetections.append(self.detections[i])
        self.detections = newDetections

    def printResults(self):
        print("Model:", self.reference,
              "\nBest Precision:", self.getMaxPrecision(),
              "%\nAverage Precision:", self.getAveragePrecision(),
              "%\nWorst Precision:", self.getMinPrecision(),
              "%\nBest Speed:", self.getMinSpeed(),
              "seconds\nAverage Speed:", self.getAverageSpeed(),
              "seconds\nWorst Speed:", self.getMaxSpeed(),
              "seconds\nBest Inference Speed:", self.getMinInferenceSpeed(),
              "seconds\nAverage Inference Speed:", self.getAverageInferenceSpeed(),
              "seconds\nWorst Inference Speed:", self.getMaxInferenceSpeed(),
              "seconds\nTotal Detections:", self.detectionsCount(),
              "\nTotal Inferences:", self.inferencesCount(),
              "\n\n")

class DetectionResult():
    def __init__(self, model, label, precision, speed, device=None, source=None, frame=None, boundingBox=None, details=None):
        self.model = model
        self.label = label
        self.precision = precision.item() if torch.is_tensor(precision) else precision
        self.speed = speed.item() if torch.is_tensor(speed) else speed
        self.device = device
        self.source = source
        self.frame = frame
        self.boundingBox = boundingBox
        self.details = details

    def printRes(self):
        print("Model:", self.model,
              "\nPrecision:", self.precision,
              "%\nSpeed:", self.speed,
              "seconds\nDevice:", self.device,
              "\nSource:", self.source,
              "\nDetails:", self.details,
              "\n\n")

target_classes = ["truck", "bus", "car", "train", "bicycle"]


VEHICLE_DETECTION_MODELS = [
    DetectionModel("FCOS", "resnet50_fpn", fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights, "fcos_resnet", (255,0,0)),
    DetectionModel("RetinaNet", "resnet50_fpn", retinanet_resnet50_fpn, RetinaNet_ResNet50_FPN_Weights, "retinanet_resnet", (255,255,0)),
    DetectionModel("RetinaNet", "resnet50_fpn_v2", retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights, "retinanet_resnet_v2", (0,234,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn", fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights, "faster_rcnn_resnet", (170,0,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn_v2", fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights, "faster_rcnn_resnet_v2", (255,127,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_fpn", fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights, "faster_rcnn_mobilenet_v3", (191,255,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_320_fpn", fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights, "faster_rcnn_mobilenet_v3_320", (0,149,255)),
    DetectionModel("MaskRCNN", "resnet50_fpn", maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights, "mask_rcnn_resnet", (255,0,170)),
    DetectionModel("MaskRCNN", "resnet50_fpn_v2", maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights, "mask_rcnn_resnet_v2", (255,212,0)),
    DetectionModel("SSD300", "vgg16", ssd300_vgg16, SSD300_VGG16_Weights, "ssd_vgg16", (106,255,0)),
    DetectionModel("SSDLite320", "mobilenet_v3_large", ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights, "ssd_mobilenet_v3", (0,64,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5nu.pt', "yolov5nu", (237,185,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5su.pt', "yolov5su", (185,215,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5mu.pt', "yolov5mu", (231,233,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5lu.pt', "yolov5lu", (220,185,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5xu.pt', "yolov5xu", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5n6u.pt', "yolov5n6u", (255,255,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5s6u.pt', "yolov5s6u", (255,0,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5m6u.pt', "yolov5m6u", (255,212,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5l6u.pt', "yolov5l6u", (191,255,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5x6u.pt', "yolov5x6u", (0,149,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8n.pt', "yolov8n", (170,0,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8s.pt', "yolov8s", (0,234,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8m.pt', "yolov8m", (255,255,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8l.pt', "yolov8l", (255,0,0)), # cycle restarts
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8x.pt', "yolov8x", (255,255,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9c.pt', "yolov9c", (0,234,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9e.pt', "yolov9e", (170,0,255))
]

def process(frame, frameIndex=-1, source=None):
    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        processed_frame = frame.copy()

        if currentModel.concept=="YOLO":
            start_time = time.time()
            results = currentModel.model(processed_frame, agnostic_nms=True, verbose=False)[0]
            processing_time = time.time() - start_time
            for result in results:
                detection_count = result.boxes.shape[0]
                for j in range(detection_count):
                    cls = int(result.boxes.cls[j].item())
                    name = result.names[cls]
                    confidence = float(result.boxes.conf[j].item())
                    bounding_box = result.boxes.xyxy[j].cpu().numpy()
                    if name in target_classes:
                        x1, y1, x2, y2 = [int(x) for x in bounding_box]
                        # Ensure x1 < x2 and y1 < y2
                        x1, x2 = min(x1, x2), max(x1, x2)
                        y1, y2 = min(y1, y2), max(y1, y2)
                        label = '{} ({:.2f}%) ({:.3f}s) {}'.format(name.upper(), confidence*100, processing_time, currentModel.reference)
                        print(label)
                        VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, name, confidence*100, processing_time, device, source, frameIndex, bounding_box, label))
                        cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                        cv2.putText(processed_frame, label, (x1+10, y1+25), 0, 0.8, currentModel.color, 2)
        else:
            weights = currentModel.weights.DEFAULT
            model = currentModel.model
            model.to(device)
            model.eval()

            # Convert frame to RGB and PIL Image, then apply transformation
            frame_rgb = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            image = transform(pil_image).unsqueeze(0).to(device)

            # Object detection
            start_time = time.time()
            with torch.no_grad():
                predictions = model(image)

            # Processing time
            processing_time = time.time() - start_time

            # Visualization
            scores = predictions[0]['scores']
            boxes = predictions[0]['boxes']
            labels = predictions[0]['labels']
            confidence_threshold = 0.5

            for score, box, label in zip(scores, boxes, labels):
                # and score > confidence_threshold
                if weights.meta["categories"][label.item()] in target_classes and score > confidence_threshold:
                    x1, y1, x2, y2 = map(int, box)
                    label_text = f'{weights.meta["categories"][label.item()].upper()} ({score*100:.2f}%) ({processing_time:.3f}s) {currentModel.reference}'
                    print(label_text)
                    VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, weights.meta["categories"][label.item()], float(score.item())*100, processing_time, device, source, frameIndex, box, label_text))
                    cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                    cv2.putText(processed_frame, label_text, (x1, y1+20), 0, 0.8, currentModel.color, 2)

        plt.imshow(processed_frame)
        plt.savefig("Analysis/"+currentModel.reference+".png", bbox_inches='tight')
        plt.show()
    return frame

def handleFrame(frame, frameIndex=-1, source=None):
    processed_frame = process(frame,frameIndex,source)

def handleRelease():
    print(device)
    print(source)

    BestPrecision=None
    BestAvgPrecision=None
    WorstPrecision=None
    BestSpeed=None
    BestAvgSpeed=None
    WorstSpeed=None
    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        currentModel.printResults()
        if BestPrecision is None or BestPrecision.getMaxPrecision()<=currentModel.getMaxPrecision():
            BestPrecision = currentModel
        if BestAvgPrecision is None or BestAvgPrecision.getAveragePrecision()<=currentModel.getAveragePrecision():
            BestAvgPrecision = currentModel
        if WorstPrecision is None or WorstPrecision.getMinPrecision()>=currentModel.getMinPrecision():
            WorstPrecision = currentModel
        if BestSpeed is None or BestSpeed.getMinSpeed()>=currentModel.getMinSpeed():
            BestSpeed = currentModel
        if BestAvgSpeed is None or BestAvgSpeed.getAverageSpeed()>=currentModel.getAverageSpeed():
            BestAvgSpeed = currentModel
        if WorstSpeed is None or WorstSpeed.getMaxSpeed()<=currentModel.getMaxSpeed():
            WorstSpeed = currentModel

    if BestPrecision is not None:
        print("Best Precision:")
        BestPrecision.printResults()
    if BestAvgPrecision is not None:
        print("Best Average Precision:")
        BestAvgPrecision.printResults()
    if WorstPrecision is not None:
        print("Worst Precision:")
        WorstPrecision.printResults()
    if BestSpeed is not None:
        print("Best Speed:")
        BestSpeed.printResults()
    if BestAvgSpeed is not None:
        print("Best Average Speed:")
        BestAvgSpeed.printResults()
    if WorstSpeed is not None:
        print("Worst Speed:")
        WorstSpeed.printResults()

source = "data/video/2034115.jpg" #"samples/new_truck.jpg"
frame = cv2.imread(source, cv2.IMREAD_COLOR)
handleFrame(frame, 1, source)





pip install ultralytics


import ultralytics
ultralytics.checks()


from ultralytics import YOLO


from ultralytics import YOLO
model = YOLO('yolov8n.pt')


from ultralytics import YOLO
plate_model = YOLO('best_plate_model.pt')


from ultralytics import YOLO
specialist_model = YOLO('best_m_specialist.pt')
# results = specialist_model.train(data='C:\\Users\\Saleh\\Desktop\\BA CV\\LKW Data\\LicensePlateProject\\datasets\\ANPRV4\\data.yaml', epochs=5)


model.info()


# Train the model on the COCO8 example dataset for 100 epochs
results = model.train(data='coco8.yaml', epochs=10)


import pytesseract
pytesseract.pytesseract.tesseract_cmd = 'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'
print(pytesseract.get_languages(config=''))





# Neuronal Network Layered Stream with the better fps
import cv2
import IPython
from PIL import Image
import pytesseract
import traceback

from ultralytics import YOLO
model = YOLO('yolov8n.pt')

# NN Layer Yolov8 Detection
def process_frame(frame):
    processed_frame = frame
    # processed_frame = cv2.flip(processed_frame, 1)
    results = model(processed_frame, agnostic_nms=True, verbose=False)[0]
    for result in results:
        detection_count = result.boxes.shape[0]
        for i in range(detection_count):
            cls = int(result.boxes.cls[i].item())
            name = result.names[cls]
            confidence = float(result.boxes.conf[i].item())
            bounding_box = result.boxes.xyxy[i].cpu().numpy()
            x1, y1, x2, y2 = [int(x) for x in bounding_box]
            # Ensure x1 < x2 and y1 < y2
            x1, x2 = min(x1, x2), max(x1, x2)
            y1, y2 = min(y1, y2), max(y1, y2)
            cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(255,255,0),2)
            cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1-15),0,0.7,(255,255,0))
    return processed_frame

cam = cv2.VideoCapture(0)
display_handle=display(None, display_id=True)

if not cam.isOpened(): print("Error: Could not open camera.")
else:
    try:
        while True:
            ret, frame = cam.read()
            if not ret:
                print("Error: Could not capture frame.")
                cam.release()
                break
            processed_frame = process_frame(frame)
            _, processed_frame = cv2.imencode('.jpeg', processed_frame)
            display_handle.update(IPython.display.Image(data=processed_frame.tobytes()))
            IPython.display.clear_output(wait=True)
    except KeyboardInterrupt:
        cam.release()
        print("Camera released.")
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Camera could not be released.")
        except:
            pass
        traceback.print_exc()





# Neuronal Network Layered Stream with the better fps
import cv2
import IPython
from PIL import Image
import traceback
import math

from ultralytics import YOLO
model = YOLO('yolov8n.pt')
plate_model = YOLO('best_plate_model.pt')

target_classes = ["truck", "bus", "car"]

# NN Layer Vehicle Found -> License Detection
def process_vehicle(processed_frame, bounding_box, name, confidence):
    x1, y1, x2, y2 = [int(x) for x in bounding_box]
    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(0,255,255),2)
    cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1-15),0,0.7,(0,255,255))
    results = plate_model(processed_frame[y1:y2,x1:x2], agnostic_nms=True, verbose=False)[0]
    for result in results:
        detection_count = result.boxes.shape[0]
        for i in range(detection_count):
            cls = int(result.boxes.cls[i].item())
            name2 = result.names[cls]
            confidence2 = float(result.boxes.conf[i].item())
            bounding_box2 = result.boxes.xyxy[i].cpu().numpy()
            a1, b1, a2, b2 = [int(x) for x in bounding_box2]
            # Ensure a1 < a2 and b1 < b2
            a1, a2 = min(a1, a2), max(a1, a2)
            b1, b2 = min(b1, b2), max(b1, b2)
            # if name2=="License_Plate": process_license(processed_frame, [x1+a1, y1+b1, x1+a2, y1+b2], name2, confidence2)

# NN Layer Yolov8 Detection
def process_frame(frame):
    processed_frame = frame
    # processed_frame = cv2.flip(processed_frame, 1)
    results = model(processed_frame, agnostic_nms=True, verbose=False)[0]
    for result in results:
        detection_count = result.boxes.shape[0]
        for i in range(detection_count):
            cls = int(result.boxes.cls[i].item())
            name = result.names[cls]
            confidence = float(result.boxes.conf[i].item())
            bounding_box = result.boxes.xyxy[i].cpu().numpy()
            x1, y1, x2, y2 = [int(x) for x in bounding_box]
            # Ensure x1 < x2 and y1 < y2
            x1, x2 = min(x1, x2), max(x1, x2)
            y1, y2 = min(y1, y2), max(y1, y2)
            if name in target_classes: process_vehicle(processed_frame, bounding_box, name, confidence)
    return processed_frame

def handleFrame(frame):
    processed_frame = process_frame(frame)
    _, processed_frame = cv2.imencode('.jpeg', processed_frame)
    display_handle.update(IPython.display.Image(data=processed_frame.tobytes()))
    IPython.display.clear_output(wait=True)

def handleRelease():
    cam.release()
    print("Source released.")

inputType="video"
# cam = cv2.VideoCapture(0)
source = "data/video/sample.MP4" if inputType=="video" else 0
cam = cv2.VideoCapture(source)
display_handle=display(None, display_id=True)
every = 60

start = 0 if source!=0 else -1
end = int(cam.get(cv2.CAP_PROP_FRAME_COUNT)) if source!=0 else -1
if source!=0: cam.set(1, start)

frameCount=start
while_safety=0
saved_count=0

if not cam.isOpened(): print("Error: Could not open source.")
else:
    try:
        while True if end<0 else frameCount<end:
            _, frame = cam.read()
            if frame is None:
                if source!=0:
                    if while_safety > 2000: break
                    while_safety += 1
                    continue
                else:
                    print("Error: Could not capture frame.")
                    cam.release()
                    break

            if every>0:
                if (frameCount+1)%math.floor(every) == 0:
                    while_safety = 0
                    handleFrame(frame)
            else:
                while_safety=0
                handleFrame(frame)
            frameCount += 1
        handleRelease()
    except KeyboardInterrupt:
        handleRelease()
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Source could not be released.")
        except:
            pass
        traceback.print_exc()





# Neuronal Network Layered Stream with the better fps
import cv2
import IPython
from PIL import Image
import traceback
import time

from ultralytics import YOLO
model = YOLO('yolov8n.pt')
specialist_model_n = YOLO('best_specialist.pt')
specialist_model_m = YOLO('best_m_specialist.pt')
specialist_model_x = YOLO('best_x_specialist.pt')
# plate_model = YOLO('vehicle_license_best.pt')
# other_plate_model = YOLO('best_3.pt')
# best_plate_model = YOLO('best_plate_model.pt')


times = []

# NN Layer Yolov8 Detection
def process_frame(frame):
    processed_frame = frame
    # processed_frame = cv2.flip(processed_frame, 1)

    start_time = time.time()
    results = specialist_model_n(processed_frame, agnostic_nms=True, verbose=False)[0]
    end_time = time.time() - start_time
    for result in results:
        detection_count = result.boxes.shape[0]
        for i in range(detection_count):
            cls = int(result.boxes.cls[i].item())
            name = result.names[cls]
            confidence = float(result.boxes.conf[i].item())
            bounding_box = result.boxes.xyxy[i].cpu().numpy()
            x1, y1, x2, y2 = [int(x) for x in bounding_box]
            # Ensure x1 < x2 and y1 < y2
            x1, x2 = min(x1, x2), max(x1, x2)
            y1, y2 = min(y1, y2), max(y1, y2)
            times.append(["specialist_model_n", end_time])

    start_time = time.time()
    results = specialist_model_m(processed_frame, agnostic_nms=True, verbose=False)[0]
    end_time = time.time() - start_time
    for result in results:
        detection_count = result.boxes.shape[0]
        for i in range(detection_count):
            cls = int(result.boxes.cls[i].item())
            name = result.names[cls]
            confidence = float(result.boxes.conf[i].item())
            bounding_box = result.boxes.xyxy[i].cpu().numpy()
            x1, y1, x2, y2 = [int(x) for x in bounding_box]
            # Ensure x1 < x2 and y1 < y2
            x1, x2 = min(x1, x2), max(x1, x2)
            y1, y2 = min(y1, y2), max(y1, y2)
            times.append(["specialist_model_m", end_time])

    start_time = time.time()
    results = specialist_model_x(processed_frame, agnostic_nms=True, verbose=False)[0]
    end_time = time.time() - start_time
    for result in results:
        detection_count = result.boxes.shape[0]
        for i in range(detection_count):
            cls = int(result.boxes.cls[i].item())
            name = result.names[cls]
            confidence = float(result.boxes.conf[i].item())
            bounding_box = result.boxes.xyxy[i].cpu().numpy()
            x1, y1, x2, y2 = [int(x) for x in bounding_box]
            # Ensure x1 < x2 and y1 < y2
            x1, x2 = min(x1, x2), max(x1, x2)
            y1, y2 = min(y1, y2), max(y1, y2)
            cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(255,0,255),2)
            cv2.putText(processed_frame, '{} {:.2f}%'.format(name, confidence*100),(x1+10,y1+100),0,0.9,(255,0,255),3)
            times.append(["specialist_model_x", end_time])

    # start_time = time.time()
    # results = plate_model(processed_frame, agnostic_nms=True, verbose=False)[0]
    # end_time = time.time() - start_time
    # for result in results:
    #     detection_count = result.boxes.shape[0]
    #     for i in range(detection_count):
    #         cls = int(result.boxes.cls[i].item())
    #         name = result.names[cls]
    #         confidence = float(result.boxes.conf[i].item())
    #         bounding_box = result.boxes.xyxy[i].cpu().numpy()
    #         x1, y1, x2, y2 = [int(x) for x in bounding_box]
    #         # Ensure x1 < x2 and y1 < y2
    #         x1, x2 = min(x1, x2), max(x1, x2)
    #         y1, y2 = min(y1, y2), max(y1, y2)
    #         # cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(255,0,255),2)
    #         # cv2.putText(processed_frame, '{} {:.2f}%'.format(name, confidence*100),(x1+10,y1-15),0,0.9,(255,0,255))
    #         times.append(["vehicle_license_best", end_time])

    # start_time = time.time()
    # results = other_plate_model(processed_frame, agnostic_nms=True, verbose=False)[0]
    # end_time = time.time() - start_time
    # for result in results:
    #     detection_count = result.boxes.shape[0]
    #     for i in range(detection_count):
    #         cls = int(result.boxes.cls[i].item())
    #         name = result.names[cls]
    #         confidence = float(result.boxes.conf[i].item())
    #         bounding_box = result.boxes.xyxy[i].cpu().numpy()
    #         x1, y1, x2, y2 = [int(x) for x in bounding_box]
    #         # Ensure x1 < x2 and y1 < y2
    #         x1, x2 = min(x1, x2), max(x1, x2)
    #         y1, y2 = min(y1, y2), max(y1, y2)
    #         # cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(255,0,255),2)
    #         # cv2.putText(processed_frame, '{} {:.2f}%'.format(name, confidence*100),(x1+10,y1-15),0,0.9,(255,0,255))
    #         times.append(["best_3", end_time])

    # start_time = time.time()
    # results = best_plate_model(processed_frame, agnostic_nms=True, verbose=False)[0]
    # end_time = time.time() - start_time
    # for result in results:
    #     detection_count = result.boxes.shape[0]
    #     for i in range(detection_count):
    #         cls = int(result.boxes.cls[i].item())
    #         name = result.names[cls]
    #         confidence = float(result.boxes.conf[i].item())
    #         bounding_box = result.boxes.xyxy[i].cpu().numpy()
    #         x1, y1, x2, y2 = [int(x) for x in bounding_box]
    #         # Ensure x1 < x2 and y1 < y2
    #         x1, x2 = min(x1, x2), max(x1, x2)
    #         y1, y2 = min(y1, y2), max(y1, y2)
    #         # cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(255,0,255),2)
    #         # cv2.putText(processed_frame, '{} {:.2f}%'.format(name, confidence*100),(x1+10,y1-15),0,0.9,(255,0,255))
    #         times.append(["best_plate_model", end_time])
    
    return processed_frame

def handleFrame(frame):
    processed_frame = process_frame(frame)
    _, processed_frame = cv2.imencode('.jpeg', processed_frame)
    display_handle.update(IPython.display.Image(data=processed_frame.tobytes()))
    IPython.display.clear_output(wait=True)

def handleRelease():
    cam.release()
    print("Source released.")
    print(times)

inputType="video"
# cam = cv2.VideoCapture(0)
source = "data/video/sample.MP4" if inputType=="video" else 0
cam = cv2.VideoCapture(source)
display_handle=display(None, display_id=True)
every = 60

start = 0 if source!=0 else -1
end = int(cam.get(cv2.CAP_PROP_FRAME_COUNT)) if source!=0 else -1
if source!=0: cam.set(1, start)

frameCount=start
while_safety=0
saved_count=0

if not cam.isOpened(): print("Error: Could not open source.")
else:
    try:
        while True if end<0 else frameCount<end:
            _, frame = cam.read()
            if frame is None:
                if source!=0:
                    if while_safety > 2000: break
                    while_safety += 1
                    continue
                else:
                    print("Error: Could not capture frame.")
                    cam.release()
                    break

            if every>0:
                if (frameCount+1)%math.floor(every) == 0:
                    while_safety = 0
                    handleFrame(frame)
            else:
                while_safety=0
                handleFrame(frame)
            frameCount += 1
        handleRelease()
    except KeyboardInterrupt:
        handleRelease()
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Source could not be released.")
        except:
            pass
        traceback.print_exc()





# Neuronal Network Layered Stream with the better fps
import cv2
import IPython
from PIL import Image
import numpy as np
import pytesseract
import traceback

from ultralytics import YOLO
model = YOLO('yolov8n.pt')
plate_model = YOLO('best_plate_model.pt')

pytesseract.pytesseract.tesseract_cmd = 'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'

# NN Layer License Plate Recognition "Dummy OCR"
def process_license(processed_frame, bounding_box, name, confidence):
    x1, y1, x2, y2 = [int(x) for x in bounding_box]
    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(255,0,255),2)
    gray = cv2.cvtColor(np.array(processed_frame[y1:y2,x1:x2]), cv2.COLOR_BGR2GRAY)
    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    plate_text = pytesseract.image_to_string(thresh, config='--psm 6 --oem 3 -c tessedit_char_whitelist=ÄÜÖABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')
    cv2.putText(processed_frame, '{} {:.2f}%'.format(plate_text, confidence*100),(x1+10,y1-15),0,0.9,(255,0,255),3)

# NN Layer Yolov8 Detection
def process_frame(frame):
    processed_frame = frame
    # processed_frame = cv2.flip(processed_frame, 1)
    results = plate_model(processed_frame, agnostic_nms=True, verbose=False)[0]
    for result in results:
        detection_count = result.boxes.shape[0]
        for i in range(detection_count):
            cls = int(result.boxes.cls[i].item())
            name = result.names[cls]
            confidence = float(result.boxes.conf[i].item())
            bounding_box = result.boxes.xyxy[i].cpu().numpy()
            x1, y1, x2, y2 = [int(x) for x in bounding_box]
            # Ensure x1 < x2 and y1 < y2
            x1, x2 = min(x1, x2), max(x1, x2)
            y1, y2 = min(y1, y2), max(y1, y2)
            # cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(255,0,255),2)
            # cv2.putText(processed_frame, '{} {:.2f}%'.format(plate_text, confidence*100),(x1+10,y1-15),0,0.9,(255,0,255))
            process_license(processed_frame, bounding_box, name, confidence)
    return processed_frame

cam = cv2.VideoCapture(0)
display_handle=display(None, display_id=True)

if not cam.isOpened(): print("Error: Could not open camera.")
else:
    try:
        while True:
            ret, frame = cam.read()
            if not ret:
                print("Error: Could not capture frame.")
                cam.release()
                break
            processed_frame = process_frame(frame)
            _, processed_frame = cv2.imencode('.jpeg', processed_frame)
            display_handle.update(IPython.display.Image(data=processed_frame.tobytes()))
            IPython.display.clear_output(wait=True)
    except KeyboardInterrupt:
        cam.release()
        print("Camera released.")
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Camera could not be released.")
        except:
            pass
        traceback.print_exc()





# Neuronal Network Layered Stream with the better fps
import cv2
import IPython
from PIL import Image
import easyocr
import traceback

from ultralytics import YOLO
model = YOLO('yolov8n.pt')
plate_model = YOLO('best_plate_model.pt')

reader = easyocr.Reader(['de'])

# NN Layer License Plate Recognition "Dummy OCR"
def process_license(processed_frame, bounding_box, name, confidence):
    x1, y1, x2, y2 = [int(x) for x in bounding_box]
    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(255,0,255),2)
    gray = cv2.cvtColor(processed_frame[y1:y2,x1:x2] , cv2.COLOR_RGB2GRAY)
    #gray = cv2.resize(gray, None, fx = 3, fy = 3, interpolation = cv2.INTER_CUBIC)
    result = reader.readtext(gray)
    text = ""
    for res in result:
        if len(result) == 1:
            text = res[1]
        if len(result) >1 and len(res[1])>6 and res[2]> 0.2:
            text = res[1]
    #     text += res[1] + " "
    cv2.putText(processed_frame, '{} {:.2f}%'.format(str(text), confidence*100),(x1+10,y1-15),0,0.9,(255,0,255),3)

# NN Layer Yolov8 Detection
def process_frame(frame):
    processed_frame = frame
    # processed_frame = cv2.flip(processed_frame, 1)
    results = plate_model(processed_frame, agnostic_nms=True, verbose=False)[0]
    for result in results:
        detection_count = result.boxes.shape[0]
        for i in range(detection_count):
            cls = int(result.boxes.cls[i].item())
            name = result.names[cls]
            confidence = float(result.boxes.conf[i].item())
            bounding_box = result.boxes.xyxy[i].cpu().numpy()
            x1, y1, x2, y2 = [int(x) for x in bounding_box]
            # Ensure x1 < x2 and y1 < y2
            x1, x2 = min(x1, x2), max(x1, x2)
            y1, y2 = min(y1, y2), max(y1, y2)
            # cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(255,0,255),2)
            # cv2.putText(processed_frame, '{} {:.2f}%'.format(name, confidence*100),(x1+10,y1-15),0,0.9,(255,0,255))
            process_license(processed_frame, bounding_box, name, confidence)
    return processed_frame

cam = cv2.VideoCapture(0)
display_handle=display(None, display_id=True)

if not cam.isOpened(): print("Error: Could not open camera.")
else:
    try:
        while True:
            ret, frame = cam.read()
            if not ret:
                print("Error: Could not capture frame.")
                cam.release()
                break
            processed_frame = process_frame(frame)
            _, processed_frame = cv2.imencode('.jpeg', processed_frame)
            display_handle.update(IPython.display.Image(data=processed_frame.tobytes()))
            IPython.display.clear_output(wait=True)
    except KeyboardInterrupt:
        cam.release()
        print("Camera released.")
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Camera could not be released.")
        except:
            pass
        traceback.print_exc()





# Neuronal Network Layered Stream with the better fps
import cv2
import IPython
import numpy as np
from PIL import Image
import pytesseract
import traceback

from ultralytics import YOLO
model = YOLO('yolov8n.pt')
plate_model = YOLO('best_plate_model.pt')

pytesseract.pytesseract.tesseract_cmd = 'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'

target_classes = ["truck", "bus", "car"]

# NN Layer License Plate Recognition "Dummy OCR"
def process_license(processed_frame, bounding_box, name, confidence):
    x1, y1, x2, y2 = [int(x) for x in bounding_box]
    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(255,0,255),2)
    gray = cv2.cvtColor(np.array(processed_frame[y1:y2,x1:x2]), cv2.COLOR_BGR2GRAY)
    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    plate_text = pytesseract.image_to_string(thresh, config='--psm 6 --oem 3 -c tessedit_char_whitelist=ÄÜÖABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')
    cv2.putText(processed_frame, '{} {:.2f}%'.format(plate_text, confidence*100),(x1+10,y1-15),0,0.9,(255,0,255),3)

# NN Layer Vehicle Found -> License Detection
def process_vehicle(processed_frame, bounding_box, name, confidence):
    x1, y1, x2, y2 = [int(x) for x in bounding_box]
    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(0,255,255),2)
    cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1-15),0,0.7,(0,255,255))
    results = plate_model(processed_frame[y1:y2,x1:x2], agnostic_nms=True, verbose=False)[0]
    for result in results:
        detection_count = result.boxes.shape[0]
        for i in range(detection_count):
            cls = int(result.boxes.cls[i].item())
            name2 = result.names[cls]
            confidence2 = float(result.boxes.conf[i].item())
            bounding_box2 = result.boxes.xyxy[i].cpu().numpy()
            a1, b1, a2, b2 = [int(x) for x in bounding_box2]
            # Ensure a1 < a2 and b1 < b2
            a1, a2 = min(a1, a2), max(a1, a2)
            b1, b2 = min(b1, b2), max(b1, b2)
            if name2=="License_Plate": process_license(processed_frame, [x1+a1, y1+b1, x1+a2, y1+b2], name2, confidence2)

# NN Layer Other Objects Detected
def process_detection(processed_frame, bounding_box, name, confidence):
    x1, y1, x2, y2 = [int(x) for x in bounding_box]
    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(255,255,0),2)
    cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1-15),0,0.7,(255,255,0))

# NN Layer Yolov8 Detection
def process_frame(frame):
    processed_frame = frame
    # processed_frame = cv2.flip(processed_frame, 1)
    results = model(processed_frame, agnostic_nms=True, verbose=False)[0]
    for result in results:
        detection_count = result.boxes.shape[0]
        for i in range(detection_count):
            cls = int(result.boxes.cls[i].item())
            name = result.names[cls]
            confidence = float(result.boxes.conf[i].item())
            bounding_box = result.boxes.xyxy[i].cpu().numpy()
            x1, y1, x2, y2 = [int(x) for x in bounding_box]
            # Ensure x1 < x2 and y1 < y2
            x1, x2 = min(x1, x2), max(x1, x2)
            y1, y2 = min(y1, y2), max(y1, y2)
            if name in target_classes: process_vehicle(processed_frame, bounding_box, name, confidence)
            else: process_detection(processed_frame, bounding_box, name, confidence)
    return processed_frame

cam = cv2.VideoCapture(0)
display_handle=display(None, display_id=True)

if not cam.isOpened(): print("Error: Could not open camera.")
else:
    try:
        while True:
            ret, frame = cam.read()
            if not ret:
                print("Error: Could not capture frame.")
                cam.release()
                break
            processed_frame = process_frame(frame)
            _, processed_frame = cv2.imencode('.jpeg', processed_frame)
            display_handle.update(IPython.display.Image(data=processed_frame.tobytes()))
            IPython.display.clear_output(wait=True)
    except KeyboardInterrupt:
        cam.release()
        print("Camera released.")
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Camera could not be released.")
        except:
            pass
        traceback.print_exc()





# Neuronal Network Layered Stream with the better fps
import cv2
import IPython
import numpy as np
from PIL import Image
import traceback
import easyocr

reader = easyocr.Reader(['de'])

from ultralytics import YOLO
model = YOLO('yolov8n.pt')
plate_model = YOLO('best_plate_model.pt')

target_classes = ["truck", "bus", "car"]

# NN Layer License Plate Recognition "Dummy OCR"
def process_license(processed_frame, bounding_box, name, confidence):
    x1, y1, x2, y2 = [int(x) for x in bounding_box]
    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(255,0,255),2)
    gray = cv2.cvtColor(np.array(processed_frame[y1:y2,x1:x2]), cv2.COLOR_BGR2GRAY)
    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    result = reader.readtext(thresh)
    text = ""
    conf = 0.0
    for res in result:
        if res[2]>conf:
            conf=res[2]
            text=res[1]
    cv2.putText(processed_frame, '{} {:.2f}%'.format(str(text), conf*100),(x1+10,y1-15),0,0.9,(255,0,255),3)

# NN Layer Vehicle Found -> License Detection
def process_vehicle(processed_frame, bounding_box, name, confidence):
    x1, y1, x2, y2 = [int(x) for x in bounding_box]
    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(0,255,255),2)
    cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1-15),0,0.7,(0,255,255))
    results = plate_model(processed_frame[y1:y2,x1:x2], agnostic_nms=True, verbose=False)[0]
    for result in results:
        detection_count = result.boxes.shape[0]
        for i in range(detection_count):
            cls = int(result.boxes.cls[i].item())
            name2 = result.names[cls]
            confidence2 = float(result.boxes.conf[i].item())
            bounding_box2 = result.boxes.xyxy[i].cpu().numpy()
            a1, b1, a2, b2 = [int(x) for x in bounding_box2]
            # Ensure a1 < a2 and b1 < b2
            a1, a2 = min(a1, a2), max(a1, a2)
            b1, b2 = min(b1, b2), max(b1, b2)
            if name2=="licence": process_license(processed_frame, [x1+a1, y1+b1, x1+a2, y1+b2], "license", confidence2)

# NN Layer Other Objects Detected
def process_detection(processed_frame, bounding_box, name, confidence):
    x1, y1, x2, y2 = [int(x) for x in bounding_box]
    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(255,255,0),2)
    cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1-15),0,0.7,(255,255,0))

# NN Layer Yolov8 Detection
def process_frame(frame):
    processed_frame = frame
    # processed_frame = cv2.flip(processed_frame, 1)
    results = model(processed_frame, agnostic_nms=True, verbose=False)[0]
    for result in results:
        detection_count = result.boxes.shape[0]
        for i in range(detection_count):
            cls = int(result.boxes.cls[i].item())
            name = result.names[cls]
            confidence = float(result.boxes.conf[i].item())
            bounding_box = result.boxes.xyxy[i].cpu().numpy()
            x1, y1, x2, y2 = [int(x) for x in bounding_box]
            # Ensure x1 < x2 and y1 < y2
            x1, x2 = min(x1, x2), max(x1, x2)
            y1, y2 = min(y1, y2), max(y1, y2)
            if name in target_classes: process_vehicle(processed_frame, bounding_box, name, confidence)
            else: process_detection(processed_frame, bounding_box, name, confidence)
    return processed_frame

cam = cv2.VideoCapture(0)
display_handle=display(None, display_id=True)

if not cam.isOpened(): print("Error: Could not open camera.")
else:
    try:
        while True:
            ret, frame = cam.read()
            if not ret:
                print("Error: Could not capture frame.")
                cam.release()
                break
            processed_frame = process_frame(frame)
            _, processed_frame = cv2.imencode('.jpeg', processed_frame)
            display_handle.update(IPython.display.Image(data=processed_frame.tobytes()))
            IPython.display.clear_output(wait=True)
    except KeyboardInterrupt:
        cam.release()
        print("Camera released.")
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Camera could not be released.")
        except:
            pass
        traceback.print_exc()





# Neuronal Network Layered Stream with the better fps
import cv2
import IPython
import numpy as np
from PIL import Image
import pytesseract
import traceback
import re

from ultralytics import YOLO
specialist_model = YOLO('best_m_specialist.pt') # Self trained yolov8m on ANPRv4 Dataset

detected_plates = []

pytesseract.pytesseract.tesseract_cmd = 'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'

# get grayscale image
def get_grayscale(image):
    return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

#thresholding
def thresholding(image):
    return cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

# NN Layer License Plate Recognition "Dummy OCR"
def process_license(processed_frame, bounding_box, name, confidence, parentName='test', parentConfidence=0.0):
    x1, y1, x2, y2 = [int(x) for x in bounding_box]
    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(255,0,255),2)
    cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1-15),0,0.7,(255,0,255))

    license_frame = processed_frame[y1:y2,x1:x2]
    grayscaled = get_grayscale(np.array(license_frame))
    threshholded = thresholding(grayscaled)
    # angle = determine_skew(threshholded)
    # print(angle)
    # threshholded = rotate(threshholded, angle, (0, 0, 0))

    results = pytesseract.image_to_data(threshholded, config='--psm 6 --oem 3 -c tessedit_char_whitelist=ÄÜÖABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')

    max_conf = 0.0
    best_text = ""
    parsedLines = results.split('\n')
    best_depth = len(parsedLines)-2
    for line in parsedLines:
        params = line.split()
        if len(params)==12 and float(params[10].replace('conf','0.0'))>max_conf:
            max_conf = float(params[10])
            best_text = params[11]
            if max_conf>0.0 and len(best_text)>0:
                detected_plates.append([parentConfidence*100, parentName.upper(), confidence*100, name.upper(), max_conf, best_text])
    cv2.putText(processed_frame, '{} {:.2f}%'.format(best_text, max_conf),(x1+10,y2+25),0,0.9,(0,0,255),3)

# NN Layer Yolov8 Detection
def process_frame(frame):
    processed_frame = frame
    # processed_frame = cv2.flip(processed_frame, 1)
    results = specialist_model(processed_frame, agnostic_nms=True, verbose=False)[0]
    for result in results:
        detection_count = result.boxes.shape[0]
        for i in range(detection_count):
            cls = int(result.boxes.cls[i].item())
            name = result.names[cls]
            confidence = float(result.boxes.conf[i].item())
            bounding_box = result.boxes.xyxy[i].cpu().numpy()
            x1, y1, x2, y2 = [int(x) for x in bounding_box]
            # Ensure x1 < x2 and y1 < y2
            x1, x2 = min(x1, x2), max(x1, x2)
            y1, y2 = min(y1, y2), max(y1, y2)
            cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(0,255,255),2)
            cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1-15),0,0.7,(0,255,255))
            if name=="license-plate": process_license(processed_frame, bounding_box, name, confidence)
    return processed_frame

cam = cv2.VideoCapture(0)
display_handle=display(None, display_id=True)

if not cam.isOpened(): print("Error: Could not open camera.")
else:
    try:
        while True:
            ret, frame = cam.read()
            if not ret:
                print("Error: Could not capture frame.")
                cam.release()
                break
            processed_frame = process_frame(frame)
            _, processed_frame = cv2.imencode('.jpeg', processed_frame)
            display_handle.update(IPython.display.Image(data=processed_frame.tobytes()))
            IPython.display.clear_output(wait=True)
    except KeyboardInterrupt:
        cam.release()
        print("Camera released.")
        correct_plates = []
        best_plates = []
        most_repeated_plates = []
        best_repeated_plates = []
        if len(detected_plates)>0:
            repetitions = {}
            count, item = 0, ''
            for plate in detected_plates:
                if plate[0]>50.0 and plate[2]>50.0 and plate[4]>50.0: correct_plates.append(plate)
                if plate[0]>70.0 and plate[2]>70.0 and plate[4]>70.0: best_plates.append(plate)
                repetitions[plate[5]] = repetitions.get(plate[5], 0) + 1
                if repetitions[plate[5]]>count: count, item = repetitions[plate[5]], plate[5]
            if len(repetitions.keys())>0:
                for itm in repetitions.keys():
                    if repetitions[itm]==count:
                        repeated_detected = [plate for plate in detected_plates if plate[5]==itm]
                        best_percentages = []
                        for plate in repeated_detected:
                            if len(best_percentages)==0: best_percentages=plate
                            elif (plate[0] + plate[2] + plate[4])/3.0>(best_percentages[0] + best_percentages[2] + best_percentages[4])/3.0: best_percentages=plate
                        most_repeated_plates.append(best_percentages)
            if len(most_repeated_plates)>0:
                for plate in most_repeated_plates:
                    best_repeated = []
                    if len(best_repeated)==0: best_repeated=plate
                    elif (plate[0] + plate[2] + plate[4])/3.0>(best_repeated[0] + best_repeated[2] + best_repeated[4])/3.0: best_repeated=plate
                best_repeated_plates.append(best_repeated)

        print("\nDetected Plates: \n", np.matrix(detected_plates))
        print("\nCorrectly Detected Plates: \n", np.matrix(correct_plates))
        print("\nBest Detected Plates: \n", np.matrix(best_plates))
        print("\nMost Repeated Plates: \n", np.matrix(most_repeated_plates))
        print("\nBest Repeated Plate: \n", np.matrix(best_repeated_plates))
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Camera could not be released.")
        except:
            pass
        traceback.print_exc()





# Neuronal Network Layered Stream with the better fps
import cv2
import IPython
import numpy as np
from PIL import Image
import pytesseract
import traceback
import re

from ultralytics import YOLO
specialist_model = YOLO('best_m_specialist.pt') # Self trained yolov8m on ANPRv4 Dataset

detected_plates = []

pytesseract.pytesseract.tesseract_cmd = 'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'

# get grayscale image
def get_grayscale(image):
    return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

#thresholding
def thresholding(image):
    return cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

# NN Layer License Plate Recognition "Dummy OCR"
def process_license(processed_frame, bounding_box, name, confidence, parentName, parentConfidence):
    x1, y1, x2, y2 = [int(x) for x in bounding_box]
    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(255,0,255),2)
    cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1-15),0,0.7,(255,0,255))

    license_frame = processed_frame[y1:y2,x1:x2]
    grayscaled = get_grayscale(np.array(license_frame))
    threshholded = thresholding(grayscaled)

    results = pytesseract.image_to_data(threshholded, config='--psm 6 --oem 3 -c tessedit_char_whitelist=ÄÜÖABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')

    max_conf = 0.0
    best_text = ""
    parsedLines = results.split('\n')
    best_depth = len(parsedLines)-2
    for line in parsedLines:
        params = line.split()
        if len(params)==12 and float(params[10].replace('conf','0.0'))>max_conf:
            max_conf = float(params[10])
            best_text = params[11]
            if max_conf>0.0 and len(best_text)>0:
                detected_plates.append([parentConfidence*100, parentName.upper(), confidence*100, name.upper(), max_conf, best_text])
    cv2.putText(processed_frame, '{} {:.2f}%'.format(best_text, max_conf),(x1+10,y2+25),0,0.9,(0,0,255),3)

# NN Layer Yolov8 Detection
def process_frame(frame):
    processed_frame = frame
    # processed_frame = cv2.flip(processed_frame, 1)
    results = specialist_model(processed_frame, agnostic_nms=True, verbose=False)[0]
    for result in results:
        detection_count = result.boxes.shape[0]
        for i in range(detection_count):
            cls = int(result.boxes.cls[i].item())
            name = result.names[cls]
            confidence = float(result.boxes.conf[i].item())
            bounding_box = result.boxes.xyxy[i].cpu().numpy()
            x1, y1, x2, y2 = [int(x) for x in bounding_box]
            x1, x2 = min(x1, x2), max(x1, x2)
            y1, y2 = min(y1, y2), max(y1, y2)

            if name=="license-plate":
                for j in range(detection_count):
                    parentCls = int(result.boxes.cls[j].item())
                    parentName = result.names[parentCls]
                    parentConfidence = float(result.boxes.conf[j].item())
                    parent_bounding_box = result.boxes.xyxy[j].cpu().numpy()
                    px1, py1, px2, py2 = [int(x) for x in parent_bounding_box]
                    px1, px2 = min(px1, px2), max(px1, px2)
                    py1, py2 = min(py1, py2), max(py1, py2)

                    if parentName!="license-plate": continue

                    if px1<=x1 and x2<=px2 and py1<=y1 and y2<=py2:
                        cv2.rectangle(processed_frame,(px1,py1),(px2,py2),(0,255,255),2)
                        cv2.putText(processed_frame, '{} {:.2f}%'.format(parentName.upper(), parentConfidence*100),(px1+10,py1-15),0,0.7,(0,255,255))
                        process_license(processed_frame, bounding_box, name, confidence, parentName, parentConfidence)
            else:
                cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(0,255,255),2)
                cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1-15),0,0.7,(0,255,255))

    return processed_frame

cam = cv2.VideoCapture(0)
display_handle=display(None, display_id=True)

if not cam.isOpened(): print("Error: Could not open camera.")
else:
    try:
        while True:
            ret, frame = cam.read()
            if not ret:
                print("Error: Could not capture frame.")
                cam.release()
                break
            processed_frame = process_frame(frame)
            _, processed_frame = cv2.imencode('.jpeg', processed_frame)
            display_handle.update(IPython.display.Image(data=processed_frame.tobytes()))
            IPython.display.clear_output(wait=True)
    except KeyboardInterrupt:
        cam.release()
        print("Camera released.")
        correct_plates = []
        better_plates = []
        best_plate = []
        most_repeated_plates = []
        best_repeated_plates = []
        highest_score = 0.0
        if len(detected_plates)>0:
            repetitions = {}
            count, item = 0, ''
            for plate in detected_plates:
                if plate[0]>50.0 and plate[2]>50.0 and plate[4]>50.0: correct_plates.append(plate)
                if plate[0]>70.0 and plate[2]>70.0 and plate[4]>70.0: better_plates.append(plate)
                if plate[0] + plate[2] + plate[4] > highest_score:
                    highest_score = plate[0] + plate[2] + plate[4]
                    best_plate = plate
                repetitions[plate[5]] = repetitions.get(plate[5], 0) + 1
                if repetitions[plate[5]]>count: count, item = repetitions[plate[5]], plate[5]
            if len(repetitions.keys())>0:
                for itm in repetitions.keys():
                    if repetitions[itm]==count:
                        repeated_detected = [plate for plate in detected_plates if plate[5]==itm]
                        best_percentages = []
                        for plate in repeated_detected:
                            if len(best_percentages)==0: best_percentages=plate
                            elif (plate[0] + plate[2] + plate[4])/3.0>(best_percentages[0] + best_percentages[2] + best_percentages[4])/3.0: best_percentages=plate
                        most_repeated_plates.append(best_percentages)
            if len(most_repeated_plates)>0:
                for plate in most_repeated_plates:
                    best_repeated = []
                    if len(best_repeated)==0: best_repeated=plate
                    elif (plate[0] + plate[2] + plate[4])/3.0>(best_repeated[0] + best_repeated[2] + best_repeated[4])/3.0: best_repeated=plate
                best_repeated_plates.append(best_repeated)

        print("\nDetected Plates: \n", np.matrix(detected_plates))
        print("\nCorrectly Detected Plates: \n", np.matrix(correct_plates))
        print("\nBetter Detected Plates: \n", np.matrix(better_plates))
        print("\nBest Detected Plate: \n", np.matrix(best_plate))
        print("\nMost Repeated Plates: \n", np.matrix(most_repeated_plates))
        print("\nBest Repeated Plate: \n", np.matrix(best_repeated_plates))
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Camera could not be released.")
        except:
            pass
        traceback.print_exc()





# Neuronal Network Layered Stream with the better fps
import cv2
import IPython
import numpy as np
from PIL import Image
import pytesseract
import traceback

from ultralytics import YOLO
model = YOLO('yolov8n.pt')
plate_model = YOLO('best_plate_model.pt')

pytesseract.pytesseract.tesseract_cmd = 'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'

target_classes = ["truck", "bus", "car"]

# NN Layer License Plate Recognition "Dummy OCR"
def process_license(processed_frame, bounding_box, name, confidence):
    x1, y1, x2, y2 = [int(x) for x in bounding_box]
    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(255,0,255),2)
    gray = cv2.cvtColor(np.array(processed_frame[y1:y2,x1:x2]), cv2.COLOR_BGR2GRAY)
    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    plate_text = pytesseract.image_to_string(thresh, config='--psm 6 --oem 3 -c tessedit_char_whitelist=ÄÜÖABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')
    cv2.putText(processed_frame, '{} {:.2f}%'.format(plate_text, confidence*100),(x1+10,y1-15),0,0.9,(255,0,255),3)

# NN Layer Vehicle Found -> License Detection
def process_vehicle(processed_frame, bounding_box, name, confidence):
    x1, y1, x2, y2 = [int(x) for x in bounding_box]
    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(0,255,255),2)
    cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1-15),0,0.7,(0,255,255))

# NN Layer Other Objects Detected
def process_detection(processed_frame, bounding_box, name, confidence):
    x1, y1, x2, y2 = [int(x) for x in bounding_box]
    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(255,255,0),2)
    cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1-15),0,0.7,(255,255,0))

# NN Layer Yolov8 Detection
def process_frame(frame):
    processed_frame = frame
    # processed_frame = cv2.flip(processed_frame, 1)
    results = model(processed_frame, agnostic_nms=True, verbose=False)[0]
    for result in results:
        detection_count = result.boxes.shape[0]
        for i in range(detection_count):
            cls = int(result.boxes.cls[i].item())
            name = result.names[cls]
            confidence = float(result.boxes.conf[i].item())
            bounding_box = result.boxes.xyxy[i].cpu().numpy()
            x1, y1, x2, y2 = [int(x) for x in bounding_box]
            # Ensure x1 < x2 and y1 < y2
            x1, x2 = min(x1, x2), max(x1, x2)
            y1, y2 = min(y1, y2), max(y1, y2)
            if name in target_classes: process_vehicle(processed_frame, bounding_box, name, confidence)
            else: process_detection(processed_frame, bounding_box, name, confidence)
    resultss = plate_model(processed_frame, agnostic_nms=True, verbose=False)[0]
    for result in resultss:
        detection_count = result.boxes.shape[0]
        for i in range(detection_count):
            cls = int(result.boxes.cls[i].item())
            name2 = result.names[cls]
            confidence2 = float(result.boxes.conf[i].item())
            bounding_box2 = result.boxes.xyxy[i].cpu().numpy()
            x1, y1, x2, y2 = [int(x) for x in bounding_box2]
            # Ensure a1 < a2 and b1 < b2
            x1, x2 = min(x1, x2), max(x1, x2)
            y1, y2 = min(y1, y2), max(y1, y2)
            if name2=="License_Plate": process_license(processed_frame, bounding_box2, name2, confidence2)
    return processed_frame

cam = cv2.VideoCapture(0)
display_handle=display(None, display_id=True)

if not cam.isOpened(): print("Error: Could not open camera.")
else:
    try:
        while True:
            ret, frame = cam.read()
            if not ret:
                print("Error: Could not capture frame.")
                cam.release()
                break
            processed_frame = process_frame(frame)
            _, processed_frame = cv2.imencode('.jpeg', processed_frame)
            display_handle.update(IPython.display.Image(data=processed_frame.tobytes()))
            IPython.display.clear_output(wait=True)
    except KeyboardInterrupt:
        cam.release()
        print("Camera released.")

        correct_plates = []
        better_plates = []
        best_plate = []
        most_repeated_plates = []
        best_repeated_plates = []
        highest_score = 0.0
        if len(detected_plates)>0:
            repetitions = {}
            count, item = 0, ''
            for plate in detected_plates:
                if plate[0]>50.0 and plate[2]>50.0 and plate[4]>50.0: correct_plates.append(plate)
                if plate[0]>70.0 and plate[2]>70.0 and plate[4]>70.0: better_plates.append(plate)
                if plate[0] + plate[2] + plate[4] > highest_score:
                    highest_score = plate[0] + plate[2] + plate[4]
                    best_plate = plate
                repetitions[plate[5]] = repetitions.get(plate[5], 0) + 1
                if repetitions[plate[5]]>count: count, item = repetitions[plate[5]], plate[5]
            if len(repetitions.keys())>0:
                for itm in repetitions.keys():
                    if repetitions[itm]==count:
                        repeated_detected = [plate for plate in detected_plates if plate[5]==itm]
                        best_percentages = []
                        for plate in repeated_detected:
                            if len(best_percentages)==0: best_percentages=plate
                            elif (plate[0] + plate[2] + plate[4])/3.0>(best_percentages[0] + best_percentages[2] + best_percentages[4])/3.0: best_percentages=plate
                        most_repeated_plates.append(best_percentages)
            if len(most_repeated_plates)>0:
                for plate in most_repeated_plates:
                    best_repeated = []
                    if len(best_repeated)==0: best_repeated=plate
                    elif (plate[0] + plate[2] + plate[4])/3.0>(best_repeated[0] + best_repeated[2] + best_repeated[4])/3.0: best_repeated=plate
                best_repeated_plates.append(best_repeated)

        print("\nDetected Plates: \n", np.matrix(detected_plates))
        print("\nCorrectly Detected Plates: \n", np.matrix(correct_plates))
        print("\nBetter Detected Plates: \n", np.matrix(better_plates))
        print("\nBest Detected Plate: \n", np.matrix(best_plate))
        print("\nMost Repeated Plates: \n", np.matrix(most_repeated_plates))
        print("\nBest Repeated Plate: \n", np.matrix(best_repeated_plates))
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Camera could not be released.")
        except:
            pass
        traceback.print_exc()





# Neuronal Network Layered Stream with the better fps
import cv2
import IPython
import numpy as np
from PIL import Image
import pytesseract
import traceback

from ultralytics import YOLO
model = YOLO('yolov8n.pt')
plate_model = YOLO('best_plate_model.pt')

pytesseract.pytesseract.tesseract_cmd = 'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'

target_classes = ["truck", "bus", "car"]

detected_plates = []

# NN Layer License Plate Recognition "Dummy OCR"
def process_license(processed_frame, bounding_box, name, confidence, parentName, parentConfidence):
    x1, y1, x2, y2 = [int(x) for x in bounding_box]
    color = (255,0,255)
    if parentName!="vehicle": color = (0,255,0)
    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),color,2)
    cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1-15),0,0.9,color,3)
    gray = cv2.cvtColor(np.array(processed_frame[y1:y2,x1:x2]), cv2.COLOR_BGR2GRAY)
    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    plate_text = pytesseract.image_to_string(thresh, config='--psm 6 --oem 3 -c tessedit_char_whitelist=ÄÜÖABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')
    max_conf = 0.0
    best_text = ""
    parsedLines = plate_text.split('\n')
    best_depth = len(parsedLines)-2
    for line in parsedLines:
        params = line.split()
        if len(params)==12 and float(params[10].replace('conf','0.0'))>max_conf:
            max_conf = float(params[10])
            best_text = params[11]
            if max_conf>0.0 and len(best_text)>0:
                detected_plates.append([parentConfidence*100, parentName.upper(), confidence*100, name.upper(), max_conf, best_text])
    cv2.putText(processed_frame, '{} {:.2f}%'.format(best_text, max_conf),(x1+10,y2+25),0,0.9,(0,0,255),3)

# NN Layer Vehicle Detection
def process_vehicle(processed_frame, bounding_box, name, confidence):
    x1, y1, x2, y2 = [int(x) for x in bounding_box]
    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(0,255,255),2)
    cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1-15),0,0.7,(0,255,255))

# NN Layer Other Objects Detected
def process_detection(processed_frame, bounding_box, name, confidence):
    x1, y1, x2, y2 = [int(x) for x in bounding_box]
    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(255,255,0),2)
    cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1-15),0,0.7,(255,255,0))

# NN Layer Yolov8 Detection
def process_frame(frame):
    processed_frame = frame
    # processed_frame = cv2.flip(processed_frame, 1)
    # results = model.track(processed_frame, agnostic_nms=True, verbose=False, persist=True)[0]
    results = model(processed_frame, agnostic_nms=True, verbose=False)[0]
    for result in results:
        detection_count = result.boxes.shape[0]
        for i in range(detection_count):
            cls = int(result.boxes.cls[i].item())
            # track_id = result.boxes.id.int().cpu().tolist()
            name = result.names[cls] # +" "+str(track_id[0])
            confidence = float(result.boxes.conf[i].item())
            bounding_box = result.boxes.xyxy[i].cpu().numpy()
            if name in target_classes: process_vehicle(processed_frame, bounding_box, name, confidence)
            else: process_detection(processed_frame, bounding_box, name, confidence)
    resultss = plate_model(processed_frame, agnostic_nms=True, verbose=False)[0]
    for result in resultss:
        detection_count = result.boxes.shape[0]
        for i in range(detection_count):
            cls = int(result.boxes.cls[i].item())
            name2 = result.names[cls]
            confidence2 = float(result.boxes.conf[i].item())
            bounding_box2 = result.boxes.xyxy[i].cpu().numpy()
            x1, y1, x2, y2 = [int(x) for x in bounding_box2]
            # Ensure a1 < a2 and b1 < b2
            x1, x2 = min(x1, x2), max(x1, x2)
            y1, y2 = min(y1, y2), max(y1, y2)

            name = "vehicle"
            confidence = 0.0
            for result in results:
                detection_count = result.boxes.shape[0]
                for i in range(detection_count):
                    cls = int(result.boxes.cls[i].item())
                    vname = result.names[cls]
                    vconfidence = float(result.boxes.conf[i].item())
                    bounding_box = result.boxes.xyxy[i].cpu().numpy()
                    # track_id = result.boxes.id.int().cpu().tolist()
                    if name in target_classes:
                        vx1, vy1, vx2, vy2 = [int(x) for x in bounding_box2]
                        # Ensure a1 < a2 and b1 < b2
                        vx1, vx2 = min(vx1, vx2), max(vx1, vx2)
                        vy1, vy2 = min(vy1, vy2), max(vy1, vy2)
                        if vx1<=x1 and x2<=vx2 and vy1<=y1 and y2<=vy2:
                            name=vname # +" "+str(track_id[0])
                            confidence=vconfidence

            if name2=="License_Plate": process_license(processed_frame, bounding_box2, name2, confidence2, name, confidence)
    return processed_frame

cam = cv2.VideoCapture(0)
display_handle=display(None, display_id=True)

if not cam.isOpened(): print("Error: Could not open camera.")
else:
    try:
        while True:
            ret, frame = cam.read()
            if not ret:
                print("Error: Could not capture frame.")
                cam.release()
                break
            processed_frame = process_frame(frame)
            _, processed_frame = cv2.imencode('.jpeg', processed_frame)
            display_handle.update(IPython.display.Image(data=processed_frame.tobytes()))
            IPython.display.clear_output(wait=True)
    except KeyboardInterrupt:
        cam.release()
        print("Camera released.")

        correct_plates = []
        better_plates = []
        best_plate = []
        most_repeated_plates = []
        best_repeated_plates = []
        highest_score = 0.0
        if len(detected_plates)>0:
            repetitions = {}
            count, item = 0, ''
            for plate in detected_plates:
                if plate[0]>50.0 and plate[2]>50.0 and plate[4]>50.0: correct_plates.append(plate)
                if plate[0]>70.0 and plate[2]>70.0 and plate[4]>70.0: better_plates.append(plate)
                if plate[0] + plate[2] + plate[4] > highest_score:
                    highest_score = plate[0] + plate[2] + plate[4]
                    best_plate = plate
                repetitions[plate[5]] = repetitions.get(plate[5], 0) + 1
                if repetitions[plate[5]]>count: count, item = repetitions[plate[5]], plate[5]
            if len(repetitions.keys())>0:
                for itm in repetitions.keys():
                    if repetitions[itm]==count:
                        repeated_detected = [plate for plate in detected_plates if plate[5]==itm]
                        best_percentages = []
                        for plate in repeated_detected:
                            if len(best_percentages)==0: best_percentages=plate
                            elif (plate[0] + plate[2] + plate[4])/3.0>(best_percentages[0] + best_percentages[2] + best_percentages[4])/3.0: best_percentages=plate
                        most_repeated_plates.append(best_percentages)
            if len(most_repeated_plates)>0:
                for plate in most_repeated_plates:
                    best_repeated = []
                    if len(best_repeated)==0: best_repeated=plate
                    elif (plate[0] + plate[2] + plate[4])/3.0>(best_repeated[0] + best_repeated[2] + best_repeated[4])/3.0: best_repeated=plate
                best_repeated_plates.append(best_repeated)

        print("\nDetected Plates: \n", np.matrix(detected_plates))
        print("\nCorrectly Detected Plates: \n", np.matrix(correct_plates))
        print("\nBetter Detected Plates: \n", np.matrix(better_plates))
        print("\nBest Detected Plate: \n", np.matrix(best_plate))
        print("\nMost Repeated Plates: \n", np.matrix(most_repeated_plates))
        print("\nBest Repeated Plate: \n", np.matrix(best_repeated_plates))
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Camera could not be released.")
        except:
            pass
        traceback.print_exc()


import torch
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class AttnLabelConverter(object):
    """ Convert between text-label and text-index """

    def __init__(self, character):
        # character (str): set of the possible characters.
        # [GO] for the start token of the attention decoder. [s] for end-of-sentence token.
        list_token = ['[GO]', '[s]']  # ['[s]','[UNK]','[PAD]','[GO]']
        list_character = list(character)
        self.character = list_token + list_character

        self.dict = {}
        for i, char in enumerate(self.character):
            # print(i, char)
            self.dict[char] = i

    def encode(self, text, batch_max_length=25):
        """ convert text-label into text-index.
        input:
            text: text labels of each image. [batch_size]
            batch_max_length: max length of text label in the batch. 25 by default

        output:
            text : the input of attention decoder. [batch_size x (max_length+2)] +1 for [GO] token and +1 for [s] token.
                text[:, 0] is [GO] token and text is padded with [GO] token after [s] token.
            length : the length of output of attention decoder, which count [s] token also. [3, 7, ....] [batch_size]
        """
        length = [len(s) + 1 for s in text]  # +1 for [s] at end of sentence.
        # batch_max_length = max(length) # this is not allowed for multi-gpu setting
        batch_max_length += 1
        # additional +1 for [GO] at first step. batch_text is padded with [GO] token after [s] token.
        batch_text = torch.LongTensor(len(text), batch_max_length + 1).fill_(0)
        for i, t in enumerate(text):
            text = list(t)
            text.append('[s]')
            text = [self.dict[char] for char in text]
            batch_text[i][1:1 + len(text)] = torch.LongTensor(text)  # batch_text[:, 0] = [GO] token
        return (batch_text.to(device), torch.IntTensor(length).to(device))

    def decode(self, text_index, length):
        """ convert text-index into text-label. """
        texts = []
        for index, l in enumerate(length):
            text = ''.join([self.character[i] for i in text_index[index, :]])
            texts.append(text)
        return texts


class Averager(object):
    """Compute average for torch.Tensor, used for loss average."""

    def __init__(self):
        self.reset()

    def add(self, v):
        count = v.data.numel()
        v = v.data.sum()
        self.n_count += count
        self.sum += v

    def reset(self):
        self.n_count = 0
        self.sum = 0

    def val(self):
        res = 0
        if self.n_count != 0:
            res = self.sum / float(self.n_count)
        return res



import torch.nn as nn

from modules.transformation import TPS_SpatialTransformerNetwork
from modules.feature_extraction import VGG_FeatureExtractor, RCNN_FeatureExtractor, ResNet_FeatureExtractor
from modules.sequence_modeling import BidirectionalLSTM
from modules.prediction import Attention

class Model(nn.Module):

    def __init__(self, num_class):
        super(Model, self).__init__()
        self.stages = {'Trans': "TPS", 'Feat': "ResNet", 'Seq': "BiLSTM", 'Pred': "Attn"}

        """ Transformation """
        self.Transformation = TPS_SpatialTransformerNetwork(F=20, I_size=(32, 100), I_r_size=(32, 100), I_channel_num=1)

        """ FeatureExtraction """
        self.FeatureExtraction = ResNet_FeatureExtractor(1, 512)
        self.FeatureExtraction_output = 512
        self.AdaptiveAvgPool = nn.AdaptiveAvgPool2d((None, 1))

        """ Sequence modeling"""
        self.SequenceModeling = nn.Sequential(BidirectionalLSTM(self.FeatureExtraction_output, 256, 256),BidirectionalLSTM(256, 256, 256))
        self.SequenceModeling_output = 256

        """ Prediction """
        self.Prediction = Attention(self.SequenceModeling_output, 256, num_class)

    def forward(self, input, text, is_train=True):
        """ Transformation stage """
        if not self.stages['Trans'] == "None":
            input = self.Transformation(input)

        """ Feature extraction stage """
        visual_feature = self.FeatureExtraction(input)
        visual_feature = self.AdaptiveAvgPool(visual_feature.permute(0, 3, 1, 2))  # [b, c, h, w] -> [b, w, c, h]
        visual_feature = visual_feature.squeeze(3)

        """ Sequence modeling stage """
        if self.stages['Seq'] == 'BiLSTM':
            contextual_feature = self.SequenceModeling(visual_feature)
        else:
            contextual_feature = visual_feature  # for convenience. this is NOT contextually modeled by BiLSTM

        """ Prediction stage """
        prediction = self.Prediction(contextual_feature.contiguous(), text, False, batch_max_length=25)

        return prediction






import cv2
import IPython
import numpy as np
from typing import Tuple, Union
import math
from PIL import Image
import pytesseract
import traceback
import re
import torch
import torch.backends.cudnn as cudnn
import torch.utils.data
import torch.nn.functional as F
import matplotlib.pyplot as plt

import torchvision.transforms as transforms

from deskew import determine_skew

import time

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

# import sys
# sys.path.append('./LPRNet')
# sys.path.append('./MTCNN')
# # from LPRNet_Test import *
# from MTCNN import *

import easyocr
reader = easyocr.Reader(['en', 'de'])

from ultralytics import YOLO
yolo_model = YOLO('yolov8m.pt')
# plate_model = YOLO('best_plate_model.pt')
plate_model = YOLO('best_3.pt')

detected_plates = []

pytesseract.pytesseract.tesseract_cmd = 'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'

target_classes = ["truck", "bus", "car"]

characters = "0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ" # '0123456789abcdefghijklmnopqrstuvwxyz' # "0123456789ÄÜÖABCDEFGHIJKLMNOPQRSTUVWXYZ"

converter = AttnLabelConverter(characters)
num_class = len(converter.character)

model = Model(num_class)
model = torch.nn.DataParallel(model).to(device)
model.load_state_dict(torch.load("TPS-ResNet-BiLSTM-Attn.pth", map_location=device))
model.eval()

# STN = STNet()
# STN.to(device)
# STN.load_state_dict(torch.load('LPRNet/weights/Final_STN_model.pth', map_location=lambda storage, loc: storage))
# STN.eval()

# get grayscale image
def get_grayscale(image):
    return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# noise removal
def remove_noise(image):
    return cv2.medianBlur(image,5)

#thresholding
def thresholding(image):
    return cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

#dilation
def dilate(image):
    kernel = np.ones((5,5),np.uint8)
    return cv2.dilate(image, kernel, iterations = 1)

#erosion
def erode(image):
    kernel = np.ones((5,5),np.uint8)
    return cv2.erode(image, kernel, iterations = 1)

#opening - erosion followed by dilation
def opening(image):
    kernel = np.ones((5,5),np.uint8)
    return cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel)

#canny edge detection
def canny(image):
    return cv2.Canny(image, 100, 200)

#skew correction
def deskew(image):
    coords = np.column_stack(np.where(image > 0))
    angle = cv2.minAreaRect(coords)[-1]
    if angle < -45:
        angle = -(90 + angle)
    else:
        angle = -angle
    (h, w) = image.shape[:2]
    center = (w // 2, h // 2)
    M = cv2.getRotationMatrix2D(center, angle, 1.0)
    rotated = cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)
    return rotated

def compute_skew(src_img):
    if len(src_img.shape) == 3:
        h, w, _ = src_img.shape
    elif len(src_img.shape) == 2:
        h, w = src_img.shape
    else:
        print('unsupported image type')
    img = cv2.medianBlur(src_img, 3)
    edges = cv2.Canny(img,  threshold1 = 30,  threshold2 = 100, apertureSize = 3, L2gradient = True)
    lines = cv2.HoughLinesP(edges, 1, math.pi/180, 30, minLineLength=w / 4.0, maxLineGap=h/4.0)
    angle = 0.0
    nlines = lines.size
    #print(nlines)
    cnt = 0
    for x1, y1, x2, y2 in lines[0]:
        ang = np.arctan2(y2 - y1, x2 - x1)
        #print(ang)
        if math.fabs(ang) <= 30: # excluding extreme rotations
            angle += ang
            cnt += 1
    if cnt == 0:
        return 0.0
    return (angle / cnt)*180/math.pi

def rotate_image(image, angle):
    image_center = tuple(np.array(image.shape[1::-1]) / 2)
    rot_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)
    result = cv2.warpAffine(image, rot_mat, image.shape[1::-1], flags=cv2.INTER_LINEAR)
    return result

def get_deskew_angle(image):
    coords = np.column_stack(np.where(image > 0))
    angle = cv2.minAreaRect(coords)[-1]
    if angle<-45: angle=-angle
    return angle

def deskew_image(image, angle):
    (h, w) = image.shape[:2]
    center = (w // 2, h // 2)
    M = cv2.getRotationMatrix2D(center, angle, 1.0)
    return cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)

#template matching
def match_template(image, template):
    return cv2.matchTemplate(image, template, cv2.TM_CCOEFF_NORMED)

def rotate(
        image: np.ndarray, angle: float, background: Union[int, Tuple[int, int, int]]
) -> np.ndarray:
    old_width, old_height = image.shape[:2]
    angle_radian = math.radians(angle)
    width = abs(np.sin(angle_radian) * old_height) + abs(np.cos(angle_radian) * old_width)
    height = abs(np.sin(angle_radian) * old_width) + abs(np.cos(angle_radian) * old_height)

    image_center = tuple(np.array(image.shape[1::-1]) / 2)
    rot_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)
    rot_mat[1, 2] += (width - old_width) / 2
    rot_mat[0, 2] += (height - old_height) / 2
    return cv2.warpAffine(image, rot_mat, (int(round(height)), int(round(width))), borderValue=background)

# NN Layer License Plate Recognition "OCR"
def process_license(processed_frame, bounding_box, name, confidence, parentName, parentConfidence):
    x1, y1, x2, y2 = [int(x) for x in bounding_box]
    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(255,0,255),2)
    cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1-15),0,0.9,(255,0,255),2)

    # Rotation
    # license_frame = deskew(processed_frame[y1:y2,x1:x2])
    license_frame = processed_frame[y1:y2,x1:x2]

    # pframe = cv2.resize(license_frame, (94, 24), interpolation=cv2.INTER_CUBIC)
    # pframe = (np.transpose(np.float32(pframe), (2, 0, 1)) - 127.5)*0.0078125
    # pframe = torch.from_numpy(pframe).float().unsqueeze(0).to("cpu")
    # pframe = STN(pframe)
    # license_frame = convert_image(pframe)
    # plt.imshow(license_frame[:,:,::-1])
    # plt.show()

    grayscaled = get_grayscale(license_frame) # get_grayscale(np.array(license_frame))
    # angle = determine_skew(grayscaled)
    threshholded = thresholding(grayscaled) # deskew_image(thresholding(grayscaled), angle)

    # newdata=pytesseract.image_to_osd(threshholded)
    # angle=float(re.search('(?<=Rotate: )\d+', newdata).group(0))
    # print('osd angle:',angle)

    # angle = compute_skew(thresholding(grayscaled))
    # threshholded = rotate_image(thresholding(grayscaled), angle)
    # dilated = dilate(threshholded)
    # plt.imshow(deskew(threshholded))

    # print(angle)
    # plt.imshow(threshholded)
    # plt.show()

    max_conf = 0.0
    best_text = ""

    results = pytesseract.image_to_data(threshholded, config='-c tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')
    # NN Layer License Plate Recognition "Dummy OCR"

    parsedLines = results.split('\n')
    best_depth = len(parsedLines)-2
    for line in parsedLines:
        params = line.split()
        if len(params)==12 and float(params[10].replace('conf','0.0'))>=max_conf:
            max_conf = float(params[10].replace('conf','0.0'))
            best_text = params[11] if params[11]!="text" else "-"
            if max_conf>=0.0 and len(best_text)>0:
                detected_plates.append([parentConfidence*100, parentName.upper(), confidence*100, name.upper(), max_conf, best_text, "pytesseract"])
    cv2.putText(processed_frame, '{} {:.2f}%'.format("[tesse]: "+best_text, max_conf),(x1+10,y2+25),0,0.9,(0,0,255),3)

    result = reader.readtext(threshholded)
    text = ""
    conf = 0.0
    for res in result:
        if res[2]>conf:
            conf=res[2]
            text=res[1]
            if res[2]*100>max_conf:
                max_conf = res[2]*100
                best_text = res[1]
                if max_conf>=0.0 and len(best_text)>0:
                    detected_plates.append([parentConfidence*100, parentName.upper(), confidence*100, name.upper(), max_conf, best_text, "easyocr"])
    cv2.putText(processed_frame, '{} {:.2f}%'.format("[easy]: "+str(text), conf*100),(x1+10,y2+57),0,0.9,(200,200,200),3)

    transform = Image.fromarray(threshholded).resize((32, 100), Image.BICUBIC)
    transform = transforms.ToTensor()(transform)
    transform = transform.sub_(0.5).div_(0.5)
    image_tensors = [image for image in [transform]]
    image_tensors = torch.cat([t.unsqueeze(0) for t in image_tensors], 0)

    with torch.no_grad():
        batch_size = image_tensors.size(0)
        image = image_tensors.to(device)
        length_for_pred = torch.IntTensor([25] * batch_size).to(device)
        text_for_pred = torch.LongTensor(batch_size, 25 + 1).fill_(0).to(device)

        preds = model(image, text_for_pred, is_train=False)

        _, preds_index = preds.max(2)
        preds_str = converter.decode(preds_index, length_for_pred)
        preds_prob = F.softmax(preds, dim=2)
        preds_max_prob, _ = preds_prob.max(dim=2)
        for pred, pred_max_prob in zip(preds_str, preds_max_prob):
            pred_EOS = pred.find('[s]')
            pred = pred[:pred_EOS]  # prune after "end of sentence" token ([s])
            pred_max_prob = pred_max_prob[:pred_EOS]
            confidence_score = pred_max_prob.cumprod(dim=0)[-1]
            if confidence_score*100>max_conf:
                    max_conf = confidence_score*100
                    best_text = pred
                    if max_conf>=0.0 and len(best_text)>0:
                        detected_plates.append([parentConfidence*100, parentName.upper(), confidence*100, name.upper(), max_conf, best_text, "resnet"])
            cv2.putText(processed_frame, '{} {:.2f}%'.format("[resnet]: "+str(pred), confidence_score*100),(x1+10,y2+90),0,0.9,(255,255,255),3)

# NN Layer Vehicle Found -> License Detection
def process_vehicle(processed_frame, bounding_box, name, confidence):

    # Yolov8n Bounding Boxes
    x1, y1, x2, y2 = [int(x) for x in bounding_box]
    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(0,255,255),2)

    cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1+25),0,0.8,(0,255,255),2)
    vehicle_frame = processed_frame[y1:y2,x1:x2]
    results = plate_model(vehicle_frame, agnostic_nms=True, verbose=False)[0]
    for result in results:
        detection_count = result.boxes.shape[0]
        for i in range(detection_count):
            cls = int(result.boxes.cls[i].item())
            name2 = result.names[cls]
            confidence2 = float(result.boxes.conf[i].item())
            bounding_box2 = result.boxes.xyxy[i].cpu().numpy()
            a1, b1, a2, b2 = [int(x) for x in bounding_box2]
            # Ensure a1 < a2 and b1 < b2
            a1, a2 = min(a1, a2), max(a1, a2)
            b1, b2 = min(b1, b2), max(b1, b2)
            # if name2=="License_Plate": process_license(processed_frame, [x1+a1, y1+b1, x1+a2, y1+b2], name2, confidence2, name, confidence)
            process_license(processed_frame, [x1+a1, y1+b1, x1+a2, y1+b2], name2, confidence2, name, confidence)

# NN Layer Other Objects Detected [Not target classes]
def process_detection(processed_frame, bounding_box, name, confidence):
    x1, y1, x2, y2 = [int(x) for x in bounding_box]
    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(255,255,0),2)
    cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1+25),0,0.8,(255,255,0),2)

# NN Layer Yolov8 Detection
def yolov8n_objects(frame):
    processed_frame = frame
    start_time = time.time()
    results = yolo_model(processed_frame, agnostic_nms=True, verbose=False)[0]
    end_time = time.time()
    processing_time = end_time - start_time
    # print(f"Processing time yolov8: {processing_time:.3f} seconds")  # Output processing time
    for result in results:
        detection_count = result.boxes.shape[0]
        for i in range(detection_count):
            cls = int(result.boxes.cls[i].item())
            name = result.names[cls]
            confidence = float(result.boxes.conf[i].item())
            bounding_box = result.boxes.xyxy[i].cpu().numpy()
            x1, y1, x2, y2 = [int(x) for x in bounding_box]
            # Ensure x1 < x2 and y1 < y2
            x1, x2 = min(x1, x2), max(x1, x2)
            y1, y2 = min(y1, y2), max(y1, y2)
            if name in target_classes: process_vehicle(processed_frame, bounding_box, name, confidence)
            else: process_detection(processed_frame, bounding_box, name, confidence)
    return processed_frame

def process_frame(frame):
    processed_frame = yolov8n_objects(frame)
    # processed_frame = cv2.flip(processed_frame, 1)
    return processed_frame

def handleFrame(frame):
    processed_frame = process_frame(frame)
    _, processed_frame = cv2.imencode('.jpeg', processed_frame)
    display_handle.update(IPython.display.Image(data=processed_frame.tobytes()))
    IPython.display.clear_output(wait=True)

def handleRelease():
    cam.release()
    print("Source released.")
    correct_plates = []
    better_plates = []
    best_plate = []
    most_repeated_plates = []
    best_repeated_plates = []
    highest_score = 0.0
    if len(detected_plates)>0:
        repetitions = {}
        count, item = 0, ''
        for plate in detected_plates:
            if plate[0]>50.0 and plate[2]>50.0 and plate[4]>50.0: correct_plates.append(plate)
            if plate[0]>70.0 and plate[2]>70.0 and plate[4]>70.0: better_plates.append(plate)
            if plate[0] + plate[2] + plate[4] > highest_score:
                highest_score = plate[0] + plate[2] + plate[4]
                best_plate = plate
            repetitions[plate[5]] = repetitions.get(plate[5], 0) + 1
            if repetitions[plate[5]]>count: count, item = repetitions[plate[5]], plate[5]
        if len(repetitions.keys())>0:
            for itm in repetitions.keys():
                if repetitions[itm]==count:
                    repeated_detected = [plate for plate in detected_plates if plate[5]==itm]
                    best_percentages = []
                    for plate in repeated_detected:
                        if len(best_percentages)==0: best_percentages=plate
                        elif (plate[0] + plate[2] + plate[4])/3.0>(best_percentages[0] + best_percentages[2] + best_percentages[4])/3.0: best_percentages=plate
                    most_repeated_plates.append(best_percentages)
        if len(most_repeated_plates)>0:
            for plate in most_repeated_plates:
                best_repeated = []
                if len(best_repeated)==0: best_repeated=plate
                elif (plate[0] + plate[2] + plate[4])/3.0>(best_repeated[0] + best_repeated[2] + best_repeated[4])/3.0: best_repeated=plate
            best_repeated_plates.append(best_repeated)
    print("\nDetected Plates: \n", np.matrix(detected_plates))
    print("\nCorrectly Detected Plates: \n", np.matrix(correct_plates))
    print("\nBetter Detected Plates: \n", np.matrix(better_plates))
    print("\nBest Detected Plate: \n", np.matrix(best_plate))
    print("\nMost Repeated Plates: \n", np.matrix(most_repeated_plates))
    print("\nBest Repeated Plate: \n", np.matrix(best_repeated_plates))

inputType="video"
# cam = cv2.VideoCapture(0)
source = "data/video/sample.MP4" if inputType=="video" else 0
cam = cv2.VideoCapture(source)
display_handle=display(None, display_id=True)
every = 60

start = 0 if source!=0 else -1
end = int(cam.get(cv2.CAP_PROP_FRAME_COUNT)) if source!=0 else -1
if source!=0: cam.set(1, start)

frameCount=start
while_safety=0
saved_count=0

if not cam.isOpened(): print("Error: Could not open source.")
else:
    try:
        while True if end<0 else frameCount<end:
            _, frame = cam.read()
            if frame is None:
                if source!=0:
                    if while_safety > 2000: break
                    while_safety += 1
                    continue
                else:
                    print("Error: Could not capture frame.")
                    cam.release()
                    break

            if every>0:
                if (frameCount+1)%math.floor(every) == 0:
                    while_safety = 0
                    handleFrame(frame)
            else:
                while_safety=0
                handleFrame(frame)
            frameCount += 1
        handleRelease()
    except KeyboardInterrupt:
        handleRelease()
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Source could not be released.")
        except:
            pass
        traceback.print_exc()





# Neuronal Network Layered Stream with the better fps
import cv2
import IPython
import numpy as np
from typing import Tuple, Union
import math
from PIL import Image
import pytesseract
import traceback

from deskew import determine_skew

from ultralytics import YOLO
model = YOLO('yolov8n.pt')
plate_model = YOLO('best_plate_model.pt')

detected_plates = []

pytesseract.pytesseract.tesseract_cmd = 'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'

target_classes = ["truck", "bus", "car"]

# get grayscale image
def get_grayscale(image):
    return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# noise removal
def remove_noise(image):
    return cv2.medianBlur(image,5)

#thresholding
def thresholding(image):
    return cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

#dilation
def dilate(image):
    kernel = np.ones((5,5),np.uint8)
    return cv2.dilate(image, kernel, iterations = 1)

#erosion
def erode(image):
    kernel = np.ones((5,5),np.uint8)
    return cv2.erode(image, kernel, iterations = 1)

#opening - erosion followed by dilation
def opening(image):
    kernel = np.ones((5,5),np.uint8)
    return cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel)

#canny edge detection
def canny(image):
    return cv2.Canny(image, 100, 200)

#skew correction
def deskew(image):
    coords = np.column_stack(np.where(image > 0))
    angle = cv2.minAreaRect(coords)[-1]
    if angle < -45:
        angle = -(90 + angle)
    else:
        angle = -angle
    (h, w) = image.shape[:2]
    center = (w // 2, h // 2)
    M = cv2.getRotationMatrix2D(center, angle, 1.0)
    rotated = cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)
    return rotated

def get_deskew_angle(image):
    coords = np.column_stack(np.where(image > 0))
    angle = cv2.minAreaRect(coords)[-1]
    if angle<-45: angle=-angle
    return angle

def deskew_image(image, angle):
    (h, w) = image.shape[:2]
    center = (w // 2, h // 2)
    M = cv2.getRotationMatrix2D(center, angle, 1.0)
    return cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)

#template matching
def match_template(image, template):
    return cv2.matchTemplate(image, template, cv2.TM_CCOEFF_NORMED)

def rotate(
        image: np.ndarray, angle: float, background: Union[int, Tuple[int, int, int]]
) -> np.ndarray:
    old_width, old_height = image.shape[:2]
    angle_radian = math.radians(angle)
    width = abs(np.sin(angle_radian) * old_height) + abs(np.cos(angle_radian) * old_width)
    height = abs(np.sin(angle_radian) * old_width) + abs(np.cos(angle_radian) * old_height)

    image_center = tuple(np.array(image.shape[1::-1]) / 2)
    rot_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)
    rot_mat[1, 2] += (width - old_width) / 2
    rot_mat[0, 2] += (height - old_height) / 2
    return cv2.warpAffine(image, rot_mat, (int(round(height)), int(round(width))), borderValue=background)

# NN Layer License Plate Recognition "OCR"
def process_license(processed_frame, bounding_box, name, confidence, parentName, parentConfidence):
    x1, y1, x2, y2 = [int(x) for x in bounding_box]
    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(255,0,255),2)
    cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1-15),0,0.7,(255,0,255))

    # Rotation
    # license_frame = deskew(processed_frame[y1:y2,x1:x2])
    license_frame = processed_frame[y1:y2,x1:x2]
    grayscaled = get_grayscale(np.array(license_frame))
    threshholded = thresholding(grayscaled)
    # angle = determine_skew(threshholded)
    # print(angle)
    # threshholded = rotate(threshholded, angle, (0, 0, 0))

    results = pytesseract.image_to_data(threshholded, config='--psm 6 --oem 3 -c tessedit_char_whitelist=ÄÜÖABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')

    max_conf = 0.0
    best_text = ""
    parsedLines = results.split('\n')
    best_depth = len(parsedLines)-2
    for line in parsedLines:
        params = line.split()
        if len(params)==12 and float(params[10].replace('conf','0.0'))>max_conf:
            max_conf = float(params[10])
            best_text = params[11]
            if max_conf>0.0 and len(best_text)>0:
                detected_plates.append([parentConfidence*100, parentName.upper(), confidence*100, name.upper(), max_conf, best_text])
    cv2.putText(processed_frame, '{} {:.2f}%'.format(best_text, max_conf),(x1+10,y2+25),0,0.9,(0,0,255),3)

# NN Layer Vehicle Found -> License Detection
def process_vehicle(processed_frame, bounding_box, name, confidence):
    x1, y1, x2, y2 = [int(x) for x in bounding_box]
    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(0,255,255),2)
    cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1-15),0,0.7,(0,255,255))
    vehicle_frame = processed_frame[y1:y2,x1:x2]
    results = plate_model(vehicle_frame, agnostic_nms=True, verbose=False)[0]
    for result in results:
        detection_count = result.boxes.shape[0]
        for i in range(detection_count):
            cls = int(result.boxes.cls[i].item())
            name2 = result.names[cls]
            confidence2 = float(result.boxes.conf[i].item())
            bounding_box2 = result.boxes.xyxy[i].cpu().numpy()
            a1, b1, a2, b2 = [int(x) for x in bounding_box2]
            # Ensure a1 < a2 and b1 < b2
            a1, a2 = min(a1, a2), max(a1, a2)
            b1, b2 = min(b1, b2), max(b1, b2)
            if name2=="License_Plate": process_license(processed_frame, [x1+a1, y1+b1, x1+a2, y1+b2], name2, confidence2, name, confidence)

# NN Layer Other Objects Detected
def process_detection(processed_frame, bounding_box, name, confidence):
    x1, y1, x2, y2 = [int(x) for x in bounding_box]
    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(255,255,0),2)
    cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1-15),0,0.7,(255,255,0))

# NN Layer Yolov8 Detection
def process_frame(frame):
    processed_frame = frame
    # processed_frame = cv2.flip(processed_frame, 1)
    results = model(processed_frame, agnostic_nms=True, verbose=False)[0]
    for result in results:
        detection_count = result.boxes.shape[0]
        for i in range(detection_count):
            cls = int(result.boxes.cls[i].item())
            name = result.names[cls]
            confidence = float(result.boxes.conf[i].item())
            bounding_box = result.boxes.xyxy[i].cpu().numpy()
            x1, y1, x2, y2 = [int(x) for x in bounding_box]
            # Ensure x1 < x2 and y1 < y2
            x1, x2 = min(x1, x2), max(x1, x2)
            y1, y2 = min(y1, y2), max(y1, y2)
            if name in target_classes: process_vehicle(processed_frame, bounding_box, name, confidence)
            else: process_detection(processed_frame, bounding_box, name, confidence)
    return processed_frame

cam = cv2.VideoCapture(0)
display_handle=display(None, display_id=True)

if not cam.isOpened(): print("Error: Could not open camera.")
else:
    try:
        while True:
            ret, frame = cam.read()
            if not ret:
                print("Error: Could not capture frame.")
                cam.release()
                break
            processed_frame = process_frame(frame)
            _, processed_frame = cv2.imencode('.jpeg', processed_frame)
            display_handle.update(IPython.display.Image(data=processed_frame.tobytes()))
            IPython.display.clear_output(wait=True)
    except KeyboardInterrupt:
        cam.release()
        print("Camera released.")

        correct_plates = []
        better_plates = []
        best_plate = []
        most_repeated_plates = []
        best_repeated_plates = []
        highest_score = 0.0
        if len(detected_plates)>0:
            repetitions = {}
            count, item = 0, ''
            for plate in detected_plates:
                if plate[0]>50.0 and plate[2]>50.0 and plate[4]>50.0: correct_plates.append(plate)
                if plate[0]>70.0 and plate[2]>70.0 and plate[4]>70.0: better_plates.append(plate)
                if plate[0] + plate[2] + plate[4] > highest_score:
                    highest_score = plate[0] + plate[2] + plate[4]
                    best_plate = plate
                repetitions[plate[5]] = repetitions.get(plate[5], 0) + 1
                if repetitions[plate[5]]>count: count, item = repetitions[plate[5]], plate[5]
            if len(repetitions.keys())>0:
                for itm in repetitions.keys():
                    if repetitions[itm]==count:
                        repeated_detected = [plate for plate in detected_plates if plate[5]==itm]
                        best_percentages = []
                        for plate in repeated_detected:
                            if len(best_percentages)==0: best_percentages=plate
                            elif (plate[0] + plate[2] + plate[4])/3.0>(best_percentages[0] + best_percentages[2] + best_percentages[4])/3.0: best_percentages=plate
                        most_repeated_plates.append(best_percentages)
            if len(most_repeated_plates)>0:
                for plate in most_repeated_plates:
                    best_repeated = []
                    if len(best_repeated)==0: best_repeated=plate
                    elif (plate[0] + plate[2] + plate[4])/3.0>(best_repeated[0] + best_repeated[2] + best_repeated[4])/3.0: best_repeated=plate
                best_repeated_plates.append(best_repeated)

        print("\nDetected Plates: \n", np.matrix(detected_plates))
        print("\nCorrectly Detected Plates: \n", np.matrix(correct_plates))
        print("\nBetter Detected Plates: \n", np.matrix(better_plates))
        print("\nBest Detected Plate: \n", np.matrix(best_plate))
        print("\nMost Repeated Plates: \n", np.matrix(most_repeated_plates))
        print("\nBest Repeated Plate: \n", np.matrix(best_repeated_plates))
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Camera could not be released.")
        except:
            pass
        traceback.print_exc()





# Neuronal Network Layered Stream with the better fps
import cv2
import IPython
import numpy as np
from typing import Tuple, Union
import math
from PIL import Image
import pytesseract
import traceback

from deskew import determine_skew

from ultralytics import YOLO
model = YOLO('yolov8n.pt')
plate_model = YOLO('best_plate_model.pt')

detected_plates = []

pytesseract.pytesseract.tesseract_cmd = 'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'

target_classes = ["truck", "bus", "car"]

# get grayscale image
def get_grayscale(image):
    return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# noise removal
def remove_noise(image):
    return cv2.medianBlur(image,5)

#thresholding
def thresholding(image):
    return cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

#dilation
def dilate(image):
    kernel = np.ones((5,5),np.uint8)
    return cv2.dilate(image, kernel, iterations = 1)

#erosion
def erode(image):
    kernel = np.ones((5,5),np.uint8)
    return cv2.erode(image, kernel, iterations = 1)

#opening - erosion followed by dilation
def opening(image):
    kernel = np.ones((5,5),np.uint8)
    return cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel)

#canny edge detection
def canny(image):
    return cv2.Canny(image, 100, 200)

#skew correction
def deskew(image):
    coords = np.column_stack(np.where(image > 0))
    angle = cv2.minAreaRect(coords)[-1]
    if angle < -45:
        angle = -(90 + angle)
    else:
        angle = -angle
    (h, w) = image.shape[:2]
    center = (w // 2, h // 2)
    M = cv2.getRotationMatrix2D(center, angle, 1.0)
    rotated = cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)
    return rotated

def get_deskew_angle(image):
    coords = np.column_stack(np.where(image > 0))
    angle = cv2.minAreaRect(coords)[-1]
    if angle<-45: angle=-angle
    return angle

def deskew_image(image, angle):
    (h, w) = image.shape[:2]
    center = (w // 2, h // 2)
    M = cv2.getRotationMatrix2D(center, angle, 1.0)
    return cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)

#template matching
def match_template(image, template):
    return cv2.matchTemplate(image, template, cv2.TM_CCOEFF_NORMED)

def rotate(
        image: np.ndarray, angle: float, background: Union[int, Tuple[int, int, int]]
) -> np.ndarray:
    old_width, old_height = image.shape[:2]
    angle_radian = math.radians(angle)
    width = abs(np.sin(angle_radian) * old_height) + abs(np.cos(angle_radian) * old_width)
    height = abs(np.sin(angle_radian) * old_width) + abs(np.cos(angle_radian) * old_height)

    image_center = tuple(np.array(image.shape[1::-1]) / 2)
    rot_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)
    rot_mat[1, 2] += (width - old_width) / 2
    rot_mat[0, 2] += (height - old_height) / 2
    return cv2.warpAffine(image, rot_mat, (int(round(height)), int(round(width))), borderValue=background)

# NN Layer License Plate Recognition "OCR"
def process_license(processed_frame, bounding_box, name, confidence, parentName, parentConfidence):
    x1, y1, x2, y2 = [int(x) for x in bounding_box]
    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(255,0,255),2)
    cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1-15),0,0.7,(255,0,255))

    # Rotation
    # license_frame = deskew(processed_frame[y1:y2,x1:x2])
    license_frame = processed_frame[y1:y2,x1:x2]
    grayscaled = get_grayscale(np.array(license_frame))
    threshholded = thresholding(grayscaled)
    angle = determine_skew(threshholded)
    print(angle)
    threshholded = rotate(threshholded, angle, (0, 0, 0))

    results = pytesseract.image_to_data(threshholded, config='--psm 6 --oem 3 -c tessedit_char_whitelist=ÄÜÖABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')

    max_conf = 0.0
    best_text = ""
    parsedLines = results.split('\n')
    best_depth = len(parsedLines)-2
    for line in parsedLines:
        params = line.split()
        if len(params)==12 and float(params[10].replace('conf','0.0'))>max_conf:
            max_conf = float(params[10])
            best_text = params[11]
            if max_conf>0.0 and len(best_text)>0:
                detected_plates.append([parentConfidence*100, parentName.upper(), confidence*100, name.upper(), max_conf, best_text])
    cv2.putText(processed_frame, '{} {:.2f}%'.format(best_text, max_conf),(x1+10,y2+25),0,0.9,(0,0,255),3)

# NN Layer Vehicle Found -> License Detection
def process_vehicle(processed_frame, bounding_box, name, confidence):
    x1, y1, x2, y2 = [int(x) for x in bounding_box]
    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(0,255,255),2)
    cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1-15),0,0.7,(0,255,255))
    vehicle_frame = processed_frame[y1:y2,x1:x2]
    results = plate_model(vehicle_frame, agnostic_nms=True, verbose=False)[0]
    for result in results:
        detection_count = result.boxes.shape[0]
        for i in range(detection_count):
            cls = int(result.boxes.cls[i].item())
            name2 = result.names[cls]
            confidence2 = float(result.boxes.conf[i].item())
            bounding_box2 = result.boxes.xyxy[i].cpu().numpy()
            a1, b1, a2, b2 = [int(x) for x in bounding_box2]
            # Ensure a1 < a2 and b1 < b2
            a1, a2 = min(a1, a2), max(a1, a2)
            b1, b2 = min(b1, b2), max(b1, b2)
            if name2=="licence": process_license(processed_frame, [x1+a1, y1+b1, x1+a2, y1+b2], "license_plate", confidence2, name, confidence)

# NN Layer Other Objects Detected
def process_detection(processed_frame, bounding_box, name, confidence):
    x1, y1, x2, y2 = [int(x) for x in bounding_box]
    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(255,255,0),2)
    cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1-15),0,0.7,(255,255,0))

# NN Layer Yolov8 Detection
def process_frame(frame):
    processed_frame = frame
    # processed_frame = cv2.flip(processed_frame, 1)
    results = model(processed_frame, agnostic_nms=True, verbose=False)[0]
    for result in results:
        detection_count = result.boxes.shape[0]
        for i in range(detection_count):
            cls = int(result.boxes.cls[i].item())
            name = result.names[cls]
            confidence = float(result.boxes.conf[i].item())
            bounding_box = result.boxes.xyxy[i].cpu().numpy()
            x1, y1, x2, y2 = [int(x) for x in bounding_box]
            # Ensure x1 < x2 and y1 < y2
            x1, x2 = min(x1, x2), max(x1, x2)
            y1, y2 = min(y1, y2), max(y1, y2)
            if name in target_classes: process_vehicle(processed_frame, bounding_box, name, confidence)
            else: process_detection(processed_frame, bounding_box, name, confidence)
    return processed_frame

cam = cv2.VideoCapture(0)
display_handle=display(None, display_id=True)

if not cam.isOpened(): print("Error: Could not open camera.")
else:
    try:
        while True:
            ret, frame = cam.read()
            if not ret:
                print("Error: Could not capture frame.")
                cam.release()
                break
            processed_frame = process_frame(frame)
            _, processed_frame = cv2.imencode('.jpeg', processed_frame)
            display_handle.update(IPython.display.Image(data=processed_frame.tobytes()))
            IPython.display.clear_output(wait=True)
    except KeyboardInterrupt:
        cam.release()
        print("Camera released.")

        correct_plates = []
        best_plates = []
        most_repeated_plates = []
        best_repeated_plates = []
        if len(detected_plates)>0:
            repetitions = {}
            count, item = 0, ''
            for plate in detected_plates:
                if plate[0]>50.0 and plate[2]>50.0 and plate[4]>50.0: correct_plates.append(plate)
                if plate[0]>70.0 and plate[2]>70.0 and plate[4]>70.0: best_plates.append(plate)
                repetitions[plate[5]] = repetitions.get(plate[5], 0) + 1
                if repetitions[plate[5]]>count: count, item = repetitions[plate[5]], plate[5]
            if len(repetitions.keys())>0:
                for itm in repetitions.keys():
                    if repetitions[itm]==count:
                        repeated_detected = [plate for plate in detected_plates if plate[5]==itm]
                        best_percentages = []
                        for plate in repeated_detected:
                            if len(best_percentages)==0: best_percentages=plate
                            elif (plate[0] + plate[2] + plate[4])/3.0>(best_percentages[0] + best_percentages[2] + best_percentages[4])/3.0: best_percentages=plate
                        most_repeated_plates.append(best_percentages)
            if len(most_repeated_plates)>0:
                for plate in most_repeated_plates:
                    best_repeated = []
                    if len(best_repeated)==0: best_repeated=plate
                    elif (plate[0] + plate[2] + plate[4])/3.0>(best_repeated[0] + best_repeated[2] + best_repeated[4])/3.0: best_repeated=plate
                best_repeated_plates.append(best_repeated)

        print("\nDetected Plates: \n", np.matrix(detected_plates))
        print("\nCorrectly Detected Plates: \n", np.matrix(correct_plates))
        print("\nBest Detected Plates: \n", np.matrix(best_plates))
        print("\nMost Repeated Plates: \n", np.matrix(most_repeated_plates))
        print("\nBest Repeated Plate: \n", np.matrix(best_repeated_plates))
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Camera could not be released.")
        except:
            pass
        traceback.print_exc()





import cv2
import numpy as np
from IPython.display import display, Image
import io

# Load YOLOv7 for object detection
net = cv2.dnn.readNet("yolov7.weights", "yolov7.cfg")
classes = []
with open("coco.names", "r") as f:
    classes = [line.strip() for line in f.readlines()]
layer_names = net.getLayerNames()
output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]

# Load pre-trained CNN model for license plate recognition
license_plate_model = YourCustomLicensePlateRecognitionModel()

# Define the colors for the bounding boxes
colors = [(0, 255, 0)]

# Define the font settings
font = cv2.FONT_HERSHEY_PLAIN
font_scale = 1
thickness = 1

# Load video capture
cap = cv2.VideoCapture(0)

while True:
    _, frame = cap.read()
    height, width, channels = frame.shape

    # Detecting objects (trucks) using YOLOv7
    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)
    net.setInput(blob)
    outs = net.forward(output_layers)

    # Showing information on the screen
    for out in outs:
        for detection in out:
            scores = detection[5:]
            class_id = np.argmax(scores)
            confidence = scores[class_id]
            if confidence > 0.5 and class_id == 7:  # 7 is the class ID for truck in COCO dataset
                # Object detected (truck)
                center_x = int(detection[0] * width)
                center_y = int(detection[1] * height)
                w = int(detection[2] * width)
                h = int(detection[3] * height)

                # Rectangle coordinates
                x = int(center_x - w / 2)
                y = int(center_y - h / 2)

                # Crop the detected license plate region
                license_plate_img = frame[y:y+h, x:x+w]

                # Perform license plate recognition using CNN
                license_plate_text = license_plate_model.predict(license_plate_img)

                # Draw bounding box around the truck
                color = colors[0]
                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)

                # Draw text (license plate) on the frame
                cv2.putText(frame, license_plate_text, (x, y - 5), font, font_scale, color, thickness)

    # Convert the frame to RGB format
    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    # Convert the frame to bytes
    _, frame_bytes = cv2.imencode('.jpg', frame_rgb)

    # Display the frame in the notebook
    display(Image(data=frame_bytes.tobytes()))

    key = cv2.waitKey(1)
    if key == 27:  # ESC key to exit
        break

cap.release()
cv2.destroyAllWindows()


# Neuronal Network Layered Stream with the better fps
import cv2
import IPython
import numpy as np
from typing import Tuple, Union
import math
from PIL import Image
import traceback

import sys
sys.path.append('./LPRNet')
sys.path.append('./MTCNN')
from LPRNet_Test import *
from MTCNN import *
import argparse
import torch
import time
import cv2
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

detected_plates = []

# NN Layer License Plate Recognition "OCR"
def process_license(processed_frame, bounding_box, name, confidence, parentName, parentConfidence):
    x1, y1, x2, y2 = [int(x) for x in bounding_box]
    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(255,0,255),2)
    cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1-15),0,0.7,(255,0,255))

    # Rotation
    # license_frame = deskew(processed_frame[y1:y2,x1:x2])
    license_frame = processed_frame[y1:y2,x1:x2]
    grayscaled = get_grayscale(np.array(license_frame))
    threshholded = thresholding(grayscaled)
    angle = determine_skew(threshholded)
    print(angle)
    threshholded = rotate(threshholded, angle, (0, 0, 0))

    results = pytesseract.image_to_data(threshholded, config='--psm 6 --oem 3 -c tessedit_char_whitelist=ÄÜÖABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')

    max_conf = 0.0
    best_text = ""
    parsedLines = results.split('\n')
    best_depth = len(parsedLines)-2
    for line in parsedLines:
        params = line.split()
        if len(params)==12 and float(params[10].replace('conf','0.0'))>max_conf:
            max_conf = float(params[10])
            best_text = params[11]
            if max_conf>0.0 and len(best_text)>0:
                detected_plates.append([parentConfidence*100, parentName.upper(), confidence*100, name.upper(), max_conf, best_text])
    cv2.putText(processed_frame, '{} {:.2f}%'.format(best_text, max_conf),(x1+10,y2+25),0,0.9,(0,0,255),3)

# NN Layer Vehicle Found -> License Detection
def process_vehicle(processed_frame, bounding_box, name, confidence):
    x1, y1, x2, y2 = [int(x) for x in bounding_box]
    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(0,255,255),2)
    cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1-15),0,0.7,(0,255,255))
    vehicle_frame = processed_frame[y1:y2,x1:x2]
    results = plate_model(vehicle_frame, agnostic_nms=True, verbose=False)[0]
    for result in results:
        detection_count = result.boxes.shape[0]
        for i in range(detection_count):
            cls = int(result.boxes.cls[i].item())
            name2 = result.names[cls]
            confidence2 = float(result.boxes.conf[i].item())
            bounding_box2 = result.boxes.xyxy[i].cpu().numpy()
            a1, b1, a2, b2 = [int(x) for x in bounding_box2]
            # Ensure a1 < a2 and b1 < b2
            a1, a2 = min(a1, a2), max(a1, a2)
            b1, b2 = min(b1, b2), max(b1, b2)
            if name2=="licence": process_license(processed_frame, [x1+a1, y1+b1, x1+a2, y1+b2], "license_plate", confidence2, name, confidence)

# NN Layer Other Objects Detected
def process_detection(processed_frame, bounding_box, name, confidence):
    x1, y1, x2, y2 = [int(x) for x in bounding_box]
    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(255,255,0),2)
    cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1-15),0,0.7,(255,255,0))

# NN Layer Yolov8 Detection
def process_frame(frame):
    processed_frame = frame
    # processed_frame = cv2.flip(processed_frame, 1)
    # results = model(processed_frame, agnostic_nms=True, verbose=False)[0]
    # for result in results:
    #     detection_count = result.boxes.shape[0]
    #     for i in range(detection_count):
    #         cls = int(result.boxes.cls[i].item())
    #         name = result.names[cls]
    #         confidence = float(result.boxes.conf[i].item())
    #         bounding_box = result.boxes.xyxy[i].cpu().numpy()
    #         x1, y1, x2, y2 = [int(x) for x in bounding_box]
    #         # Ensure x1 < x2 and y1 < y2
    #         x1, x2 = min(x1, x2), max(x1, x2)
    #         y1, y2 = min(y1, y2), max(y1, y2)
    #         if name in target_classes: process_vehicle(processed_frame, bounding_box, name, confidence)
    #         else: process_detection(processed_frame, bounding_box, name, confidence)

    input = np.copy(frame)
    bboxes = create_mtcnn_net(input, (50, 15), device, p_model_path='MTCNN/weights/pnet_Weights', o_model_path='MTCNN/weights/onet_Weights')

    for i in range(bboxes.shape[0]):
        bbox = bboxes[i, :4]
        x1, y1, x2, y2 = [int(bbox[j]) for j in range(4)]
        x1, x2 = min(x1, x2), max(x1, x2)
        y1, y2 = min(y1, y2), max(y1, y2)
        process_license(processed_frame, bounding_box)
    return processed_frame

cam = cv2.VideoCapture(0)
display_handle=display(None, display_id=True)

if not cam.isOpened(): print("Error: Could not open camera.")
else:
    try:
        while True:
            ret, frame = cam.read()
            if not ret:
                print("Error: Could not capture frame.")
                cam.release()
                break
            processed_frame = process_frame(frame)
            _, processed_frame = cv2.imencode('.jpeg', processed_frame)
            display_handle.update(IPython.display.Image(data=processed_frame.tobytes()))
            IPython.display.clear_output(wait=True)
    except KeyboardInterrupt:
        cam.release()
        print("Camera released.")

        correct_plates = []
        best_plates = []
        most_repeated_plates = []
        best_repeated_plates = []
        if len(detected_plates)>0:
            repetitions = {}
            count, item = 0, ''
            for plate in detected_plates:
                if plate[0]>50.0 and plate[2]>50.0 and plate[4]>50.0: correct_plates.append(plate)
                if plate[0]>70.0 and plate[2]>70.0 and plate[4]>70.0: best_plates.append(plate)
                repetitions[plate[5]] = repetitions.get(plate[5], 0) + 1
                if repetitions[plate[5]]>count: count, item = repetitions[plate[5]], plate[5]
            if len(repetitions.keys())>0:
                for itm in repetitions.keys():
                    if repetitions[itm]==count:
                        repeated_detected = [plate for plate in detected_plates if plate[5]==itm]
                        best_percentages = []
                        for plate in repeated_detected:
                            if len(best_percentages)==0: best_percentages=plate
                            elif (plate[0] + plate[2] + plate[4])/3.0>(best_percentages[0] + best_percentages[2] + best_percentages[4])/3.0: best_percentages=plate
                        most_repeated_plates.append(best_percentages)
            if len(most_repeated_plates)>0:
                for plate in most_repeated_plates:
                    best_repeated = []
                    if len(best_repeated)==0: best_repeated=plate
                    elif (plate[0] + plate[2] + plate[4])/3.0>(best_repeated[0] + best_repeated[2] + best_repeated[4])/3.0: best_repeated=plate
                best_repeated_plates.append(best_repeated)

        print("\nDetected Plates: \n", np.matrix(detected_plates))
        print("\nCorrectly Detected Plates: \n", np.matrix(correct_plates))
        print("\nBest Detected Plates: \n", np.matrix(best_plates))
        print("\nMost Repeated Plates: \n", np.matrix(most_repeated_plates))
        print("\nBest Repeated Plate: \n", np.matrix(best_repeated_plates))
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Camera could not be released.")
        except:
            pass
        traceback.print_exc()


import torchvision
from torchvision.models.detection import *

 # DetectionModel("FCOS", "resnet50_fpn", fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights, "fcos_resnet", (255,0,0)),
 #    DetectionModel("RetinaNet", "resnet50_fpn", retinanet_resnet50_fpn, RetinaNet_ResNet50_FPN_Weights, "retinanet_resnet", (255,255,0)),
 #    DetectionModel("RetinaNet", "resnet50_fpn_v2", retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights, "retinanet_resnet_v2", (0,234,255)),
 #    DetectionModel("FasterRCNN", "resnet50_fpn", fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights, "faster_rcnn_resnet", (170,0,255)),
 #    DetectionModel("FasterRCNN", "resnet50_fpn_v2", fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights, "faster_rcnn_resnet_v2", (255,127,0)),
 #    DetectionModel("FasterRCNN", "mobilenet_v3_large_fpn", fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights, "faster_rcnn_mobilenet_v3", (191,255,0)),
 #    DetectionModel("FasterRCNN", "mobilenet_v3_large_320_fpn", fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights, "faster_rcnn_mobilenet_v3_320", (0,149,255)),
 #    DetectionModel("MaskRCNN", "resnet50_fpn", maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights, "mask_rcnn_resnet", (255,0,170)),
 #    DetectionModel("MaskRCNN", "resnet50_fpn_v2", maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights, "mask_rcnn_resnet_v2", (255,212,0)),
 #    DetectionModel("SSD300", "vgg16", ssd300_vgg16, SSD300_VGG16_Weights, "ssd_vgg16", (106,255,0)),
 #    DetectionModel("SSDLite320", "mobilenet_v3_large", ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights, "ssd_mobilenet_v3", (0,64,255)),

model = fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)
model.eval()

# Load pre-trained Faster R-CNN
# model_faster_rcnn = fasterrcnn_resnet50_fpn(pretrained=True)
# model_faster_rcnn.eval()

# Load pre-trained Mask R-CNN
# model_mask_rcnn = maskrcnn_resnet50_fpn(pretrained=True)
# model_mask_rcnn.eval()


"data/video/sample.MP4"


import cv2
import numpy as np
from PIL import Image
from IPython.display import display, clear_output
import torchvision.transforms as T
import torch
import time
import traceback
import math

transform = T.Compose([T.ToTensor()])

COCO_INSTANCE_CATEGORY_NAMES = [
    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',
    'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',
    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',
    'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'N/A', 'N/A',
    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',
    'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',
    'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife',
    'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',
    'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',
    'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',
    'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote',
    'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',
    'refrigerator', 'N/A', 'book', 'clock', 'vase', 'scissors',
    'teddy bear', 'hair drier', 'toothbrush'
]

def process_frame(model, frame):
    # Transform the frame to tensor
    frame = transform(frame).unsqueeze(0)
    with torch.no_grad():
        start_time = time.time()  # Start time measurement
        prediction = model(frame)
        end_time = time.time()  # End time measurement
    processing_time = end_time - start_time
    print(f"Processing time: {processing_time:.3f} seconds")  # Output processing time
    return prediction

def process(frame):
    prediction = process_frame(model_faster_rcnn, frame)

    # Visualization: Draw bounding boxes and masks
    for element in range(len(prediction[0]['boxes'])):
        box = prediction[0]['boxes'][element].cpu().numpy()
        score = prediction[0]['scores'][element].cpu().numpy()
        label_idx = prediction[0]['labels'][element].cpu().numpy()

        if score > 0.5:  # Filter out detections with low confidence
            label_name = COCO_INSTANCE_CATEGORY_NAMES[label_idx]  # Map the label index to its name
            x1, y1, x2, y2 = map(int, box)
            label_text = f'{label_name} {score:.2f}'  # Use label_name instead of label_idx
            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
            cv2.putText(frame, label_text, (x1, y1 + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

    return frame

def handleFrame(frame):
    processed_frame = process(frame)

    # Display in Jupyter Notebook
    img = Image.fromarray(processed_frame, 'RGB')
    display(img)
    clear_output(wait=True)
    # _, processed_frame = cv2.imencode('.jpeg', processed_frame)
    # display_handle.update(IPython.display.Image(data=processed_frame.tobytes()))
    # IPython.display.clear_output(wait=True)

def handleRelease():
    cam.release()
    print("Source released.")

inputType="video"
# cam = cv2.VideoCapture(0)
source = "data/video/sample.MP4" if inputType=="video" else 0
cam = cv2.VideoCapture(source)
display_handle=display(None, display_id=True)
every = 60

start = 0 if source!=0 else -1
end = int(cam.get(cv2.CAP_PROP_FRAME_COUNT)) if source!=0 else -1
if source!=0: cam.set(1, start)

frameCount=start
while_safety=0
saved_count=0

if not cam.isOpened(): print("Error: Could not open source.")
else:
    try:
        while True if end<0 else frameCount<end:
            _, frame = cam.read()
            if frame is None:
                if source!=0:
                    if while_safety > 2000: break
                    while_safety += 1
                    continue
                else:
                    print("Error: Could not capture frame.")
                    cam.release()
                    break

            if every>0:
                if (frameCount+1)%math.floor(every) == 0:
                    while_safety = 0
                    handleFrame(frame)
            else:
                while_safety=0
                handleFrame(frame)
            frameCount += 1
        handleRelease()
    except KeyboardInterrupt:
        handleRelease()
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Source could not be released.")
        except:
            pass
        traceback.print_exc()





import cv2
import IPython
import numpy as np
from typing import Tuple, Union
import math
from PIL import Image
import pytesseract
import traceback
import re
import torchvision.transforms as transforms
import time
from IPython.display import display, clear_output

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# import easyocr
# reader = easyocr.Reader(['de'])

from ultralytics import YOLO
yolo_model = YOLO('yolov8m.pt')
plate_model = YOLO('best_plate_model.pt')

detected_plates = []

pytesseract.pytesseract.tesseract_cmd = 'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'

target_classes = ["truck", "bus", "car"]

characters = "0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ" # '0123456789abcdefghijklmnopqrstuvwxyz' # "0123456789ÄÜÖABCDEFGHIJKLMNOPQRSTUVWXYZ"

# get grayscale image
def get_grayscale(image):
    return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

#thresholding
def thresholding(image):
    return cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

# NN Layer License Plate Recognition "OCR"
def process_license(processed_frame, bounding_box, name, confidence, parentName, parentConfidence):
    x1, y1, x2, y2 = [int(x) for x in bounding_box]
    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(255,0,255),2)
    cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1-15),0,0.9,(255,0,255),2)

    # Rotation
    license_frame = processed_frame[y1:y2,x1:x2]

    grayscaled = get_grayscale(license_frame)
    threshholded = thresholding(grayscaled)

    max_conf = 0.0
    best_text = ""

    results = pytesseract.image_to_data(threshholded, config='--psm 6 --oem 3 -c tessedit_char_whitelist=ÄÜÖABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')
    # NN Layer License Plate Recognition "Dummy OCR"

    parsedLines = results.split('\n')
    best_depth = len(parsedLines)-2
    for line in parsedLines:
        params = line.split()
        if len(params)==12 and float(params[10].replace('conf','0.0'))>max_conf:
            max_conf = float(params[10])
            best_text = params[11]
            if max_conf>0.0 and len(best_text)>0:
                detected_plates.append([parentConfidence*100, parentName.upper(), confidence*100, name.upper(), max_conf, best_text, "pytesseract"])
    cv2.putText(processed_frame, '{} {:.2f}%'.format("[tesse]: "+best_text, max_conf),(x1+10,y2+25),0,0.9,(0,0,255),3)

# NN Layer Vehicle Found -> License Detection
def process_vehicle(processed_frame, bounding_box, name, confidence):

    # Yolov8n Bounding Boxes
    x1, y1, x2, y2 = [int(x) for x in bounding_box]
    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(0,255,255),2)

    cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1+25),0,0.8,(0,255,255),2)
    vehicle_frame = processed_frame[y1:y2,x1:x2]
    start_time = time.time()
    results = plate_model(vehicle_frame, agnostic_nms=True, verbose=False)[0]
    end_time = time.time()
    processing_time = end_time - start_time
    print(f"Processing time yolov8_license: {processing_time:.3f} seconds")  # Output processing time
    for result in results:
        detection_count = result.boxes.shape[0]
        for i in range(detection_count):
            cls = int(result.boxes.cls[i].item())
            name2 = result.names[cls]
            confidence2 = float(result.boxes.conf[i].item())
            bounding_box2 = result.boxes.xyxy[i].cpu().numpy()
            a1, b1, a2, b2 = [int(x) for x in bounding_box2]
            # Ensure a1 < a2 and b1 < b2
            a1, a2 = min(a1, a2), max(a1, a2)
            b1, b2 = min(b1, b2), max(b1, b2)
            if name2=="License_Plate": process_license(processed_frame, [x1+a1, y1+b1, x1+a2, y1+b2], name2, confidence2, name, confidence)

# NN Layer Other Objects Detected [Not target classes]
def process_detection(processed_frame, bounding_box, name, confidence):
    x1, y1, x2, y2 = [int(x) for x in bounding_box]
    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(255,255,0),2)
    cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1+25),0,0.8,(255,255,0),2)

# NN Layer Yolov8 Detection
def yolov8n_objects(frame):
    processed_frame = frame
    start_time = time.time()
    results = yolo_model(processed_frame, agnostic_nms=True, verbose=False)[0]
    end_time = time.time()
    processing_time = end_time - start_time
    print(f"Processing time yolov8m: {processing_time:.3f} seconds")  # Output processing time
    for result in results:
        detection_count = result.boxes.shape[0]
        for i in range(detection_count):
            cls = int(result.boxes.cls[i].item())
            name = result.names[cls]
            confidence = float(result.boxes.conf[i].item())
            bounding_box = result.boxes.xyxy[i].cpu().numpy()
            x1, y1, x2, y2 = [int(x) for x in bounding_box]
            # Ensure x1 < x2 and y1 < y2
            x1, x2 = min(x1, x2), max(x1, x2)
            y1, y2 = min(y1, y2), max(y1, y2)
            if name in target_classes: process_vehicle(processed_frame, bounding_box, name, confidence)
            else: process_detection(processed_frame, bounding_box, name, confidence)
    return processed_frame

def process_frame(frame):
    processed_frame = yolov8n_objects(frame)
    # processed_frame = cv2.flip(processed_frame, 1)
    return processed_frame

def handleFrame(frame):
    processed_frame = process_frame(frame)

    # Display in Jupyter Notebook
    img = Image.fromarray(processed_frame, 'RGB')
    display(img)
    clear_output(wait=True)
    # _, processed_frame = cv2.imencode('.jpeg', processed_frame)
    # display_handle.update(IPython.display.Image(data=processed_frame.tobytes()))
    # IPython.display.clear_output(wait=True)

def handleRelease():
    cam.release()
    print("Source released.")
    correct_plates = []
    better_plates = []
    best_plate = []
    most_repeated_plates = []
    best_repeated_plates = []
    highest_score = 0.0
    if len(detected_plates)>0:
        repetitions = {}
        count, item = 0, ''
        for plate in detected_plates:
            if plate[0]>50.0 and plate[2]>50.0 and plate[4]>50.0: correct_plates.append(plate)
            if plate[0]>70.0 and plate[2]>70.0 and plate[4]>70.0: better_plates.append(plate)
            if plate[0] + plate[2] + plate[4] > highest_score:
                highest_score = plate[0] + plate[2] + plate[4]
                best_plate = plate
            repetitions[plate[5]] = repetitions.get(plate[5], 0) + 1
            if repetitions[plate[5]]>count: count, item = repetitions[plate[5]], plate[5]
        if len(repetitions.keys())>0:
            for itm in repetitions.keys():
                if repetitions[itm]==count:
                    repeated_detected = [plate for plate in detected_plates if plate[5]==itm]
                    best_percentages = []
                    for plate in repeated_detected:
                        if len(best_percentages)==0: best_percentages=plate
                        elif (plate[0] + plate[2] + plate[4])/3.0>(best_percentages[0] + best_percentages[2] + best_percentages[4])/3.0: best_percentages=plate
                    most_repeated_plates.append(best_percentages)
        if len(most_repeated_plates)>0:
            for plate in most_repeated_plates:
                best_repeated = []
                if len(best_repeated)==0: best_repeated=plate
                elif (plate[0] + plate[2] + plate[4])/3.0>(best_repeated[0] + best_repeated[2] + best_repeated[4])/3.0: best_repeated=plate
            best_repeated_plates.append(best_repeated)
    print("\nDetected Plates: \n", np.matrix(detected_plates))
    print("\nCorrectly Detected Plates: \n", np.matrix(correct_plates))
    print("\nBetter Detected Plates: \n", np.matrix(better_plates))
    print("\nBest Detected Plate: \n", np.matrix(best_plate))
    print("\nMost Repeated Plates: \n", np.matrix(most_repeated_plates))
    print("\nBest Repeated Plate: \n", np.matrix(best_repeated_plates))

inputType="video"
# cam = cv2.VideoCapture(0)
source = "data/video/sample.MP4" if inputType=="video" else 0
cam = cv2.VideoCapture(source)
display_handle=display(None, display_id=True)
every = 60

start = 0 if source!=0 else -1
end = int(cam.get(cv2.CAP_PROP_FRAME_COUNT)) if source!=0 else -1
if source!=0: cam.set(1, start)

frameCount=start
while_safety=0
saved_count=0

if not cam.isOpened(): print("Error: Could not open source.")
else:
    try:
        while True if end<0 else frameCount<end:
            _, frame = cam.read()
            if frame is None:
                if source!=0:
                    if while_safety > 2000: break
                    while_safety += 1
                    continue
                else:
                    print("Error: Could not capture frame.")
                    cam.release()
                    break

            if every>0:
                if (frameCount+1)%math.floor(every) == 0:
                    while_safety = 0
                    handleFrame(frame)
            else:
                while_safety=0
                handleFrame(frame)
            frameCount += 1
        handleRelease()
    except KeyboardInterrupt:
        handleRelease()
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Source could not be released.")
        except:
            pass
        traceback.print_exc()


import cv2
import IPython
import numpy as np
from typing import Tuple, Union
import math
from PIL import Image
import pytesseract
import traceback
import re
import torchvision.transforms as transforms
import time
from IPython.display import display, clear_output

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# import easyocr
# reader = easyocr.Reader(['de'])

from ultralytics import YOLO
yolo_model = YOLO('yolov8x.pt')
plate_model = YOLO('vehicle_license_best.pt')

detected_plates = []

pytesseract.pytesseract.tesseract_cmd = 'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'

target_classes = ["truck", "bus", "car"]

characters = "0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ" # '0123456789abcdefghijklmnopqrstuvwxyz' # "0123456789ÄÜÖABCDEFGHIJKLMNOPQRSTUVWXYZ"

# get grayscale image
def get_grayscale(image):
    return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

#thresholding
def thresholding(image):
    return cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

# NN Layer License Plate Recognition "OCR"
def process_license(processed_frame, bounding_box, name, confidence, parentName, parentConfidence):
    x1, y1, x2, y2 = [int(x) for x in bounding_box]
    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(255,0,255),2)
    cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1-15),0,0.9,(255,0,255),2)

    # Rotation
    license_frame = processed_frame[y1:y2,x1:x2]

    grayscaled = get_grayscale(license_frame)
    threshholded = thresholding(grayscaled)

    max_conf = 0.0
    best_text = ""

    results = pytesseract.image_to_data(threshholded, config='--psm 6 --oem 3 -c tessedit_char_whitelist=ÄÜÖABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')
    # NN Layer License Plate Recognition "Dummy OCR"

    parsedLines = results.split('\n')
    best_depth = len(parsedLines)-2
    for line in parsedLines:
        params = line.split()
        if len(params)==12 and float(params[10].replace('conf','0.0'))>max_conf:
            max_conf = float(params[10])
            best_text = params[11]
            if max_conf>0.0 and len(best_text)>0:
                detected_plates.append([parentConfidence*100, parentName.upper(), confidence*100, name.upper(), max_conf, best_text, "pytesseract"])
    cv2.putText(processed_frame, '{} {:.2f}%'.format("[tesse]: "+best_text, max_conf),(x1+10,y2+25),0,0.9,(0,0,255),3)

# NN Layer Vehicle Found -> License Detection
def process_vehicle(processed_frame, bounding_box, name, confidence):

    # Yolov8n Bounding Boxes
    x1, y1, x2, y2 = [int(x) for x in bounding_box]
    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(0,255,255),2)

    cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1+25),0,0.8,(0,255,255),2)
    vehicle_frame = processed_frame[y1:y2,x1:x2]
    start_time = time.time()
    results = plate_model(vehicle_frame, agnostic_nms=True, verbose=False)[0]
    end_time = time.time()
    processing_time = end_time - start_time
    print(f"Processing time yolov8_license: {processing_time:.3f} seconds")  # Output processing time
    for result in results:
        detection_count = result.boxes.shape[0]
        for i in range(detection_count):
            cls = int(result.boxes.cls[i].item())
            name2 = result.names[cls]
            confidence2 = float(result.boxes.conf[i].item())
            bounding_box2 = result.boxes.xyxy[i].cpu().numpy()
            a1, b1, a2, b2 = [int(x) for x in bounding_box2]
            # Ensure a1 < a2 and b1 < b2
            a1, a2 = min(a1, a2), max(a1, a2)
            b1, b2 = min(b1, b2), max(b1, b2)
            # if name2=="License_Plate":
            process_license(processed_frame, [x1+a1, y1+b1, x1+a2, y1+b2], name2, confidence2, name, confidence)

# NN Layer Other Objects Detected [Not target classes]
def process_detection(processed_frame, bounding_box, name, confidence):
    x1, y1, x2, y2 = [int(x) for x in bounding_box]
    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(255,255,0),2)
    cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1+25),0,0.8,(255,255,0),2)

# NN Layer Yolov8 Detection
def yolov8n_objects(frame):
    processed_frame = frame
    start_time = time.time()
    results = yolo_model(processed_frame, agnostic_nms=True, verbose=False)[0]
    end_time = time.time()
    processing_time = end_time - start_time
    print(f"Processing time yolov8n: {processing_time:.3f} seconds")  # Output processing time
    for result in results:
        detection_count = result.boxes.shape[0]
        for i in range(detection_count):
            cls = int(result.boxes.cls[i].item())
            name = result.names[cls]
            confidence = float(result.boxes.conf[i].item())
            bounding_box = result.boxes.xyxy[i].cpu().numpy()
            x1, y1, x2, y2 = [int(x) for x in bounding_box]
            # Ensure x1 < x2 and y1 < y2
            x1, x2 = min(x1, x2), max(x1, x2)
            y1, y2 = min(y1, y2), max(y1, y2)
            if name in target_classes: process_vehicle(processed_frame, bounding_box, name, confidence)
            else: process_detection(processed_frame, bounding_box, name, confidence)
    return processed_frame

def process_frame(frame):
    processed_frame = yolov8n_objects(frame)
    # processed_frame = cv2.flip(processed_frame, 1)
    return processed_frame

def handleFrame(frame):
    processed_frame = process_frame(frame)

    # Display in Jupyter Notebook
    img = Image.fromarray(processed_frame, 'RGB')
    display(img)
    clear_output(wait=True)
    # _, processed_frame = cv2.imencode('.jpeg', processed_frame)
    # display_handle.update(IPython.display.Image(data=processed_frame.tobytes()))
    # IPython.display.clear_output(wait=True)

def handleRelease():
    cam.release()
    print("Source released.")
    correct_plates = []
    better_plates = []
    best_plate = []
    most_repeated_plates = []
    best_repeated_plates = []
    highest_score = 0.0
    if len(detected_plates)>0:
        repetitions = {}
        count, item = 0, ''
        for plate in detected_plates:
            if plate[0]>50.0 and plate[2]>50.0 and plate[4]>50.0: correct_plates.append(plate)
            if plate[0]>70.0 and plate[2]>70.0 and plate[4]>70.0: better_plates.append(plate)
            if plate[0] + plate[2] + plate[4] > highest_score:
                highest_score = plate[0] + plate[2] + plate[4]
                best_plate = plate
            repetitions[plate[5]] = repetitions.get(plate[5], 0) + 1
            if repetitions[plate[5]]>count: count, item = repetitions[plate[5]], plate[5]
        if len(repetitions.keys())>0:
            for itm in repetitions.keys():
                if repetitions[itm]==count:
                    repeated_detected = [plate for plate in detected_plates if plate[5]==itm]
                    best_percentages = []
                    for plate in repeated_detected:
                        if len(best_percentages)==0: best_percentages=plate
                        elif (plate[0] + plate[2] + plate[4])/3.0>(best_percentages[0] + best_percentages[2] + best_percentages[4])/3.0: best_percentages=plate
                    most_repeated_plates.append(best_percentages)
        if len(most_repeated_plates)>0:
            for plate in most_repeated_plates:
                best_repeated = []
                if len(best_repeated)==0: best_repeated=plate
                elif (plate[0] + plate[2] + plate[4])/3.0>(best_repeated[0] + best_repeated[2] + best_repeated[4])/3.0: best_repeated=plate
            best_repeated_plates.append(best_repeated)
    print("\nDetected Plates: \n", np.matrix(detected_plates))
    print("\nCorrectly Detected Plates: \n", np.matrix(correct_plates))
    print("\nBetter Detected Plates: \n", np.matrix(better_plates))
    print("\nBest Detected Plate: \n", np.matrix(best_plate))
    print("\nMost Repeated Plates: \n", np.matrix(most_repeated_plates))
    print("\nBest Repeated Plate: \n", np.matrix(best_repeated_plates))

inputType="video"
# cam = cv2.VideoCapture(0)
source = "data/video/sample.MP4" if inputType=="video" else 0
cam = cv2.VideoCapture(source)
display_handle=display(None, display_id=True)
every = 60

start = 0 if source!=0 else -1
end = int(cam.get(cv2.CAP_PROP_FRAME_COUNT)) if source!=0 else -1
if source!=0: cam.set(1, start)

frameCount=start
while_safety=0
saved_count=0

if not cam.isOpened(): print("Error: Could not open source.")
else:
    try:
        while True if end<0 else frameCount<end:
            _, frame = cam.read()
            if frame is None:
                if source!=0:
                    if while_safety > 2000: break
                    while_safety += 1
                    continue
                else:
                    print("Error: Could not capture frame.")
                    cam.release()
                    break

            if every>0:
                if (frameCount+1)%math.floor(every) == 0:
                    while_safety = 0
                    handleFrame(frame)
            else:
                while_safety=0
                handleFrame(frame)
            frameCount += 1
        handleRelease()
    except KeyboardInterrupt:
        handleRelease()
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Source could not be released.")
        except:
            pass
        traceback.print_exc()


import cv2
import numpy as np
from IPython.display import display, clear_output
from PIL import Image
import time  # Import time module to measure processing time
import traceback
import math

# Load pre-trained model and setup
net = cv2.dnn.readNetFromCaffe('MobileNetSSD_deploy.prototxt', 'MobileNetSSD_deploy.caffemodel')
video = cv2.VideoCapture("data/video/sample.MP4")  # Use 0 for web camera

# Define the list of class labels MobileNet SSD was trained to detect
CLASSES = ["background", "aeroplane", "bicycle", "bird", "boat",
           "bottle", "bus", "car", "cat", "chair", "cow", "diningtable",
           "dog", "horse", "motorbike", "person", "pottedplant", "sheep",
           "sofa", "train", "tvmonitor"]

def process(frame):
    start_time = time.time()  # Start time measurement

    # Convert the frame to a blob and pass the blob through the network
    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 0.007843, (300, 300), 127.5)
    net.setInput(blob)
    detections = net.forward()

    end_time = time.time()  # End time measurement
    processing_time = end_time - start_time  # Calculate processing time
    print(f"Processing time: {processing_time:.3f} seconds")  # Print processing time

    # Loop over the detections
    for i in np.arange(0, detections.shape[2]):
        confidence = detections[0, 0, i, 2]

        # Filter out weak detections
        if confidence > 0.2:
            idx = int(detections[0, 0, i, 1])
            label = "{}: {:.2f}%".format(CLASSES[idx], confidence * 100)
            print(label)  # Print detected objects and confidence

            box = detections[0, 0, i, 3:7] * np.array([frame.shape[1], frame.shape[0], frame.shape[1], frame.shape[0]])
            (startX, startY, endX, endY) = box.astype("int")

            # Draw the prediction on the frame
            cv2.rectangle(frame, (startX, startY), (endX, endY), (23, 230, 210), 2)
            y = startY - 15 if startY - 15 > 15 else startY + 15
            cv2.putText(frame, label, (startX, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (23, 230, 210), 2)

    return frame

def handleFrame(frame):
    processed_frame = process(frame)

    # Display in Jupyter Notebook
    img = Image.fromarray(processed_frame, 'RGB')
    display(img)
    clear_output(wait=True)
    # _, processed_frame = cv2.imencode('.jpeg', processed_frame)
    # display_handle.update(IPython.display.Image(data=processed_frame.tobytes()))
    # IPython.display.clear_output(wait=True)

def handleRelease():
    cam.release()
    print("Source released.")

inputType="video"
# cam = cv2.VideoCapture(0)
source = "data/video/sample.MP4" if inputType=="video" else 0
cam = cv2.VideoCapture(source)
display_handle=display(None, display_id=True)
every = 60

start = 0 if source!=0 else -1
end = int(cam.get(cv2.CAP_PROP_FRAME_COUNT)) if source!=0 else -1
if source!=0: cam.set(1, start)

frameCount=start
while_safety=0
saved_count=0

if not cam.isOpened(): print("Error: Could not open source.")
else:
    try:
        while True if end<0 else frameCount<end:
            _, frame = cam.read()
            if frame is None:
                if source!=0:
                    if while_safety > 2000: break
                    while_safety += 1
                    continue
                else:
                    print("Error: Could not capture frame.")
                    cam.release()
                    break

            if every>0:
                if (frameCount+1)%math.floor(every) == 0:
                    while_safety = 0
                    handleFrame(frame)
            else:
                while_safety=0
                handleFrame(frame)
            frameCount += 1
        handleRelease()
    except KeyboardInterrupt:
        handleRelease()
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Source could not be released.")
        except:
            pass
        traceback.print_exc()


from IPython.display import display, clear_output
import numpy as np
from PIL import Image

# Initialize video capture
video = cv2.VideoCapture(0)

try:
    while True:
        # Capture frame-by-frame
        ret, frame = video.read()
        if not ret:
            break

        # Prepare the frame for detection
        (h, w) = frame.shape[:2]
        blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 0.007843, (300, 300), 127.5)

        # Pass the blob through the network and obtain the detections
        net.setInput(blob)
        detections = net.forward()

        # Loop over the detections
        for i in np.arange(0, detections.shape[2]):
            confidence = detections[0, 0, i, 2]

            # Filter out weak detections
            if confidence > 0.2:
                idx = int(detections[0, 0, i, 1])
                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])
                (startX, startY, endX, endY) = box.astype("int")

                label = "{}: {:.2f}%".format(CLASSES[idx], confidence * 100)
                cv2.rectangle(frame, (startX, startY), (endX, endY), (23, 230, 210), 2)
                y = startY - 15 if startY - 15 > 15 else startY + 15
                cv2.putText(frame, label, (startX, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

        # Convert the frame for display
        display_image = Image.fromarray(frame)
        display(display_image)
        clear_output(wait=True)

finally:
    video.release()


import cv2
import torch
import torchvision.transforms as transforms
from torchvision.models.detection import ssd300_vgg16
from PIL import Image
from IPython.display import display, clear_output
import time
import traceback
import math

COCO_INSTANCE_CATEGORY_NAMES = [
    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',
    'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',
    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',
    'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'N/A', 'N/A',
    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',
    'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',
    'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife',
    'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',
    'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',
    'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',
    'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote',
    'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',
    'refrigerator', 'N/A', 'book', 'clock', 'vase', 'scissors',
    'teddy bear', 'hair drier', 'toothbrush'
]

# Load the pre-trained SSD model
model = ssd300_vgg16(pretrained=True)
model.eval()  # Set the model to evaluation mode
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Initialize video capture
video = cv2.VideoCapture("data/video/sample.MP4")

# Transformation for the input image
transform = transforms.Compose([
    #transforms.Resize((300, 300)),
    transforms.ToTensor(),
])

def process(frame):
    start_time = time.time()

    # Convert frame to RGB and PIL Image, then apply transformation
    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    pil_image = Image.fromarray(frame_rgb)
    image = transform(pil_image).unsqueeze(0).to(device)

    # Object detection
    with torch.no_grad():
        predictions = model(image)

    # Processing time
    processing_time = time.time() - start_time
    print(f"Processing time: {processing_time:.3f} seconds")

    # Visualization
    scores = predictions[0]['scores']
    boxes = predictions[0]['boxes']
    labels = predictions[0]['labels']
    confidence_threshold = 0.5

    for score, box, label in zip(scores, boxes, labels):
        if score > confidence_threshold:
            x1, y1, x2, y2 = map(int, box)
            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
            label_text = f'{COCO_INSTANCE_CATEGORY_NAMES[label.item()]} ({score:.2f})'
            cv2.putText(frame, label_text, (x1, y1+20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

    return frame

def handleFrame(frame):
    processed_frame = process(frame)

    # Display in Jupyter Notebook
    img = Image.fromarray(processed_frame, 'RGB')
    display(img)
    clear_output(wait=True)
    # _, processed_frame = cv2.imencode('.jpeg', processed_frame)
    # display_handle.update(IPython.display.Image(data=processed_frame.tobytes()))
    # IPython.display.clear_output(wait=True)

def handleRelease():
    cam.release()
    print("Source released.")

inputType="video"
# cam = cv2.VideoCapture(0)
source = "data/video/sample.MP4" if inputType=="video" else 0
cam = cv2.VideoCapture(source)
display_handle=display(None, display_id=True)
every = 60

start = 0 if source!=0 else -1
end = int(cam.get(cv2.CAP_PROP_FRAME_COUNT)) if source!=0 else -1
if source!=0: cam.set(1, start)

frameCount=start
while_safety=0
saved_count=0

if not cam.isOpened(): print("Error: Could not open source.")
else:
    try:
        while True if end<0 else frameCount<end:
            _, frame = cam.read()
            if frame is None:
                if source!=0:
                    if while_safety > 2000: break
                    while_safety += 1
                    continue
                else:
                    print("Error: Could not capture frame.")
                    cam.release()
                    break

            if every>0:
                if (frameCount+1)%math.floor(every) == 0:
                    while_safety = 0
                    handleFrame(frame)
            else:
                while_safety=0
                handleFrame(frame)
            frameCount += 1
        handleRelease()
    except KeyboardInterrupt:
        handleRelease()
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Source could not be released.")
        except:
            pass
        traceback.print_exc()





import cv2
import torch
from torchvision.models.detection import *
from torchvision.io.image import read_image
from torchvision.utils import draw_bounding_boxes
from torchvision.transforms.functional import to_pil_image
import torchvision.transforms as transforms
from PIL import Image
import IPython
from IPython.display import display, clear_output
import time
import traceback
import math
from ultralytics import YOLO

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

transform = transforms.Compose([
    #transforms.Resize((300, 300)),
    transforms.ToTensor(),
])

class DetectionModel():
    def __init__(self, concept, backbone, model, weights, reference, color=None):
        self.concept = concept
        self.backbone = backbone
        self.model = model(weights) if concept=="YOLO" else model(weights=weights.DEFAULT)
        self.weights = weights
        self.reference = reference
        self.color = color
        self.detections = []

    def addDetection(self, detection):
        self.detections.append(detection)

    def getDetections(self):
        return self.detections

    def clearDetections(self):
        self.detections = []

    def detectionsCount(self):
        return len(self.detections)

    def inferencesCount(self):
        length = 0
        addedFrames = []
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    addedFrames.append(self.detections[i].frame)
        return len(addedFrames)

    def getAveragePrecision(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].precision
            avg = sum/len(self.detections)
        return  avg.item() if torch.is_tensor(avg) else avg

    def getMaxPrecision(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].precision:
                max=self.detections[i].precision
        return max.item() if torch.is_tensor(max) else max

    def getMinPrecision(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].precision:
                min=self.detections[i].precision
        return 0.0 if min==9999999.99 else min.item() if torch.is_tensor(min) else min

    def getAverageSpeed(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].speed
            avg = sum/len(self.detections)
        return avg

    def getMaxSpeed(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].speed:
                max=self.detections[i].speed
        return max

    def getMinSpeed(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].speed:
                min=self.detections[i].speed
        return 0.0 if min==9999999.99 else min

    def getAverageInferenceSpeed(self):
        sum=0.0
        avg=0.0
        addedFrames=[]
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    sum+=self.detections[i].speed
                    addedFrames.append(self.detections[i].frame)
            avg = sum/len(addedFrames) if len(addedFrames)>0 else 0.0
        return avg

    def getMaxInferenceSpeed(self):
        max=0.0
        addedFrames=[]
        for i in range(len(self.detections)):
            if max<self.detections[i].speed and self.detections[i].frame not in addedFrames:
                max=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return max

    def getMinInferenceSpeed(self):
        min=9999999.99
        addedFrames=[]
        for i in range(len(self.detections)):
            if min>self.detections[i].speed and self.detections[i].frame not in addedFrames:
                min=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return 0.0 if min==9999999.99 else min

    def filterPrecisionsBelow(self, minPrecision):
        newDetections = []
        for i in range(len(self.detections)):
            if self.detections[i].precision>=minPrecision:
                newDetections.append(self.detections[i])
        self.detections = newDetections

    def printResults(self):
        print("Model:", self.reference,
              "\nBest Precision:", self.getMaxPrecision(),
              "%\nAverage Precision:", self.getAveragePrecision(),
              "%\nWorst Precision:", self.getMinPrecision(),
              "%\nBest Speed:", self.getMinSpeed(),
              "seconds\nAverage Speed:", self.getAverageSpeed(),
              "seconds\nWorst Speed:", self.getMaxSpeed(),
              "seconds\nBest Inference Speed:", self.getMinInferenceSpeed(),
              "seconds\nAverage Inference Speed:", self.getAverageInferenceSpeed(),
              "seconds\nWorst Inference Speed:", self.getMaxInferenceSpeed(),
              "seconds\nTotal Detections:", self.detectionsCount(),
              "\nTotal Inferences:", self.inferencesCount(),
              "\n\n")

class DetectionResult():
    def __init__(self, model, label, precision, speed, device=None, source=None, frame=None, boundingBox=None, details=None):
        self.model = model
        self.label = label
        self.precision = precision.item() if torch.is_tensor(precision) else precision
        self.speed = speed.item() if torch.is_tensor(speed) else speed
        self.device = device
        self.source = source
        self.frame = frame
        self.boundingBox = boundingBox
        self.details = details

target_classes = ["truck", "bus", "car", "train", "bicycle"]


VEHICLE_DETECTION_MODELS = [
    DetectionModel("FCOS", "resnet50_fpn", fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights, "fcos_resnet", (255,0,0)),
    DetectionModel("RetinaNet", "resnet50_fpn", retinanet_resnet50_fpn, RetinaNet_ResNet50_FPN_Weights, "retinanet_resnet", (255,255,0)),
    DetectionModel("RetinaNet", "resnet50_fpn_v2", retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights, "retinanet_resnet_v2", (0,234,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn", fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights, "faster_rcnn_resnet", (170,0,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn_v2", fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights, "faster_rcnn_resnet_v2", (255,127,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_fpn", fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights, "faster_rcnn_mobilenet_v3", (191,255,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_320_fpn", fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights, "faster_rcnn_mobilenet_v3_320", (0,149,255)),
    DetectionModel("MaskRCNN", "resnet50_fpn", maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights, "mask_rcnn_resnet", (255,0,170)),
    DetectionModel("MaskRCNN", "resnet50_fpn_v2", maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights, "mask_rcnn_resnet_v2", (255,212,0)),
    DetectionModel("SSD300", "vgg16", ssd300_vgg16, SSD300_VGG16_Weights, "ssd_vgg16", (106,255,0)),
    DetectionModel("SSDLite320", "mobilenet_v3_large", ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights, "ssd_mobilenet_v3", (0,64,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5nu.pt', "yolov5nu", (237,185,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5su.pt', "yolov5su", (185,215,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5mu.pt', "yolov5mu", (231,233,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5lu.pt', "yolov5lu", (220,185,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5xu.pt', "yolov5xu", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5n6u.pt', "yolov5n6u", (143,35,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5s6u.pt', "yolov5s6u", (35,98,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5m6u.pt', "yolov5m6u", (143,106,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5l6u.pt', "yolov5l6u", (107,35,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5x6u.pt', "yolov5x6u", (79,143,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8n.pt', "yolov8n", (0,0,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8s.pt', "yolov8s", (115,115,115)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8m.pt', "yolov8m", (204,204,204)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8l.pt', "yolov8l", (255,0,0)), # cycle restarts
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8x.pt', "yolov8x", (255,255,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9c.pt', "yolov9c", (0,234,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9e.pt', "yolov9e", (170,0,255))
]

def process(frame, frameIndex=-1, source=None):
    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        processed_frame = frame.copy()

        if currentModel.concept=="YOLO":
            start_time = time.time()
            results = currentModel.model(processed_frame, agnostic_nms=True, verbose=False)[0]
            processing_time = time.time() - start_time
            for result in results:
                detection_count = result.boxes.shape[0]
                for j in range(detection_count):
                    cls = int(result.boxes.cls[j].item())
                    name = result.names[cls]
                    confidence = float(result.boxes.conf[j].item())
                    bounding_box = result.boxes.xyxy[j].cpu().numpy()
                    if name in target_classes:
                        x1, y1, x2, y2 = [int(x) for x in bounding_box]
                        # Ensure x1 < x2 and y1 < y2
                        x1, x2 = min(x1, x2), max(x1, x2)
                        y1, y2 = min(y1, y2), max(y1, y2)
                        label = '{} ({:.2f}%) ({:.3f}s) {}'.format(name.upper(), confidence*100, processing_time, currentModel.reference)
                        print(label)
                        VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, name, confidence*100, processing_time, device, source, frameIndex, bounding_box, label))
                        cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                        cv2.putText(processed_frame, label, (x1+10, y1+25), 0, 0.8, currentModel.color, 2)
        else:
            weights = currentModel.weights.DEFAULT
            model = currentModel.model
            model.to(device)
            model.eval()

            # Convert frame to RGB and PIL Image, then apply transformation
            frame_rgb = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            image = transform(pil_image).unsqueeze(0).to(device)

            # Object detection
            start_time = time.time()
            with torch.no_grad():
                predictions = model(image)

            # Processing time
            processing_time = time.time() - start_time

            # Visualization
            scores = predictions[0]['scores']
            boxes = predictions[0]['boxes']
            labels = predictions[0]['labels']
            confidence_threshold = 0.5

            for score, box, label in zip(scores, boxes, labels):
                # and score > confidence_threshold
                if weights.meta["categories"][label.item()] in target_classes:
                    x1, y1, x2, y2 = map(int, box)
                    label_text = f'{weights.meta["categories"][label.item()].upper()} ({score*100:.2f}%) ({processing_time:.3f}s) {currentModel.reference}'
                    print(label_text)
                    VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, weights.meta["categories"][label.item()], float(score.item())*100, processing_time, device, source, frameIndex, box, label_text))
                    cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                    cv2.putText(processed_frame, label_text, (x1, y1+20), 0, 0.8, currentModel.color, 2)

        img = Image.fromarray(processed_frame, 'RGB')
        display(img)
        clear_output(wait=True)
    return frame

def handleFrame(frame, frameIndex=-1, source=None):
    processed_frame = process(frame,frameIndex,source)

def handleRelease():
    cam.release()
    print("Source released.")

    print(device)
    print(source)

    BestPrecision=None
    BestAvgPrecision=None
    WorstPrecision=None
    BestSpeed=None
    BestAvgSpeed=None
    WorstSpeed=None

    Models=[]
    BestPrecisions=[]
    AvgPrecisions=[]
    WorstPrecisions=[]
    BestSpeeds=[]
    AvgSpeeds=[]
    WorstSpeeds=[]
    BestInferenceSpeeds=[]
    AvgInferenceSpeeds=[]
    WorstInferenceSpeeds=[]
    TotalDetectionsArr=[]
    TotalInferencesArr=[]

    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        currentModel.printResults()

        Models.append(currentModel.reference)
        BestPrecisions.append(currentModel.getMaxPrecision())
        AvgPrecisions.append(currentModel.getAveragePrecision())
        WorstPrecisions.append(currentModel.getMinPrecision())
        BestSpeeds.append(currentModel.getMinSpeed())
        AvgSpeeds.append(currentModel.getAverageSpeed())
        WorstSpeeds.append(currentModel.getMaxSpeed())
        BestInferenceSpeeds.append(currentModel.getMinInferenceSpeed())
        AvgInferenceSpeeds.append(currentModel.getAverageInferenceSpeed())
        WorstInferenceSpeeds.append(currentModel.getMaxInferenceSpeed())
        TotalDetectionsArr.append(currentModel.getMaxPrecision())
        TotalInferencesArr.append(currentModel.getMaxPrecision())

        if BestPrecision is None or BestPrecision.getMaxPrecision()<=currentModel.getMaxPrecision():
            BestPrecision = currentModel
        if BestAvgPrecision is None or BestAvgPrecision.getAveragePrecision()<=currentModel.getAveragePrecision():
            BestAvgPrecision = currentModel
        if WorstPrecision is None or WorstPrecision.getMinPrecision()>=currentModel.getMinPrecision():
            WorstPrecision = currentModel
        if BestSpeed is None or BestSpeed.getMinSpeed()>=currentModel.getMinSpeed():
            BestSpeed = currentModel
        if BestAvgSpeed is None or BestAvgSpeed.getAverageSpeed()>=currentModel.getAverageSpeed():
            BestAvgSpeed = currentModel
        if WorstSpeed is None or WorstSpeed.getMaxSpeed()<=currentModel.getMaxSpeed():
            WorstSpeed = currentModel

    if BestPrecision is not None:
        print("Best Precision:")
        BestPrecision.printResults()
    if BestAvgPrecision is not None:
        print("Best Average Precision:")
        BestAvgPrecision.printResults()
    if WorstPrecision is not None:
        print("Worst Precision:")
        WorstPrecision.printResults()
    if BestSpeed is not None:
        print("Best Speed:")
        BestSpeed.printResults()
    if BestAvgSpeed is not None:
        print("Best Average Speed:")
        BestAvgSpeed.printResults()
    if WorstSpeed is not None:
        print("Worst Speed:")
        WorstSpeed.printResults()

    print("models=",models)
    print("best_precisions=",BestPrecisions)
    print("avg_precisions=",AvgPrecisions)
    print("worst_precisions=",WorstPrecisions)
    print("best_speeds=",BestSpeeds)
    print("avg_speeds=",AvgSpeeds)
    print("worst_speeds=",WorstSpeeds)
    print("best_inference_speeds=",BestInferenceSpeeds)
    print("avg_inference_speeds=",AvgInferenceSpeeds)
    print("worst_inference_speeds=",WorstInferenceSpeeds)
    print("total_detections=",TotalDetectionsArr)
    print("total_inferences=",TotalInferencesArr)

inputType="video"
source = "data/video/sample.MP4" if inputType=="video" else 0
cam = cv2.VideoCapture(source)
display_handle=display(None, display_id=True)
every = 60

start = 0 if source!=0 else -1
end = int(cam.get(cv2.CAP_PROP_FRAME_COUNT)) if source!=0 else -1
if source!=0: cam.set(1, start)

frameCount=start
while_safety=0
saved_count=0

if not cam.isOpened(): print("Error: Could not open source.")
else:
    try:
        while True if end<0 else frameCount<end:
            _, frame = cam.read()
            if frame is None:
                if source!=0:
                    if while_safety > 2000: break
                    while_safety += 1
                    continue
                else:
                    print("Error: Could not capture frame.")
                    cam.release()
                    break

            if every>0:
                if (frameCount+1)%math.floor(every) == 0:
                    while_safety = 0
                    handleFrame(frame, frameCount, source)
            else:
                while_safety=0
                handleFrame(frame)
            frameCount += 1
        handleRelease()
    except KeyboardInterrupt:
        handleRelease()
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Source could not be released.")
        except:
            pass
        traceback.print_exc()


Source released.
cpu
data/video/sample.MP4
Model: fcos_resnet
Best Precision: 84.01497006416321 %
Average Precision: 33.320973298471905 %
Worst Precision: 20.00025510787964 %
Best Speed: 1.1732428073883057 seconds
Average Speed: 1.393990372315697 seconds
Worst Speed: 1.8190703392028809 seconds
Best Inference Speed: 1.1732428073883057 seconds
Average Inference Speed: 1.4067243933677673 seconds
Worst Inference Speed: 1.8190703392028809 seconds
Total Detections: 920
Total Inferences: 84


Model: retinanet_resnet
Best Precision: 86.20967268943787 %
Average Precision: 13.884030922648927 %
Worst Precision: 5.000218749046326 %
Best Speed: 1.3777101039886475 seconds
Average Speed: 1.539636810522122 seconds
Worst Speed: 2.024221897125244 seconds
Best Inference Speed: 1.3777101039886475 seconds
Average Inference Speed: 1.5591738365945362 seconds
Worst Inference Speed: 2.024221897125244 seconds
Total Detections: 2017
Total Inferences: 84


Model: retinanet_resnet_v2
Best Precision: 96.10866904258728 %
Average Precision: 15.765904373912859 %
Worst Precision: 5.0005074590444565 %
Best Speed: 1.316899061203003 seconds
Average Speed: 1.5364464754804212 seconds
Worst Speed: 1.824617624282837 seconds
Best Inference Speed: 1.316899061203003 seconds
Average Inference Speed: 1.5387198300588698 seconds
Worst Inference Speed: 1.824617624282837 seconds
Total Detections: 1614
Total Inferences: 84


Model: faster_rcnn_resnet
Best Precision: 99.24747347831726 %
Average Precision: 33.83567893643253 %
Worst Precision: 5.061345919966698 %
Best Speed: 1.3534657955169678 seconds
Average Speed: 1.499310665572716 seconds
Worst Speed: 1.722820520401001 seconds
Best Inference Speed: 1.3534657955169678 seconds
Average Inference Speed: 1.5141752362251282 seconds
Worst Inference Speed: 1.722820520401001 seconds
Total Detections: 755
Total Inferences: 84


Model: faster_rcnn_resnet_v2
Best Precision: 99.60418939590454 %
Average Precision: 33.03067530483079 %
Worst Precision: 5.0137486308813095 %
Best Speed: 2.0542116165161133 seconds
Average Speed: 2.183287256357202 seconds
Worst Speed: 2.480558395385742 seconds
Best Inference Speed: 2.0542116165161133 seconds
Average Inference Speed: 2.1902064993267967 seconds
Worst Inference Speed: 2.480558395385742 seconds
Total Detections: 836
Total Inferences: 84


Model: faster_rcnn_mobilenet_v3
Best Precision: 99.35287833213806 %
Average Precision: 37.7154503027216 %
Worst Precision: 5.046924203634262 %
Best Speed: 0.2831146717071533 seconds
Average Speed: 0.3571851962651962 seconds
Worst Speed: 0.5471971035003662 seconds
Best Inference Speed: 0.2831146717071533 seconds
Average Inference Speed: 0.3590155187107268 seconds
Worst Inference Speed: 0.5471971035003662 seconds
Total Detections: 390
Total Inferences: 84


Model: faster_rcnn_mobilenet_v3_320
Best Precision: 98.90456199645996 %
Average Precision: 38.089748695492744 %
Worst Precision: 5.015124008059502 %
Best Speed: 0.12997961044311523 seconds
Average Speed: 0.1928540668487549 seconds
Worst Speed: 0.3026401996612549 seconds
Best Inference Speed: 0.12997961044311523 seconds
Average Inference Speed: 0.18705087048666819 seconds
Worst Inference Speed: 0.3026401996612549 seconds
Total Detections: 375
Total Inferences: 84


Model: mask_rcnn_resnet
Best Precision: 98.711097240448 %
Average Precision: 32.748217028387316 %
Worst Precision: 5.009864270687103 %
Best Speed: 1.4615504741668701 seconds
Average Speed: 1.6194732850577431 seconds
Worst Speed: 1.898716926574707 seconds
Best Inference Speed: 1.4615504741668701 seconds
Average Inference Speed: 1.6260126403399877 seconds
Worst Inference Speed: 1.898716926574707 seconds
Total Detections: 757
Total Inferences: 84


Model: mask_rcnn_resnet_v2
Best Precision: 99.41349625587463 %
Average Precision: 37.25264283556382 %
Worst Precision: 5.005624517798424 %
Best Speed: 2.1518988609313965 seconds
Average Speed: 2.314586741109904 seconds
Worst Speed: 2.637709617614746 seconds
Best Inference Speed: 2.1518988609313965 seconds
Average Inference Speed: 2.317195841244289 seconds
Worst Inference Speed: 2.637709617614746 seconds
Total Detections: 647
Total Inferences: 84


Model: ssd_vgg16
Best Precision: 95.25397419929504 %
Average Precision: 6.433906628387384 %
Worst Precision: 2.197788655757904 %
Best Speed: 0.21824193000793457 seconds
Average Speed: 0.2923531746055868 seconds
Worst Speed: 0.47712063789367676 seconds
Best Inference Speed: 0.21824193000793457 seconds
Average Inference Speed: 0.2940050959587097 seconds
Worst Inference Speed: 0.47712063789367676 seconds
Total Detections: 4807
Total Inferences: 84


Model: ssd_mobilenet_v3
Best Precision: 97.86267876625061 %
Average Precision: 3.645043957223961 %
Worst Precision: 0.7162508554756641 %
Best Speed: 0.10079312324523926 seconds
Average Speed: 0.11409947089094424 seconds
Worst Speed: 0.15995168685913086 seconds
Best Inference Speed: 0.10079312324523926 seconds
Average Inference Speed: 0.11439896197546096 seconds
Worst Inference Speed: 0.15995168685913086 seconds
Total Detections: 7633
Total Inferences: 84


Model: yolov5nu
Best Precision: 89.55094814300537 %
Average Precision: 59.80522941558733 %
Worst Precision: 25.00203847885132 %
Best Speed: 0.07529926300048828 seconds
Average Speed: 0.13100226865995915 seconds
Worst Speed: 3.792293071746826 seconds
Best Inference Speed: 0.07529926300048828 seconds
Average Inference Speed: 0.14237770689539161 seconds
Worst Inference Speed: 3.792293071746826 seconds
Total Detections: 109
Total Inferences: 83


Model: yolov5su
Best Precision: 94.16013956069946 %
Average Precision: 65.16339556106085 %
Worst Precision: 28.957846760749817 %
Best Speed: 0.10192203521728516 seconds
Average Speed: 0.16693149029629903 seconds
Worst Speed: 3.118795156478882 seconds
Best Inference Speed: 0.10192203521728516 seconds
Average Inference Speed: 0.1726582930201576 seconds
Worst Inference Speed: 3.118795156478882 seconds
Total Detections: 103
Total Inferences: 84


Model: yolov5mu
Best Precision: 95.83572149276733 %
Average Precision: 62.61705411959064 %
Worst Precision: 25.00402331352234 %
Best Speed: 0.18625998497009277 seconds
Average Speed: 0.2854649392209312 seconds
Worst Speed: 3.4319112300872803 seconds
Best Inference Speed: 0.18625998497009277 seconds
Average Inference Speed: 0.299864079271044 seconds
Worst Inference Speed: 3.4319112300872803 seconds
Total Detections: 129
Total Inferences: 84


Model: yolov5lu
Best Precision: 95.45990824699402 %
Average Precision: 73.18180666438171 %
Worst Precision: 25.045570731163025 %
Best Speed: 0.29940295219421387 seconds
Average Speed: 0.4553763674838202 seconds
Worst Speed: 4.052781343460083 seconds
Best Inference Speed: 0.29940295219421387 seconds
Average Inference Speed: 0.4578117217336382 seconds
Worst Inference Speed: 4.052781343460083 seconds
Total Detections: 112
Total Inferences: 84


Model: yolov5xu
Best Precision: 96.57882452011108 %
Average Precision: 75.30654810112098 %
Worst Precision: 25.6292462348938 %
Best Speed: 0.4998500347137451 seconds
Average Speed: 0.6954899101421751 seconds
Worst Speed: 4.530595541000366 seconds
Best Inference Speed: 0.4998500347137451 seconds
Average Inference Speed: 0.7029802855991182 seconds
Worst Inference Speed: 4.530595541000366 seconds
Total Detections: 116
Total Inferences: 84


Model: yolov5n6u
Best Precision: 89.94278907775879 %
Average Precision: 63.64890622723963 %
Worst Precision: 25.59078335762024 %
Best Speed: 0.16034460067749023 seconds
Average Speed: 0.23529362432735482 seconds
Worst Speed: 3.058438539505005 seconds
Best Inference Speed: 0.16034460067749023 seconds
Average Inference Speed: 0.23942346516109647 seconds
Worst Inference Speed: 3.058438539505005 seconds
Total Detections: 97
Total Inferences: 84


Model: yolov5s6u
Best Precision: 93.81153583526611 %
Average Precision: 68.91447240787167 %
Worst Precision: 25.863394141197205 %
Best Speed: 0.2637665271759033 seconds
Average Speed: 0.3475853704637097 seconds
Worst Speed: 0.5153534412384033 seconds
Best Inference Speed: 0.2637665271759033 seconds
Average Inference Speed: 0.35105364581188525 seconds
Worst Inference Speed: 0.5153534412384033 seconds
Total Detections: 124
Total Inferences: 83


Model: yolov5m6u
Best Precision: 96.59476280212402 %
Average Precision: 74.37427713245641 %
Worst Precision: 27.020099759101868 %
Best Speed: 0.5487680435180664 seconds
Average Speed: 0.7006321598271854 seconds
Worst Speed: 0.9698598384857178 seconds
Best Inference Speed: 0.5487680435180664 seconds
Average Inference Speed: 0.70167647499636 seconds
Worst Inference Speed: 0.9698598384857178 seconds
Total Detections: 122
Total Inferences: 83


Model: yolov5l6u
Best Precision: 96.98338508605957 %
Average Precision: 76.79304788189549 %
Worst Precision: 25.58591663837433 %
Best Speed: 0.9606988430023193 seconds
Average Speed: 1.187310512988798 seconds
Worst Speed: 1.4951603412628174 seconds
Best Inference Speed: 0.9606988430023193 seconds
Average Inference Speed: 1.1886320841021654 seconds
Worst Inference Speed: 1.4951603412628174 seconds
Total Detections: 124
Total Inferences: 82


Model: yolov5x6u
Best Precision: 96.82453870773315 %
Average Precision: 77.06774789553423 %
Worst Precision: 25.649192929267883 %
Best Speed: 1.6439409255981445 seconds
Average Speed: 1.966013585604154 seconds
Worst Speed: 7.776415586471558 seconds
Best Inference Speed: 1.6439409255981445 seconds
Average Inference Speed: 1.9896536781674339 seconds
Worst Inference Speed: 7.776415586471558 seconds
Total Detections: 130
Total Inferences: 84


Model: yolov8n
Best Precision: 89.87798690795898 %
Average Precision: 64.94164858201538 %
Worst Precision: 25.362318754196167 %
Best Speed: 0.07329058647155762 seconds
Average Speed: 0.13206798620898313 seconds
Worst Speed: 3.7222094535827637 seconds
Best Inference Speed: 0.07329058647155762 seconds
Average Inference Speed: 0.1383342742919922 seconds
Worst Inference Speed: 3.7222094535827637 seconds
Total Detections: 99
Total Inferences: 84


Model: yolov8s
Best Precision: 96.51697874069214 %
Average Precision: 69.82134878635406 %
Worst Precision: 25.33295452594757 %
Best Speed: 0.1032400131225586 seconds
Average Speed: 0.17700084993394755 seconds
Worst Speed: 3.933476686477661 seconds
Best Inference Speed: 0.1032400131225586 seconds
Average Inference Speed: 0.18842212075278872 seconds
Worst Inference Speed: 3.933476686477661 seconds
Total Detections: 118
Total Inferences: 84


Model: yolov8m
Best Precision: 96.09609842300415 %
Average Precision: 75.26495310393247 %
Worst Precision: 25.73632299900055 %
Best Speed: 0.20502161979675293 seconds
Average Speed: 0.3077530839226463 seconds
Worst Speed: 4.108064889907837 seconds
Best Inference Speed: 0.20502161979675293 seconds
Average Inference Speed: 0.3193101513953436 seconds
Worst Inference Speed: 4.108064889907837 seconds
Total Detections: 110
Total Inferences: 84


Model: yolov8l
Best Precision: 96.97030186653137 %
Average Precision: 75.66016041918805 %
Worst Precision: 26.122286915779114 %
Best Speed: 0.32570862770080566 seconds
Average Speed: 0.5104293112169233 seconds
Worst Speed: 4.404922962188721 seconds
Best Inference Speed: 0.32570862770080566 seconds
Average Inference Speed: 0.5122670786721366 seconds
Worst Inference Speed: 4.404922962188721 seconds
Total Detections: 114
Total Inferences: 84


Model: yolov8x
Best Precision: 96.95289731025696 %
Average Precision: 82.63096755689328 %
Worst Precision: 26.093509793281555 %
Best Speed: 0.4530792236328125 seconds
Average Speed: 0.6732257142797247 seconds
Worst Speed: 5.091320276260376 seconds
Best Inference Speed: 0.4530792236328125 seconds
Average Inference Speed: 0.6738595877374921 seconds
Worst Inference Speed: 5.091320276260376 seconds
Total Detections: 111
Total Inferences: 84


Model: yolov9c
Best Precision: 95.78874111175537 %
Average Precision: 75.23137887494754 %
Worst Precision: 25.40087103843689 %
Best Speed: 0.2946157455444336 seconds
Average Speed: 0.42067946374943826 seconds
Worst Speed: 3.5673258304595947 seconds
Best Inference Speed: 0.2946157455444336 seconds
Average Inference Speed: 0.4262077411015828 seconds
Worst Inference Speed: 3.5673258304595947 seconds
Total Detections: 113
Total Inferences: 84


Model: yolov9e
Best Precision: 96.19094133377075 %
Average Precision: 76.31394955538964 %
Worst Precision: 25.914165377616882 %
Best Speed: 0.5655300617218018 seconds
Average Speed: 0.7747633309327355 seconds
Worst Speed: 4.026620388031006 seconds
Best Inference Speed: 0.5655300617218018 seconds
Average Inference Speed: 0.7866396875608535 seconds
Worst Inference Speed: 4.026620388031006 seconds
Total Detections: 129
Total Inferences: 84


Best Precision:
Model: faster_rcnn_resnet_v2
Best Precision: 99.60418939590454 %
Average Precision: 33.03067530483079 %
Worst Precision: 5.0137486308813095 %
Best Speed: 2.0542116165161133 seconds
Average Speed: 2.183287256357202 seconds
Worst Speed: 2.480558395385742 seconds
Best Inference Speed: 2.0542116165161133 seconds
Average Inference Speed: 2.1902064993267967 seconds
Worst Inference Speed: 2.480558395385742 seconds
Total Detections: 836
Total Inferences: 84


Best Average Precision:
Model: yolov8x
Best Precision: 96.95289731025696 %
Average Precision: 82.63096755689328 %
Worst Precision: 26.093509793281555 %
Best Speed: 0.4530792236328125 seconds
Average Speed: 0.6732257142797247 seconds
Worst Speed: 5.091320276260376 seconds
Best Inference Speed: 0.4530792236328125 seconds
Average Inference Speed: 0.6738595877374921 seconds
Worst Inference Speed: 5.091320276260376 seconds
Total Detections: 111
Total Inferences: 84


Worst Precision:
Model: ssd_mobilenet_v3
Best Precision: 97.86267876625061 %
Average Precision: 3.645043957223961 %
Worst Precision: 0.7162508554756641 %
Best Speed: 0.10079312324523926 seconds
Average Speed: 0.11409947089094424 seconds
Worst Speed: 0.15995168685913086 seconds
Best Inference Speed: 0.10079312324523926 seconds
Average Inference Speed: 0.11439896197546096 seconds
Worst Inference Speed: 0.15995168685913086 seconds
Total Detections: 7633
Total Inferences: 84


Best Speed:
Model: yolov8n
Best Precision: 89.87798690795898 %
Average Precision: 64.94164858201538 %
Worst Precision: 25.362318754196167 %
Best Speed: 0.07329058647155762 seconds
Average Speed: 0.13206798620898313 seconds
Worst Speed: 3.7222094535827637 seconds
Best Inference Speed: 0.07329058647155762 seconds
Average Inference Speed: 0.1383342742919922 seconds
Worst Inference Speed: 3.7222094535827637 seconds
Total Detections: 99
Total Inferences: 84


Best Average Speed:
Model: ssd_mobilenet_v3
Best Precision: 97.86267876625061 %
Average Precision: 3.645043957223961 %
Worst Precision: 0.7162508554756641 %
Best Speed: 0.10079312324523926 seconds
Average Speed: 0.11409947089094424 seconds
Worst Speed: 0.15995168685913086 seconds
Best Inference Speed: 0.10079312324523926 seconds
Average Inference Speed: 0.11439896197546096 seconds
Worst Inference Speed: 0.15995168685913086 seconds
Total Detections: 7633
Total Inferences: 84


Worst Speed:
Model: yolov5x6u
Best Precision: 96.82453870773315 %
Average Precision: 77.06774789553423 %
Worst Precision: 25.649192929267883 %
Best Speed: 1.6439409255981445 seconds
Average Speed: 1.966013585604154 seconds
Worst Speed: 7.776415586471558 seconds
Best Inference Speed: 1.6439409255981445 seconds
Average Inference Speed: 1.9896536781674339 seconds
Worst Inference Speed: 7.776415586471558 seconds
Total Detections: 130
Total Inferences: 84




# Model names
models = [
    "fcos_resnet", "retinanet_resnet", "retinanet_resnet_v2", "faster_rcnn_resnet",
    "faster_rcnn_resnet_v2", "faster_rcnn_mobilenet_v3", "faster_rcnn_mobilenet_v3_320",
    "mask_rcnn_resnet", "mask_rcnn_resnet_v2", "ssd_vgg16", "ssd_mobilenet_v3",
    "yolov5nu", "yolov5su", "yolov5mu", "yolov5lu", "yolov5xu", "yolov5n6u", "yolov5s6u",
    "yolov5m6u", "yolov5l6u", "yolov5x6u", "yolov8n", "yolov8s", "yolov8m", "yolov8l",
    "yolov8x", "yolov9c", "yolov9e"
]

# Precision metrics
best_precision = np.array([
    84.014970, 86.209673, 96.108669, 99.247473, 99.604189, 99.352878, 98.904562,
    98.711097, 99.413496, 95.253974, 97.862679, 89.550948, 94.160140, 95.835721,
    95.459908, 96.578825, 89.942789, 93.811536, 96.594763, 96.983385, 96.824539,
    89.877987, 96.516979, 96.096098, 96.970302, 96.952897, 95.788741, 96.190941
])
average_precision = np.array([
    33.320973, 13.884031, 15.765904, 33.835679, 33.030675, 37.715450, 38.089749,
    32.748217, 37.252643, 6.433907, 3.645044, 59.805229, 65.163396, 62.617054,
    73.181807, 75.306548, 63.648906, 68.914472, 74.374277, 76.793048, 77.067748,
    64.941649, 69.821349, 75.264953, 75.660160, 82.630968, 75.231379, 76.313950
])
worst_precision = np.array([
    20.000255, 5.000219, 5.000508, 5.061346, 5.013749, 5.046924, 5.015124,
    5.009864, 5.005625, 2.197789, 0.716251, 25.002038, 28.957847, 25.004023,
    25.045571, 25.629246, 25.590783, 25.863394, 27.020100, 25.585917, 25.649193,
    25.362319, 25.332955, 25.736323, 26.122287, 26.093510, 25.400871, 25.914165
])

# Speed metrics (seconds)
best_speed = np.array([
    1.173243, 1.377710, 1.316899, 1.353466, 2.054212, 0.283115, 0.129980,
    1.461550, 2.151899, 0.218242, 0.100793, 0.075299, 0.101922, 0.186260,
    0.299403, 0.499850, 0.160345, 0.263767, 0.548768, 0.960699, 1.643941,
    0.073291, 0.103240, 0.205022, 0.325709, 0.453079, 0.294616, 0.565530
])
average_speed = np.array([
    1.393990, 1.539637, 1.536446, 1.499311, 2.183287, 0.357185, 0.192854,
    1.619473, 2.314587, 0.292353, 0.114099, 0.131002, 0.166931, 0.285465,
    0.455376, 0.695490, 0.235294, 0.347585, 0.700632, 1.187311, 1.966014,
    0.132068, 0.177001, 0.307753, 0.510429, 0.673226, 0.420679, 0.774763
])
worst_speed = np.array([
    1.819070, 2.024222, 1.824618, 1.722821, 2.480558, 0.547197, 0.302640,
    1.898717, 2.637710, 0.477121, 0.159952, 3.792293, 3.118795, 3.431911,
    4.052781, 4.530596, 3.058439, 0.515353, 0.969860, 1.495160, 7.776416,
    3.722209, 3.933477, 4.108065, 4.404923, 5.091320, 3.567326, 4.026620
])

# Inference Speed metrics (seconds)
best_inference_speed = np.array([
    1.173243, 1.377710, 1.316899, 1.353466, 2.054212, 0.283115, 0.129980,
    1.461550, 2.151899, 0.218242, 0.100793, 0.075299, 0.101922, 0.186260,
    0.299403, 0.499850, 0.160345, 0.263767, 0.548768, 0.960699, 1.643941,
    0.073291, 0.103240, 0.205022, 0.325709, 0.453079, 0.294616, 0.565530
])
average_inference_speed = np.array([
    1.406724, 1.559174, 1.538720, 1.514175, 2.190206, 0.359016, 0.187051,
    1.626013, 2.317196, 0.294005, 0.114399, 0.142378, 0.172658, 0.299864,
    0.457812, 0.702980, 0.239423, 0.351054, 0.701676, 1.188632, 1.989654,
    0.138334, 0.188422, 0.319310, 0.512267, 0.673860, 0.426208, 0.786640
])
worst_inference_speed = np.array([
    1.819070, 2.024222, 1.824618, 1.722821, 2.480558, 0.547197, 0.302640,
    1.898717, 2.637710, 0.477121, 0.159952, 3.792293, 3.118795, 3.431911,
    4.052781, 4.530596, 3.058439, 0.515353, 0.969860, 1.495160, 7.776416,
    3.722209, 3.933477, 4.108065, 4.404923, 5.091320, 3.567326, 4.026620
])


import numpy as np
import matplotlib.pyplot as plt
import matplotlib

# Generate a color map
num_models = len(models)
cmap = matplotlib.colormaps.get_cmap('nipy_spectral')
colors = [cmap(i / num_models) for i in range(num_models)]

# Create a figure and plot with unique colors
plt.figure(figsize=(14, 10))
for i, model in enumerate(models):
    plt.scatter(average_speed[i], average_precision[i], color=colors[i], label=model, edgecolor='black')

print("DEVICE: CPU (Intel i7-14700KF)")
print("SOURCE: data/video/sample.MP4 (1080p59.94fps Video)")
print("SKIP EVERY 60 FRAMES")
print("TOTALLY PROCESSED FRAMES: 84")
print("COCO Dataset, Classes truck, bus, car, train, bicycle")
# Axis labels and plot title
plt.xlabel('Average Speed (seconds)')
plt.ylabel('Average Precision (%)')
plt.title('Model Performance: Average Detection Speed x Average Accuracy')
plt.legend(loc='upper left', bbox_to_anchor=(1.05, 1), fontsize='small')
plt.grid(True)
plt.show()

# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_precision, average_precision, worst_precision])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Precision (%)", "Average Precision (%)", "Worst Precision (%)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

plt.title("Model Precision Metrics")
plt.show()

# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_speed, average_speed, worst_speed])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Speed (s)", "Average Speed (s)", "Worst Speed (s)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

plt.title("Model Speed Metrics")
plt.show()

# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_inference_speed, average_inference_speed, worst_inference_speed])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Inference Speed (s)", "Average Inference Speed (s)", "Worst Inference Speed (s)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

plt.title("Model Inference Speed Metrics")
plt.show()


import cv2
import torch
from torchvision.models.detection import *
from torchvision.io.image import read_image
from torchvision.utils import draw_bounding_boxes
from torchvision.transforms.functional import to_pil_image
import torchvision.transforms as transforms
from PIL import Image
import IPython
from IPython.display import display, clear_output
import time
import traceback
import math
from ultralytics import YOLO

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

inputType="video"
source = "data/video/sample.MP4" if inputType=="video" else 0
cam = cv2.VideoCapture(source)
display_handle=display(None, display_id=True)
every = 60

start = 0 if source!=0 else -1
end = int(cam.get(cv2.CAP_PROP_FRAME_COUNT)) if source!=0 else -1
if source!=0: cam.set(1, start)

frameCount=start
while_safety=0
saved_count=0

transform = transforms.Compose([
    #transforms.Resize((300, 300)),
    transforms.ToTensor(),
])

class DetectionResult():
    def __init__(self, model, label, precision, speed, deviceInput=None, sourceInput=None, frame=None, boundingBox=None, details=None):
        self.model = model
        self.label = label
        self.precision = precision.item() if torch.is_tensor(precision) else precision
        self.speed = speed.item() if torch.is_tensor(speed) else speed
        self.device = deviceInput if deviceInput not None else device
        self.source = sourceInput if sourceInput not None else source
        self.frame = frame
        self.boundingBox = boundingBox
        self.details = details

class ModelResults():
    def __init__(self, model, deviceInput, source, detections,
                 best_precision, average_precision, worst_precision,
                 best_speed, average_speed, worst_speed,
                 best_inference_speed, average_inference_speed, worst_inference_speed,
                 total_detections, total_inferences):
        self.model = model
        self.device = deviceInput if deviceInput not None else device
        self.source = source
        self.detections = detections
        self.best_precision = best_precision
        self.average_precision = average_precision
        self.worst_precision = worst_precision
        self.best_speed = best_speed
        self.average_speed = average_speed
        self.worst_speed = worst_speed
        self.best_inference_speed = best_inference_speed
        self.average_inference_speed = average_inference_speed
        self.worst_inference_speed = worst_inference_speed
        self.total_detections = total_detections
        self.total_inferences = total_inferences

    def show(self):
        print("Model:", self.model,
              "\nDevice:",self.device,
              "\nSource:",self.source,
              "\nBest Precision:", self.best_precision,
              "%\nAverage Precision:", self.average_precision,
              "%\nWorst Precision:", self.worst_precision,
              "%\nBest Speed:", self.best_speed,
              "seconds\nAverage Speed:", self.average_speed,
              "seconds\nWorst Speed:", self.worst_speed,
              "seconds\nBest Inference Speed:", self.best_inference_speed,
              "seconds\nAverage Inference Speed:", self.average_inference_speed,
              "seconds\nWorst Inference Speed:", self.worst_inference_speed,
              "seconds\nTotal Detections:", self.total_detections,
              "\nTotal Inferences:", self.total_inferences,
              "\n\n")

class DetectionModel():
    def __init__(self, concept, backbone, model, weights, reference, color=None, results=None):
        self.concept = concept
        self.backbone = backbone
        self.model = model(weights) if concept=="YOLO" else model(weights=weights.DEFAULT)
        self.weights = weights
        self.reference = reference
        self.color = color
        self.detections = []
        self.results = results

    def addDetection(self, detection):
        self.detections.append(detection)

    def getDetections(self):
        return self.detections

    def clearDetections(self):
        self.detections = []

    def detectionsCount(self):
        return len(self.detections)

    def inferencesCount(self):
        length = 0
        addedFrames = []
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    addedFrames.append(self.detections[i].frame)
        return len(addedFrames)

    def getAveragePrecision(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].precision
            avg = sum/len(self.detections)
        return  avg.item() if torch.is_tensor(avg) else avg

    def getMaxPrecision(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].precision:
                max=self.detections[i].precision
        return max.item() if torch.is_tensor(max) else max

    def getMinPrecision(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].precision:
                min=self.detections[i].precision
        return 0.0 if min==9999999.99 else min.item() if torch.is_tensor(min) else min

    def getAverageSpeed(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].speed
            avg = sum/len(self.detections)
        return avg

    def getMaxSpeed(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].speed:
                max=self.detections[i].speed
        return max

    def getMinSpeed(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].speed:
                min=self.detections[i].speed
        return 0.0 if min==9999999.99 else min

    def getAverageInferenceSpeed(self):
        sum=0.0
        avg=0.0
        addedFrames=[]
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    sum+=self.detections[i].speed
                    addedFrames.append(self.detections[i].frame)
            avg = sum/len(addedFrames) if len(addedFrames)>0 else 0.0
        return avg

    def getMaxInferenceSpeed(self):
        max=0.0
        addedFrames=[]
        for i in range(len(self.detections)):
            if max<self.detections[i].speed and self.detections[i].frame not in addedFrames:
                max=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return max

    def getMinInferenceSpeed(self):
        min=9999999.99
        addedFrames=[]
        for i in range(len(self.detections)):
            if min>self.detections[i].speed and self.detections[i].frame not in addedFrames:
                min=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return 0.0 if min==9999999.99 else min

    def filterPrecisionsBelow(self, minPrecision):
        newDetections = []
        for i in range(len(self.detections)):
            if self.detections[i].precision>=minPrecision:
                newDetections.append(self.detections[i])
        self.detections = newDetections

    def hasResults(self):
        return self.results is not None

    def loadResults(self, results):
        self.results = results

    def printResults(self):
        if not self.hasResults():
            sourceInput = self.detections[0].source if len(self.detections)>0 else source
            deviceInput = self.detections[0].device if len(self.detections)>0 else device
            self.loadResults(
                ModelResults(
                    self.reference, deviceInput, sourceInput, self.detections,
                    self.getMaxPrecision(), self.getAveragePrecision(), self.getMinPrecision(),
                    self.getMinSpeed(), self.getAverageSpeed(), self.getMaxSpeed(),
                    self.getMinInferenceSpeed(), self.getAverageInferenceSpeed(), self.getMaxInferenceSpeed(),
                    self.detectionsCount(), self.inferencesCount()
                )
            )

        self.results.show()

target_classes = ["truck", "bus", "car", "train", "bicycle"]

VEHICLE_DETECTION_MODELS = [
    DetectionModel("FCOS", "resnet50_fpn", fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights, "fcos_resnet", (255,0,0)),
    DetectionModel("RetinaNet", "resnet50_fpn", retinanet_resnet50_fpn, RetinaNet_ResNet50_FPN_Weights, "retinanet_resnet", (255,255,0)),
    DetectionModel("RetinaNet", "resnet50_fpn_v2", retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights, "retinanet_resnet_v2", (0,234,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn", fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights, "faster_rcnn_resnet", (170,0,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn_v2", fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights, "faster_rcnn_resnet_v2", (255,127,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_fpn", fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights, "faster_rcnn_mobilenet_v3", (191,255,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_320_fpn", fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights, "faster_rcnn_mobilenet_v3_320", (0,149,255)),
    DetectionModel("MaskRCNN", "resnet50_fpn", maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights, "mask_rcnn_resnet", (255,0,170)),
    DetectionModel("MaskRCNN", "resnet50_fpn_v2", maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights, "mask_rcnn_resnet_v2", (255,212,0)),
    DetectionModel("SSD300", "vgg16", ssd300_vgg16, SSD300_VGG16_Weights, "ssd_vgg16", (106,255,0)),
    DetectionModel("SSDLite320", "mobilenet_v3_large", ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights, "ssd_mobilenet_v3", (0,64,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5nu.pt', "yolov5nu", (237,185,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5su.pt', "yolov5su", (185,215,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5mu.pt', "yolov5mu", (231,233,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5lu.pt', "yolov5lu", (220,185,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5xu.pt', "yolov5xu", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5n6u.pt', "yolov5n6u", (170,0,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5s6u.pt', "yolov5s6u", (255,255,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5m6u.pt', "yolov5m6u", (143,106,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5l6u.pt', "yolov5l6u", (255,0,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5x6u.pt', "yolov5x6u", (0,64,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8n.pt', "yolov8n", (0,234,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8s.pt', "yolov8s", (115,115,115)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8m.pt', "yolov8m", (204,204,204)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8l.pt', "yolov8l", (255,0,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8x.pt', "yolov8x", (255,255,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9c.pt', "yolov9c", (0,234,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9e.pt', "yolov9e", (170,0,255))
]

def process(frame, frameIndex=-1, source=None):
    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        processed_frame = frame.copy()

        if currentModel.concept=="YOLO":
            start_time = time.time()
            results = currentModel.model(processed_frame, agnostic_nms=True, verbose=False, device=device)[0]
            processing_time = time.time() - start_time
            for result in results:
                detection_count = result.boxes.shape[0]
                for j in range(detection_count):
                    cls = int(result.boxes.cls[j].item())
                    name = result.names[cls]
                    confidence = float(result.boxes.conf[j].item())
                    bounding_box = result.boxes.xyxy[j].cpu().numpy()
                    if name in target_classes:
                        x1, y1, x2, y2 = [int(x) for x in bounding_box]
                        # Ensure x1 < x2 and y1 < y2
                        x1, x2 = min(x1, x2), max(x1, x2)
                        y1, y2 = min(y1, y2), max(y1, y2)
                        label = '{} ({:.2f}%) ({:.3f}s) {}'.format(name.upper(), confidence*100, processing_time, currentModel.reference)
                        print(label)
                        VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, name, confidence*100, processing_time, device, source, frameIndex, bounding_box, label))
                        cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                        cv2.putText(processed_frame, label, (x1+10, y1+25), 0, 0.8, currentModel.color, 2)
        else:
            weights = currentModel.weights.DEFAULT
            model = currentModel.model
            model.eval()

            # Convert frame to RGB and PIL Image, then apply transformation
            frame_rgb = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            image = transform(pil_image).unsqueeze(0).to(device)

            # Object detection
            start_time = time.time()
            with torch.no_grad():
                predictions = model(image)

            # Processing time
            processing_time = time.time() - start_time

            # Visualization
            scores = predictions[0]['scores']
            boxes = predictions[0]['boxes']
            labels = predictions[0]['labels']
            confidence_threshold = 0.5

            for score, box, label in zip(scores, boxes, labels):
                # and score > confidence_threshold
                if weights.meta["categories"][label.item()] in target_classes:
                    x1, y1, x2, y2 = map(int, box)
                    label_text = f'{weights.meta["categories"][label.item()].upper()} ({score*100:.2f}%) ({processing_time:.3f}s) {currentModel.reference}'
                    print(label_text)
                    VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, weights.meta["categories"][label.item()], float(score.item())*100, processing_time, device, source, frameIndex, box, label_text))
                    cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                    cv2.putText(processed_frame, label_text, (x1, y1+20), 0, 0.8, currentModel.color, 2)

        img = Image.fromarray(processed_frame, 'RGB')
        display(img)
        clear_output(wait=True)
    return frame

def handleFrame(frame, frameIndex=-1, source=None):
    processed_frame = process(frame,frameIndex,source)

def handleRelease():
    cam.release()
    print("Source released.")

    BestPrecision=None
    BestAvgPrecision=None
    WorstPrecision=None
    BestSpeed=None
    BestAvgSpeed=None
    WorstSpeed=None
    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        currentModel.printResults()
        if BestPrecision is None or BestPrecision.getMaxPrecision()<=currentModel.getMaxPrecision():
            BestPrecision = currentModel
        if BestAvgPrecision is None or BestAvgPrecision.getAveragePrecision()<=currentModel.getAveragePrecision():
            BestAvgPrecision = currentModel
        if WorstPrecision is None or WorstPrecision.getMinPrecision()>=currentModel.getMinPrecision():
            WorstPrecision = currentModel
        if BestSpeed is None or BestSpeed.getMinSpeed()>=currentModel.getMinSpeed():
            BestSpeed = currentModel
        if BestAvgSpeed is None or BestAvgSpeed.getAverageSpeed()>=currentModel.getAverageSpeed():
            BestAvgSpeed = currentModel
        if WorstSpeed is None or WorstSpeed.getMaxSpeed()<=currentModel.getMaxSpeed():
            WorstSpeed = currentModel

    if BestPrecision is not None:
        print("Best Precision:")
        BestPrecision.printResults()
    if BestAvgPrecision is not None:
        print("Best Average Precision:")
        BestAvgPrecision.printResults()
    if WorstPrecision is not None:
        print("Worst Precision:")
        WorstPrecision.printResults()
    if BestSpeed is not None:
        print("Best Speed:")
        BestSpeed.printResults()
    if BestAvgSpeed is not None:
        print("Best Average Speed:")
        BestAvgSpeed.printResults()
    if WorstSpeed is not None:
        print("Worst Speed:")
        WorstSpeed.printResults()

if not cam.isOpened(): print("Error: Could not open source.")
else:
    try:
        while True if end<0 else frameCount<end:
            _, frame = cam.read()
            if frame is None:
                if source!=0:
                    if while_safety > 2000: break
                    while_safety += 1
                    continue
                else:
                    print("Error: Could not capture frame.")
                    cam.release()
                    break

            if every>0:
                if (frameCount+1)%math.floor(every) == 0:
                    while_safety = 0
                    handleFrame(frame, frameCount, source)
            else:
                while_safety=0
                handleFrame(frame)
            frameCount += 1
        handleRelease()
    except KeyboardInterrupt:
        handleRelease()
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Source could not be released.")
        except:
            pass
        traceback.print_exc()


Source released.
Model: fcos_resnet
Best Precision: 84.01497006416321 %
Average Precision: 34.31522064877971 %
Worst Precision: 20.016169548034668 %
Best Speed: 1.3147878646850586 seconds
Average Speed: 1.6757691221861963 seconds
Worst Speed: 2.063619613647461 seconds
Best Inference Speed: 1.3147878646850586 seconds
Average Inference Speed: 1.7024165590604146 seconds
Worst Inference Speed: 2.063619613647461 seconds
Total Detections: 809
Total Inferences: 84


Model: retinanet_resnet
Best Precision: 86.20967268943787 %
Average Precision: 13.914700622645844 %
Worst Precision: 5.000218749046326 %
Best Speed: 1.4359545707702637 seconds
Average Speed: 1.8022473817854596 seconds
Worst Speed: 2.2492806911468506 seconds
Best Inference Speed: 1.4359545707702637 seconds
Average Inference Speed: 1.7978160778681438 seconds
Worst Inference Speed: 2.2492806911468506 seconds
Total Detections: 1824
Total Inferences: 84


Model: retinanet_resnet_v2
Best Precision: 96.10866904258728 %
Average Precision: 16.147036431847436 %
Worst Precision: 5.0005074590444565 %
Best Speed: 1.4612510204315186 seconds
Average Speed: 1.8591078235914833 seconds
Worst Speed: 2.364978313446045 seconds
Best Inference Speed: 1.4612510204315186 seconds
Average Inference Speed: 1.8891064609800066 seconds
Worst Inference Speed: 2.364978313446045 seconds
Total Detections: 1520
Total Inferences: 84


Model: faster_rcnn_resnet
Best Precision: 99.24747347831726 %
Average Precision: 35.27663397164389 %
Worst Precision: 5.061345919966698 %
Best Speed: 1.47483229637146 seconds
Average Speed: 1.6666392898013833 seconds
Worst Speed: 1.87638258934021 seconds
Best Inference Speed: 1.47483229637146 seconds
Average Inference Speed: 1.672638592265901 seconds
Worst Inference Speed: 1.87638258934021 seconds
Total Detections: 699
Total Inferences: 84


Model: faster_rcnn_resnet_v2
Best Precision: 99.60418939590454 %
Average Precision: 33.39243041243491 %
Worst Precision: 5.043753981590271 %
Best Speed: 2.1579532623291016 seconds
Average Speed: 2.3665511102476486 seconds
Worst Speed: 2.6729507446289062 seconds
Best Inference Speed: 2.1579532623291016 seconds
Average Inference Speed: 2.3762204902512685 seconds
Worst Inference Speed: 2.6729507446289062 seconds
Total Detections: 811
Total Inferences: 84


Model: faster_rcnn_mobilenet_v3
Best Precision: 99.35287833213806 %
Average Precision: 37.630537044670845 %
Worst Precision: 5.046924203634262 %
Best Speed: 0.32329869270324707 seconds
Average Speed: 0.43964511818355984 seconds
Worst Speed: 0.6118793487548828 seconds
Best Inference Speed: 0.32329869270324707 seconds
Average Inference Speed: 0.43872777053288053 seconds
Worst Inference Speed: 0.6118793487548828 seconds
Total Detections: 360
Total Inferences: 84


Model: faster_rcnn_mobilenet_v3_320
Best Precision: 98.90456199645996 %
Average Precision: 39.035241101324026 %
Worst Precision: 5.015124008059502 %
Best Speed: 0.15319299697875977 seconds
Average Speed: 0.23418851563767792 seconds
Worst Speed: 0.38596081733703613 seconds
Best Inference Speed: 0.15319299697875977 seconds
Average Inference Speed: 0.2331624570347014 seconds
Worst Inference Speed: 0.38596081733703613 seconds
Total Detections: 337
Total Inferences: 84


Model: mask_rcnn_resnet
Best Precision: 98.711097240448 %
Average Precision: 33.507678455415444 %
Worst Precision: 5.009864270687103 %
Best Speed: 1.5584602355957031 seconds
Average Speed: 1.8010743240311764 seconds
Worst Speed: 2.048617124557495 seconds
Best Inference Speed: 1.5584602355957031 seconds
Average Inference Speed: 1.8049743885085696 seconds
Worst Inference Speed: 2.048617124557495 seconds
Total Detections: 683
Total Inferences: 84


Model: mask_rcnn_resnet_v2
Best Precision: 99.41349625587463 %
Average Precision: 37.65366062972049 %
Worst Precision: 5.005624517798424 %
Best Speed: 2.281203269958496 seconds
Average Speed: 2.565716048681869 seconds
Worst Speed: 2.8655378818511963 seconds
Best Inference Speed: 2.281203269958496 seconds
Average Inference Speed: 2.5558127221607028 seconds
Worst Inference Speed: 2.8655378818511963 seconds
Total Detections: 629
Total Inferences: 84


Model: ssd_vgg16
Best Precision: 95.25397419929504 %
Average Precision: 6.4159581760102995 %
Worst Precision: 2.197788655757904 %
Best Speed: 0.23832917213439941 seconds
Average Speed: 0.3788482849748267 seconds
Worst Speed: 0.5898928642272949 seconds
Best Inference Speed: 0.23832917213439941 seconds
Average Inference Speed: 0.390825507186708 seconds
Worst Inference Speed: 0.5898928642272949 seconds
Total Detections: 4726
Total Inferences: 84


Model: ssd_mobilenet_v3
Best Precision: 97.86267876625061 %
Average Precision: 3.639173901591716 %
Worst Precision: 0.7162508554756641 %
Best Speed: 0.10209488868713379 seconds
Average Speed: 0.1220671693685016 seconds
Worst Speed: 0.16486883163452148 seconds
Best Inference Speed: 0.10209488868713379 seconds
Average Inference Speed: 0.123209467955998 seconds
Worst Inference Speed: 0.16486883163452148 seconds
Total Detections: 7116
Total Inferences: 84


Model: yolov5nu
Best Precision: 89.55094814300537 %
Average Precision: 60.619191741043664 %
Worst Precision: 25.00203847885132 %
Best Speed: 0.08482599258422852 seconds
Average Speed: 0.1447036603711686 seconds
Worst Speed: 3.8249993324279785 seconds
Best Inference Speed: 0.08482599258422852 seconds
Average Inference Speed: 0.15523647113018726 seconds
Worst Inference Speed: 3.8249993324279785 seconds
Total Detections: 106
Total Inferences: 83


Model: yolov5su
Best Precision: 94.16013956069946 %
Average Precision: 65.40687045838573 %
Worst Precision: 28.957846760749817 %
Best Speed: 0.11858272552490234 seconds
Average Speed: 0.20037320580812965 seconds
Worst Speed: 3.202934741973877 seconds
Best Inference Speed: 0.11858272552490234 seconds
Average Inference Speed: 0.2074131483123416 seconds
Worst Inference Speed: 3.202934741973877 seconds
Total Detections: 101
Total Inferences: 84


Model: yolov5mu
Best Precision: 95.83572149276733 %
Average Precision: 62.67815518473822 %
Worst Precision: 25.00402331352234 %
Best Speed: 0.2091972827911377 seconds
Average Speed: 0.3180890253611973 seconds
Worst Speed: 3.5376415252685547 seconds
Best Inference Speed: 0.2091972827911377 seconds
Average Inference Speed: 0.3337593646276565 seconds
Worst Inference Speed: 3.5376415252685547 seconds
Total Detections: 126
Total Inferences: 84


Model: yolov5lu
Best Precision: 95.45990824699402 %
Average Precision: 73.07906499854079 %
Worst Precision: 25.045570731163025 %
Best Speed: 0.3530998229980469 seconds
Average Speed: 0.5372842453621529 seconds
Worst Speed: 3.817481756210327 seconds
Best Inference Speed: 0.3530998229980469 seconds
Average Inference Speed: 0.5462017343157813 seconds
Worst Inference Speed: 3.817481756210327 seconds
Total Detections: 111
Total Inferences: 84


Model: yolov5xu
Best Precision: 96.57882452011108 %
Average Precision: 75.16264031762663 %
Worst Precision: 25.6292462348938 %
Best Speed: 0.5162966251373291 seconds
Average Speed: 0.7972573549851127 seconds
Worst Speed: 4.638925552368164 seconds
Best Inference Speed: 0.5162966251373291 seconds
Average Inference Speed: 0.8226594981693086 seconds
Worst Inference Speed: 4.638925552368164 seconds
Total Detections: 115
Total Inferences: 84


Model: yolov5n6u
Best Precision: 89.94278907775879 %
Average Precision: 64.1948022654182 %
Worst Precision: 25.59078335762024 %
Best Speed: 0.17982006072998047 seconds
Average Speed: 0.27164958652697113 seconds
Worst Speed: 3.2517809867858887 seconds
Best Inference Speed: 0.17982006072998047 seconds
Average Inference Speed: 0.27749409562065486 seconds
Worst Inference Speed: 3.2517809867858887 seconds
Total Detections: 95
Total Inferences: 84


Model: yolov5s6u
Best Precision: 93.81153583526611 %
Average Precision: 68.75755653633335 %
Worst Precision: 25.863394141197205 %
Best Speed: 0.3013331890106201 seconds
Average Speed: 0.39206495517637674 seconds
Worst Speed: 0.6164541244506836 seconds
Best Inference Speed: 0.3013331890106201 seconds
Average Inference Speed: 0.39915244550590057 seconds
Worst Inference Speed: 0.6164541244506836 seconds
Total Detections: 123
Total Inferences: 83


Model: yolov5m6u
Best Precision: 96.59476280212402 %
Average Precision: 74.30387755070836 %
Worst Precision: 27.020099759101868 %
Best Speed: 0.662224292755127 seconds
Average Speed: 0.8063682977818261 seconds
Worst Speed: 1.104745626449585 seconds
Best Inference Speed: 0.662224292755127 seconds
Average Inference Speed: 0.8202784463583704 seconds
Worst Inference Speed: 1.104745626449585 seconds
Total Detections: 121
Total Inferences: 83


Model: yolov5l6u
Best Precision: 96.98338508605957 %
Average Precision: 76.82155244718722 %
Worst Precision: 25.58591663837433 %
Best Speed: 1.1083221435546875 seconds
Average Speed: 1.3617152353612387 seconds
Worst Speed: 1.94746732711792 seconds
Best Inference Speed: 1.1083221435546875 seconds
Average Inference Speed: 1.3701340977738543 seconds
Worst Inference Speed: 1.94746732711792 seconds
Total Detections: 123
Total Inferences: 82


Model: yolov5x6u
Best Precision: 96.82453870773315 %
Average Precision: 77.49861474148929 %
Worst Precision: 25.649192929267883 %
Best Speed: 1.6921055316925049 seconds
Average Speed: 2.1854661237448454 seconds
Worst Speed: 7.180513381958008 seconds
Best Inference Speed: 1.6921055316925049 seconds
Average Inference Speed: 2.2230422837393626 seconds
Worst Inference Speed: 7.180513381958008 seconds
Total Detections: 128
Total Inferences: 84


Model: yolov8n
Best Precision: 89.87798690795898 %
Average Precision: 64.94164858201538 %
Worst Precision: 25.362318754196167 %
Best Speed: 0.08196425437927246 seconds
Average Speed: 0.13927612882671933 seconds
Worst Speed: 3.286020517349243 seconds
Best Inference Speed: 0.08196425437927246 seconds
Average Inference Speed: 0.14423098734446935 seconds
Worst Inference Speed: 3.286020517349243 seconds
Total Detections: 99
Total Inferences: 84


Model: yolov8s
Best Precision: 96.51697874069214 %
Average Precision: 69.695577560327 %
Worst Precision: 25.33295452594757 %
Best Speed: 0.11999130249023438 seconds
Average Speed: 0.20084755644839034 seconds
Worst Speed: 3.2653698921203613 seconds
Best Inference Speed: 0.11999130249023438 seconds
Average Inference Speed: 0.2124943421000526 seconds
Worst Inference Speed: 3.2653698921203613 seconds
Total Detections: 117
Total Inferences: 84


Model: yolov8m
Best Precision: 96.09609842300415 %
Average Precision: 75.19671217017218 %
Worst Precision: 25.73632299900055 %
Best Speed: 0.21349716186523438 seconds
Average Speed: 0.3369998231940313 seconds
Worst Speed: 3.898172616958618 seconds
Best Inference Speed: 0.21349716186523438 seconds
Average Inference Speed: 0.3480233351389567 seconds
Worst Inference Speed: 3.898172616958618 seconds
Total Detections: 109
Total Inferences: 84


Model: yolov8l
Best Precision: 96.97030186653137 %
Average Precision: 76.0991909750947 %
Worst Precision: 26.122286915779114 %
Best Speed: 0.37825632095336914 seconds
Average Speed: 0.5933225240793314 seconds
Worst Speed: 4.1653971672058105 seconds
Best Inference Speed: 0.37825632095336914 seconds
Average Inference Speed: 0.6058329428945269 seconds
Worst Inference Speed: 4.1653971672058105 seconds
Total Detections: 111
Total Inferences: 84


Model: yolov8x
Best Precision: 96.95289731025696 %
Average Precision: 82.78851406140761 %
Worst Precision: 26.093509793281555 %
Best Speed: 0.5179610252380371 seconds
Average Speed: 0.7353021036494862 seconds
Worst Speed: 5.12945294380188 seconds
Best Inference Speed: 0.5179610252380371 seconds
Average Inference Speed: 0.7519198656082153 seconds
Worst Inference Speed: 5.12945294380188 seconds
Total Detections: 110
Total Inferences: 84


Model: yolov9c
Best Precision: 95.78874111175537 %
Average Precision: 75.3404206729361 %
Worst Precision: 25.40087103843689 %
Best Speed: 0.3158283233642578 seconds
Average Speed: 0.46963344301496235 seconds
Worst Speed: 3.6877005100250244 seconds
Best Inference Speed: 0.3158283233642578 seconds
Average Inference Speed: 0.4779577851295471 seconds
Worst Inference Speed: 3.6877005100250244 seconds
Total Detections: 112
Total Inferences: 84


Model: yolov9e
Best Precision: 96.19094133377075 %
Average Precision: 76.23227601870894 %
Worst Precision: 25.914165377616882 %
Best Speed: 0.6387596130371094 seconds
Average Speed: 0.857001380994916 seconds
Worst Speed: 4.554773330688477 seconds
Best Inference Speed: 0.6387596130371094 seconds
Average Inference Speed: 0.8848170666467576 seconds
Worst Inference Speed: 4.554773330688477 seconds
Total Detections: 128
Total Inferences: 84


Best Precision:
Model: faster_rcnn_resnet_v2
Best Precision: 99.60418939590454 %
Average Precision: 33.39243041243491 %
Worst Precision: 5.043753981590271 %
Best Speed: 2.1579532623291016 seconds
Average Speed: 2.3665511102476486 seconds
Worst Speed: 2.6729507446289062 seconds
Best Inference Speed: 2.1579532623291016 seconds
Average Inference Speed: 2.3762204902512685 seconds
Worst Inference Speed: 2.6729507446289062 seconds
Total Detections: 811
Total Inferences: 84


Best Average Precision:
Model: yolov8x
Best Precision: 96.95289731025696 %
Average Precision: 82.78851406140761 %
Worst Precision: 26.093509793281555 %
Best Speed: 0.5179610252380371 seconds
Average Speed: 0.7353021036494862 seconds
Worst Speed: 5.12945294380188 seconds
Best Inference Speed: 0.5179610252380371 seconds
Average Inference Speed: 0.7519198656082153 seconds
Worst Inference Speed: 5.12945294380188 seconds
Total Detections: 110
Total Inferences: 84


Worst Precision:
Model: ssd_mobilenet_v3
Best Precision: 97.86267876625061 %
Average Precision: 3.639173901591716 %
Worst Precision: 0.7162508554756641 %
Best Speed: 0.10209488868713379 seconds
Average Speed: 0.1220671693685016 seconds
Worst Speed: 0.16486883163452148 seconds
Best Inference Speed: 0.10209488868713379 seconds
Average Inference Speed: 0.123209467955998 seconds
Worst Inference Speed: 0.16486883163452148 seconds
Total Detections: 7116
Total Inferences: 84


Best Speed:
Model: yolov8n
Best Precision: 89.87798690795898 %
Average Precision: 64.94164858201538 %
Worst Precision: 25.362318754196167 %
Best Speed: 0.08196425437927246 seconds
Average Speed: 0.13927612882671933 seconds
Worst Speed: 3.286020517349243 seconds
Best Inference Speed: 0.08196425437927246 seconds
Average Inference Speed: 0.14423098734446935 seconds
Worst Inference Speed: 3.286020517349243 seconds
Total Detections: 99
Total Inferences: 84


Best Average Speed:
Model: ssd_mobilenet_v3
Best Precision: 97.86267876625061 %
Average Precision: 3.639173901591716 %
Worst Precision: 0.7162508554756641 %
Best Speed: 0.10209488868713379 seconds
Average Speed: 0.1220671693685016 seconds
Worst Speed: 0.16486883163452148 seconds
Best Inference Speed: 0.10209488868713379 seconds
Average Inference Speed: 0.123209467955998 seconds
Worst Inference Speed: 0.16486883163452148 seconds
Total Detections: 7116
Total Inferences: 84


Worst Speed:
Model: yolov5x6u
Best Precision: 96.82453870773315 %
Average Precision: 77.49861474148929 %
Worst Precision: 25.649192929267883 %
Best Speed: 1.6921055316925049 seconds
Average Speed: 2.1854661237448454 seconds
Worst Speed: 7.180513381958008 seconds
Best Inference Speed: 1.6921055316925049 seconds
Average Inference Speed: 2.2230422837393626 seconds
Worst Inference Speed: 7.180513381958008 seconds
Total Detections: 128
Total Inferences: 84



import numpy as np
import matplotlib.pyplot as plt
import matplotlib

# Model names and data
models = [
    "fcos_resnet", "retinanet_resnet", "retinanet_resnet_v2", "faster_rcnn_resnet",
    "faster_rcnn_resnet_v2", "faster_rcnn_mobilenet_v3", "faster_rcnn_mobilenet_v3_320",
    "mask_rcnn_resnet", "mask_rcnn_resnet_v2", "ssd_vgg16", "ssd_mobilenet_v3",
    "yolov5nu", "yolov5su", "yolov5mu", "yolov5lu", "yolov5xu", "yolov5n6u", "yolov5s6u",
    "yolov5m6u", "yolov5l6u", "yolov5x6u", "yolov8n", "yolov8s", "yolov8m", "yolov8l",
    "yolov8x", "yolov9c", "yolov9e"
]

# Average Speed and Precision
speed_avg = np.array([
    1.6758, 1.8022, 1.8591, 1.6666, 2.3666, 0.4396, 0.2342, 1.8011, 2.5657, 0.3788,
    0.1221, 0.1447, 0.2004, 0.3181, 0.5373, 0.7973, 0.2716, 0.3921, 0.8064, 1.3617,
    2.1855, 0.1393, 0.2008, 0.3370, 0.5933, 0.7353, 0.4696, 0.8570
])
precision_avg = np.array([
    34.3152, 13.9147, 16.1470, 35.2766, 33.3924, 37.6305, 39.0352, 33.5077, 37.6537,
    6.4159, 3.6392, 60.6192, 65.4069, 62.6782, 73.0791, 75.1626, 64.1948, 68.7576,
    74.3039, 76.8216, 77.4986, 64.9416, 69.6956, 75.1967, 76.0992, 82.7885, 75.3404, 76.2323
])

# Generate a color map
num_models = len(models)
cmap = matplotlib.colormaps.get_cmap('nipy_spectral')
colors = [cmap(i / num_models) for i in range(num_models)]

# Create a figure and plot with unique colors
plt.figure(figsize=(14, 10))
for i, model in enumerate(models):
    plt.scatter(speed_avg[i], precision_avg[i], color=colors[i], label=model, edgecolor='black')

print("DEVICE: CPU (Intel i7-14700KF)")
print("SOURCE: data/video/sample.MP4 (1080p59.94fps Video)")
print("SKIP EVERY 60 FRAMES")
print("TOTALLY PROCESSED FRAMES: 84")
# Axis labels and plot title
plt.xlabel('Average Speed (seconds)')
plt.ylabel('Average Precision (%)')
plt.title('Model Performance: Average Detection Speed x Average Accuracy')
plt.legend(loc='upper left', bbox_to_anchor=(1.05, 1), fontsize='small')
plt.grid(True)
plt.show()


import numpy as np
import matplotlib.pyplot as plt
import matplotlib

# Model names
models = [
    "fcos_resnet", "retinanet_resnet", "retinanet_resnet_v2", "faster_rcnn_resnet",
    "faster_rcnn_resnet_v2", "faster_rcnn_mobilenet_v3", "faster_rcnn_mobilenet_v3_320",
    "mask_rcnn_resnet", "mask_rcnn_resnet_v2", "ssd_vgg16", "ssd_mobilenet_v3",
    "yolov5nu", "yolov5su", "yolov5mu", "yolov5lu", "yolov5xu", "yolov5n6u", "yolov5s6u",
    "yolov5m6u", "yolov5l6u", "yolov5x6u", "yolov8n", "yolov8s", "yolov8m", "yolov8l",
    "yolov8x", "yolov9c", "yolov9e"
]

# Average Inference Speed
inference_speed_avg = np.array([
    1.7024, 1.7978, 1.8891, 1.6726, 2.3762, 0.4387, 0.2332, 1.8050, 2.5558, 0.3908,
    0.1232, 0.1552, 0.2074, 0.3338, 0.5462, 0.8227, 0.2775, 0.3992, 0.8203, 1.3701,
    2.2230, 0.1442, 0.2125, 0.3480, 0.6058, 0.7519, 0.4779, 0.8848
])

# Average Precision
precision_avg = np.array([
    34.3152, 13.9147, 16.1470, 35.2766, 33.3924, 37.6305, 39.0352, 33.5077, 37.6537,
    6.4159, 3.6392, 60.6192, 65.4069, 62.6782, 73.0791, 75.1626, 64.1948, 68.7576,
    74.3039, 76.8216, 77.4986, 64.9416, 69.6956, 75.1967, 76.0992, 82.7885, 75.3404, 76.2323
])

# Generate a color map
num_models = len(models)
cmap = matplotlib.colormaps.get_cmap('nipy_spectral')
colors = [cmap(i / num_models) for i in range(num_models)]

# Create a figure and plot with unique colors
plt.figure(figsize=(14, 10))
for i, model in enumerate(models):
    plt.scatter(inference_speed_avg[i], precision_avg[i], color=colors[i], label=model, edgecolor='black')

# Axis labels and plot title
print("DEVICE: CPU (Intel i7-14700KF)")
print("SOURCE: data/video/sample.MP4 (1080p59.94fps Video)")
print("SKIP EVERY 60 FRAMES")
print("TOTALLY PROCESSED FRAMES: 84")
# Axis labels and plot title
plt.xlabel('Average Speed (seconds)')
plt.ylabel('Average Precision (%)')
plt.title('Model Performance: Average Inference Speed x Average Accuracy')
plt.legend(loc='upper left', bbox_to_anchor=(1.05, 1), fontsize='small')
plt.grid(True)
plt.show()


import numpy as np
import matplotlib.pyplot as plt

# Model names
models = [
    "fcos_resnet", "retinanet_resnet", "retinanet_resnet_v2", "faster_rcnn_resnet",
    "faster_rcnn_resnet_v2", "faster_rcnn_mobilenet_v3", "faster_rcnn_mobilenet_v3_320",
    "mask_rcnn_resnet", "mask_rcnn_resnet_v2", "ssd_vgg16", "ssd_mobilenet_v3",
    "yolov5nu", "yolov5su", "yolov5mu", "yolov5lu", "yolov5xu", "yolov5n6u", "yolov5s6u",
    "yolov5m6u", "yolov5l6u", "yolov5x6u", "yolov8n", "yolov8s", "yolov8m", "yolov8l",
    "yolov8x", "yolov9c", "yolov9e"
]

# Best, Average, and Worst Precision for each model
best_precision = np.array([
    84.01, 86.21, 96.11, 99.25, 99.60, 99.35, 98.90, 98.71, 99.41,
    95.25, 97.86, 89.55, 94.16, 95.84, 95.46, 96.58, 89.94, 93.81,
    96.59, 96.98, 96.82, 89.88, 96.52, 96.10, 96.97, 96.95, 95.79, 96.19
])

average_precision = np.array([
    34.32, 13.91, 16.15, 35.28, 33.39, 37.63, 39.04, 33.51, 37.65,
    6.42, 3.64, 60.62, 65.41, 62.68, 73.08, 75.16, 64.19, 68.76,
    74.30, 76.82, 77.50, 64.94, 69.70, 75.20, 76.10, 82.79, 75.34, 76.23
])

worst_precision = np.array([
    20.02, 5.00, 5.00, 5.06, 5.04, 5.05, 5.02, 5.01, 5.01,
    2.20, 0.72, 25.00, 28.96, 25.00, 25.05, 25.63, 25.59, 25.86,
    27.02, 25.59, 25.65, 25.36, 25.33, 25.74, 26.12, 26.09, 25.40, 25.91
])


# Set up the figure and axis for plotting (if plotting a graph alongside)
fig, ax = plt.subplots(figsize=(12, 8))
# Hide axes
ax.axis('tight')
ax.axis('off')

# The table data
table_data = np.array([models, best_precision, average_precision, worst_precision]).T

# Create the table
table = plt.table(cellText=table_data, colLabels=["Model", "Best Precision (%)", "Average Precision (%)", "Worst Precision (%)"],
                  cellLoc='center', loc='center', colColours=["palegreen"] * 4)

table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

print("DEVICE: CPU (Intel i7-14700KF)")
print("SOURCE: data/video/sample.MP4 (1080p59.94fps Video)")
print("SKIP EVERY 60 FRAMES")
print("TOTALLY PROCESSED FRAMES: 84")

plt.title("Model Precision Metrics")
plt.show()


import numpy as np
import matplotlib.pyplot as plt

# Model names
models = [
    "fcos_resnet", "retinanet_resnet", "retinanet_resnet_v2", "faster_rcnn_resnet",
    "faster_rcnn_resnet_v2", "faster_rcnn_mobilenet_v3", "faster_rcnn_mobilenet_v3_320",
    "mask_rcnn_resnet", "mask_rcnn_resnet_v2", "ssd_vgg16", "ssd_mobilenet_v3",
    "yolov5nu", "yolov5su", "yolov5mu", "yolov5lu", "yolov5xu", "yolov5n6u", "yolov5s6u",
    "yolov5m6u", "yolov5l6u", "yolov5x6u", "yolov8n", "yolov8s", "yolov8m", "yolov8l",
    "yolov8x", "yolov9c", "yolov9e"
]

# Best, Average, and Worst Inference Speed for each model (in seconds)
best_speed = np.array([
    1.3148, 1.4360, 1.4613, 1.4748, 2.1580, 0.3233, 0.1532, 1.5585, 2.2812, 0.2383,
    0.1021, 0.0848, 0.1186, 0.2092, 0.3531, 0.5163, 0.1798, 0.3013, 0.6622, 1.1083,
    1.6921, 0.0819, 0.1199, 0.2135, 0.3783, 0.5179, 0.3158, 0.6388
])
average_speed = np.array([
    1.7024, 1.7978, 1.8891, 1.6726, 2.3762, 0.4387, 0.2332, 1.8050, 2.5558, 0.3908,
    0.1232, 0.1552, 0.2074, 0.3338, 0.5462, 0.8227, 0.2775, 0.3992, 0.8203, 1.3701,
    2.2230, 0.1442, 0.2125, 0.3480, 0.6058, 0.7519, 0.4779, 0.8848
])
worst_speed = np.array([
    2.0636, 2.2493, 2.3650, 1.8764, 2.6730, 0.6119, 0.3860, 2.0486, 2.8655, 0.5899,
    0.1649, 3.8250, 3.2030, 3.5376, 3.8175, 4.6390, 3.2518, 0.6165, 1.1047, 1.9475,
    7.1805, 3.2860, 3.2654, 3.8982, 4.1654, 5.1295, 3.6877, 4.5548
])


# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_speed, average_speed, worst_speed])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Speed (s)", "Average Speed (s)", "Worst Speed (s)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

print("DEVICE: CPU (Intel i7-14700KF)")
print("SOURCE: data/video/sample.MP4 (1080p59.94fps Video)")
print("SKIP EVERY 60 FRAMES")
print("TOTALLY PROCESSED FRAMES: 84")

plt.title("Model Inference Speed Metrics")
plt.show()


import numpy as np
import matplotlib.pyplot as plt

# Model names
models = [
    "fcos_resnet", "retinanet_resnet", "retinanet_resnet_v2", "faster_rcnn_resnet",
    "faster_rcnn_resnet_v2", "faster_rcnn_mobilenet_v3", "faster_rcnn_mobilenet_v3_320",
    "mask_rcnn_resnet", "mask_rcnn_resnet_v2", "ssd_vgg16", "ssd_mobilenet_v3",
    "yolov5nu", "yolov5su", "yolov5mu", "yolov5lu", "yolov5xu", "yolov5n6u", "yolov5s6u",
    "yolov5m6u", "yolov5l6u", "yolov5x6u", "yolov8n", "yolov8s", "yolov8m", "yolov8l",
    "yolov8x", "yolov9c", "yolov9e"
]

# Best, Average, and Worst Speed for each model (in seconds)
best_speed = np.array([
    1.3148, 1.4359, 1.4613, 1.4748, 2.1579, 0.3233, 0.1532, 1.5585, 2.2812, 0.2383,
    0.1021, 0.0848, 0.1186, 0.2092, 0.3531, 0.5163, 0.1798, 0.3013, 0.6622, 1.1083,
    1.6921, 0.0819, 0.1200, 0.2135, 0.3783, 0.5179, 0.3158, 0.6388
])
average_speed = np.array([
    1.6758, 1.8022, 1.8591, 1.6666, 2.3666, 0.4396, 0.2342, 1.8011, 2.5657, 0.3908,
    0.1221, 0.1447, 0.2004, 0.3181, 0.5373, 0.7973, 0.2716, 0.3921, 0.8064, 1.3617,
    2.1855, 0.1393, 0.2008, 0.3370, 0.5933, 0.7353, 0.4696, 0.8570
])
worst_speed = np.array([
    2.0636, 2.2493, 2.3650, 1.8764, 2.6730, 0.6119, 0.3860, 2.0486, 2.8655, 0.5899,
    0.1649, 3.8250, 3.2030, 3.5376, 3.8175, 4.6390, 3.2518, 0.6165, 1.1047, 1.9475,
    7.1805, 3.2860, 3.2654, 3.8982, 4.1654, 5.1295, 3.6877, 4.5548
])


# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_speed, average_speed, worst_speed])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Speed (s)", "Average Speed (s)", "Worst Speed (s)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

plt.title("Model Speed Metrics")
plt.show()


import cv2
import torch
from torchvision.models.detection import *
from torchvision.io.image import read_image
from torchvision.utils import draw_bounding_boxes
from torchvision.transforms.functional import to_pil_image
import torchvision.transforms as transforms
from PIL import Image
import IPython
from IPython.display import display, clear_output
import time
import traceback
import math
from ultralytics import YOLO

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

transform = transforms.Compose([
    #transforms.Resize((300, 300)),
    transforms.ToTensor(),
])

class DetectionModel():
    def __init__(self, concept, backbone, model, weights, reference, color=None):
        self.concept = concept
        self.backbone = backbone
        self.model = model(weights) if concept=="YOLO" else model(weights=weights.DEFAULT)
        self.weights = weights
        self.reference = reference
        self.color = color
        self.detections = []

    def addDetection(self, detection):
        self.detections.append(detection)

    def getDetections(self):
        return self.detections

    def clearDetections(self):
        self.detections = []

    def detectionsCount(self):
        return len(self.detections)

    def inferencesCount(self):
        length = 0
        addedFrames = []
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    addedFrames.append(self.detections[i].frame)
        return len(addedFrames)

    def getAveragePrecision(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].precision
            avg = sum/len(self.detections)
        return  avg.item() if torch.is_tensor(avg) else avg

    def getMaxPrecision(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].precision:
                max=self.detections[i].precision
        return max.item() if torch.is_tensor(max) else max

    def getMinPrecision(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].precision:
                min=self.detections[i].precision
        return 0.0 if min==9999999.99 else min.item() if torch.is_tensor(min) else min

    def getAverageSpeed(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].speed
            avg = sum/len(self.detections)
        return avg

    def getMaxSpeed(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].speed:
                max=self.detections[i].speed
        return max

    def getMinSpeed(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].speed:
                min=self.detections[i].speed
        return 0.0 if min==9999999.99 else min

    def getAverageInferenceSpeed(self):
        sum=0.0
        avg=0.0
        addedFrames=[]
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    sum+=self.detections[i].speed
                    addedFrames.append(self.detections[i].frame)
            avg = sum/len(addedFrames) if len(addedFrames)>0 else 0.0
        return avg

    def getMaxInferenceSpeed(self):
        max=0.0
        addedFrames=[]
        for i in range(len(self.detections)):
            if max<self.detections[i].speed and self.detections[i].frame not in addedFrames:
                max=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return max

    def getMinInferenceSpeed(self):
        min=9999999.99
        addedFrames=[]
        for i in range(len(self.detections)):
            if min>self.detections[i].speed and self.detections[i].frame not in addedFrames:
                min=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return 0.0 if min==9999999.99 else min

    def filterPrecisionsBelow(self, minPrecision):
        newDetections = []
        for i in range(len(self.detections)):
            if self.detections[i].precision>=minPrecision:
                newDetections.append(self.detections[i])
        self.detections = newDetections

    def printResults(self):
        print("Model:", self.reference,
              "\nBest Precision:", self.getMaxPrecision(),
              "%\nAverage Precision:", self.getAveragePrecision(),
              "%\nWorst Precision:", self.getMinPrecision(),
              "%\nBest Speed:", self.getMinSpeed(),
              "seconds\nAverage Speed:", self.getAverageSpeed(),
              "seconds\nWorst Speed:", self.getMaxSpeed(),
              "seconds\nBest Inference Speed:", self.getMinInferenceSpeed(),
              "seconds\nAverage Inference Speed:", self.getAverageInferenceSpeed(),
              "seconds\nWorst Inference Speed:", self.getMaxInferenceSpeed(),
              "seconds\nTotal Detections:", self.detectionsCount(),
              "\nTotal Inferences:", self.inferencesCount(),
              "\n\n")

class DetectionResult():
    def __init__(self, model, label, precision, speed, device=None, source=None, frame=None, boundingBox=None, details=None):
        self.model = model
        self.label = label
        self.precision = precision.item() if torch.is_tensor(precision) else precision
        self.speed = speed.item() if torch.is_tensor(speed) else speed
        self.device = device
        self.source = source
        self.frame = frame
        self.boundingBox = boundingBox
        self.details = details

target_classes = ["truck", "bus", "car", "train", "bicycle"]


VEHICLE_DETECTION_MODELS = [
    DetectionModel("FCOS", "resnet50_fpn", fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights, "fcos_resnet", (255,0,0)),
    DetectionModel("RetinaNet", "resnet50_fpn", retinanet_resnet50_fpn, RetinaNet_ResNet50_FPN_Weights, "retinanet_resnet", (255,255,0)),
    DetectionModel("RetinaNet", "resnet50_fpn_v2", retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights, "retinanet_resnet_v2", (0,234,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn", fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights, "faster_rcnn_resnet", (170,0,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn_v2", fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights, "faster_rcnn_resnet_v2", (255,127,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_fpn", fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights, "faster_rcnn_mobilenet_v3", (191,255,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_320_fpn", fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights, "faster_rcnn_mobilenet_v3_320", (0,149,255)),
    DetectionModel("MaskRCNN", "resnet50_fpn", maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights, "mask_rcnn_resnet", (255,0,170)),
    DetectionModel("MaskRCNN", "resnet50_fpn_v2", maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights, "mask_rcnn_resnet_v2", (255,212,0)),
    DetectionModel("SSD300", "vgg16", ssd300_vgg16, SSD300_VGG16_Weights, "ssd_vgg16", (106,255,0)),
    DetectionModel("SSDLite320", "mobilenet_v3_large", ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights, "ssd_mobilenet_v3", (0,64,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5nu.pt', "yolov5nu", (237,185,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5su.pt', "yolov5su", (185,215,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5mu.pt', "yolov5mu", (231,233,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5lu.pt', "yolov5lu", (220,185,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5xu.pt', "yolov5xu", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5n6u.pt', "yolov5n6u", (143,35,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5s6u.pt', "yolov5s6u", (35,98,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5m6u.pt', "yolov5m6u", (143,106,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5l6u.pt', "yolov5l6u", (107,35,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5x6u.pt', "yolov5x6u", (79,143,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8n.pt', "yolov8n", (0,0,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8s.pt', "yolov8s", (115,115,115)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8m.pt', "yolov8m", (204,204,204)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8l.pt', "yolov8l", (255,0,0)), # cycle restarts
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8x.pt', "yolov8x", (255,255,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9c.pt', "yolov9c", (0,234,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9e.pt', "yolov9e", (170,0,255))
]

def process(frame, frameIndex=-1, source=None):
    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        processed_frame = frame.copy()

        if currentModel.concept=="YOLO":
            start_time = time.time()
            results = currentModel.model(processed_frame, agnostic_nms=True, verbose=False)[0]
            processing_time = time.time() - start_time
            for result in results:
                detection_count = result.boxes.shape[0]
                for j in range(detection_count):
                    cls = int(result.boxes.cls[j].item())
                    name = result.names[cls]
                    confidence = float(result.boxes.conf[j].item())
                    bounding_box = result.boxes.xyxy[j].cpu().numpy()
                    if name in target_classes:
                        x1, y1, x2, y2 = [int(x) for x in bounding_box]
                        # Ensure x1 < x2 and y1 < y2
                        x1, x2 = min(x1, x2), max(x1, x2)
                        y1, y2 = min(y1, y2), max(y1, y2)
                        label = '{} ({:.2f}%) ({:.3f}s) {}'.format(name.upper(), confidence*100, processing_time, currentModel.reference)
                        print(label)
                        VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, name, confidence*100, processing_time, device, source, frameIndex, bounding_box, label))
                        cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                        cv2.putText(processed_frame, label, (x1+10, y1+25), 0, 0.8, currentModel.color, 2)
        else:
            weights = currentModel.weights.DEFAULT
            model = currentModel.model
            model.to(device)
            model.eval()

            # Convert frame to RGB and PIL Image, then apply transformation
            frame_rgb = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            image = transform(pil_image).unsqueeze(0).to(device)

            # Object detection
            start_time = time.time()
            with torch.no_grad():
                predictions = model(image)

            # Processing time
            processing_time = time.time() - start_time

            # Visualization
            scores = predictions[0]['scores']
            boxes = predictions[0]['boxes']
            labels = predictions[0]['labels']
            confidence_threshold = 0.5

            for score, box, label in zip(scores, boxes, labels):
                # and score > confidence_threshold
                if weights.meta["categories"][label.item()] in target_classes:
                    x1, y1, x2, y2 = map(int, box)
                    label_text = f'{weights.meta["categories"][label.item()].upper()} ({score*100:.2f}%) ({processing_time:.3f}s) {currentModel.reference}'
                    print(label_text)
                    VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, weights.meta["categories"][label.item()], float(score.item())*100, processing_time, device, source, frameIndex, box, label_text))
                    cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                    cv2.putText(processed_frame, label_text, (x1, y1+20), 0, 0.8, currentModel.color, 2)

        img = Image.fromarray(processed_frame, 'RGB')
        display(img)
        clear_output(wait=True)
    return frame

def handleFrame(frame, frameIndex=-1, source=None):
    processed_frame = process(frame,frameIndex,source)

def handleRelease():
    cam.release()
    print("Source released.")

    print(device)
    print(source)

    BestPrecision=None
    BestAvgPrecision=None
    WorstPrecision=None
    BestSpeed=None
    BestAvgSpeed=None
    WorstSpeed=None
    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        currentModel.printResults()
        if BestPrecision is None or BestPrecision.getMaxPrecision()<=currentModel.getMaxPrecision():
            BestPrecision = currentModel
        if BestAvgPrecision is None or BestAvgPrecision.getAveragePrecision()<=currentModel.getAveragePrecision():
            BestAvgPrecision = currentModel
        if WorstPrecision is None or WorstPrecision.getMinPrecision()>=currentModel.getMinPrecision():
            WorstPrecision = currentModel
        if BestSpeed is None or BestSpeed.getMinSpeed()>=currentModel.getMinSpeed():
            BestSpeed = currentModel
        if BestAvgSpeed is None or BestAvgSpeed.getAverageSpeed()>=currentModel.getAverageSpeed():
            BestAvgSpeed = currentModel
        if WorstSpeed is None or WorstSpeed.getMaxSpeed()<=currentModel.getMaxSpeed():
            WorstSpeed = currentModel

    if BestPrecision is not None:
        print("Best Precision:")
        BestPrecision.printResults()
    if BestAvgPrecision is not None:
        print("Best Average Precision:")
        BestAvgPrecision.printResults()
    if WorstPrecision is not None:
        print("Worst Precision:")
        WorstPrecision.printResults()
    if BestSpeed is not None:
        print("Best Speed:")
        BestSpeed.printResults()
    if BestAvgSpeed is not None:
        print("Best Average Speed:")
        BestAvgSpeed.printResults()
    if WorstSpeed is not None:
        print("Worst Speed:")
        WorstSpeed.printResults()

inputType="video"
source = "data/video/vecteezy_car-and-truck-traffic-on-the-highway-in-europe-poland_7957364.MP4" if inputType=="video" else 0
cam = cv2.VideoCapture(source)
display_handle=display(None, display_id=True)
every = 60

start = 0 if source!=0 else -1
end = int(cam.get(cv2.CAP_PROP_FRAME_COUNT)) if source!=0 else -1
if source!=0: cam.set(1, start)

frameCount=start
while_safety=0
saved_count=0

if not cam.isOpened(): print("Error: Could not open source.")
else:
    try:
        while True if end<0 else frameCount<end:
            _, frame = cam.read()
            if frame is None:
                if source!=0:
                    if while_safety > 2000: break
                    while_safety += 1
                    continue
                else:
                    print("Error: Could not capture frame.")
                    cam.release()
                    break

            if every>0:
                if (frameCount+1)%math.floor(every) == 0:
                    while_safety = 0
                    handleFrame(frame, frameCount, source)
            else:
                while_safety=0
                handleFrame(frame)
            frameCount += 1
        handleRelease()
    except KeyboardInterrupt:
        handleRelease()
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Source could not be released.")
        except:
            pass
        traceback.print_exc()


Source released.
cpu
data/video/vecteezy_car-and-truck-traffic-on-the-highway-in-europe-poland_7957364.MP4
Model: fcos_resnet
Best Precision: 88.38133811950684 %
Average Precision: 41.43324620949412 %
Worst Precision: 25.272268056869507 %
Best Speed: 1.4783985614776611 seconds
Average Speed: 1.6542653643969194 seconds
Worst Speed: 2.144266366958618 seconds
Best Inference Speed: 1.4783985614776611 seconds
Average Inference Speed: 1.6587184429168702 seconds
Worst Inference Speed: 2.144266366958618 seconds
Total Detections: 412
Total Inferences: 5


Model: retinanet_resnet
Best Precision: 94.31947469711304 %
Average Precision: 26.668555585175863 %
Worst Precision: 5.019436776638031 %
Best Speed: 1.554119348526001 seconds
Average Speed: 1.6321384598774538 seconds
Worst Speed: 1.7094383239746094 seconds
Best Inference Speed: 1.554119348526001 seconds
Average Inference Speed: 1.6301272869110108 seconds
Worst Inference Speed: 1.7094383239746094 seconds
Total Detections: 1076
Total Inferences: 5


Model: retinanet_resnet_v2
Best Precision: 94.65104341506958 %
Average Precision: 28.262778328694772 %
Worst Precision: 5.091705545783043 %
Best Speed: 1.5576648712158203 seconds
Average Speed: 1.5887088875959414 seconds
Worst Speed: 1.6650762557983398 seconds
Best Inference Speed: 1.5576648712158203 seconds
Average Inference Speed: 1.5913765907287598 seconds
Worst Inference Speed: 1.6650762557983398 seconds
Total Detections: 808
Total Inferences: 5


Model: faster_rcnn_resnet
Best Precision: 99.8128354549408 %
Average Precision: 34.53327437617997 %
Worst Precision: 5.013693496584892 %
Best Speed: 1.5493974685668945 seconds
Average Speed: 1.5758296567241439 seconds
Worst Speed: 1.6262013912200928 seconds
Best Inference Speed: 1.5493974685668945 seconds
Average Inference Speed: 1.5766679286956786 seconds
Worst Inference Speed: 1.6262013912200928 seconds
Total Detections: 449
Total Inferences: 5


Model: faster_rcnn_resnet_v2
Best Precision: 99.86345171928406 %
Average Precision: 44.35066577444331 %
Worst Precision: 5.013208091259003 %
Best Speed: 2.185394763946533 seconds
Average Speed: 2.2690543633716596 seconds
Worst Speed: 2.362788438796997 seconds
Best Inference Speed: 2.185394763946533 seconds
Average Inference Speed: 2.2694464206695555 seconds
Worst Inference Speed: 2.362788438796997 seconds
Total Detections: 347
Total Inferences: 5


Model: faster_rcnn_mobilenet_v3
Best Precision: 99.15496110916138 %
Average Precision: 25.370935108899793 %
Worst Precision: 5.0094034522771835 %
Best Speed: 0.5133233070373535 seconds
Average Speed: 0.6018298966229375 seconds
Worst Speed: 0.6727936267852783 seconds
Best Inference Speed: 0.5133233070373535 seconds
Average Inference Speed: 0.6020257472991943 seconds
Worst Inference Speed: 0.6727936267852783 seconds
Total Detections: 433
Total Inferences: 5


Model: faster_rcnn_mobilenet_v3_320
Best Precision: 99.13085103034973 %
Average Precision: 20.852530515668068 %
Worst Precision: 5.037260055541992 %
Best Speed: 0.22069215774536133 seconds
Average Speed: 0.28212835463610564 seconds
Worst Speed: 0.37053799629211426 seconds
Best Inference Speed: 0.22069215774536133 seconds
Average Inference Speed: 0.28377113342285154 seconds
Worst Inference Speed: 0.37053799629211426 seconds
Total Detections: 220
Total Inferences: 5


Model: mask_rcnn_resnet
Best Precision: 99.87894892692566 %
Average Precision: 38.42796711716801 %
Worst Precision: 5.086960643529892 %
Best Speed: 2.317978858947754 seconds
Average Speed: 2.5589838886260985 seconds
Worst Speed: 2.837881088256836 seconds
Best Inference Speed: 2.317978858947754 seconds
Average Inference Speed: 2.550578308105469 seconds
Worst Inference Speed: 2.837881088256836 seconds
Total Detections: 400
Total Inferences: 5


Model: mask_rcnn_resnet_v2
Best Precision: 99.84723925590515 %
Average Precision: 45.330616445013675 %
Worst Precision: 5.062004923820496 %
Best Speed: 3.0094871520996094 seconds
Average Speed: 3.2602682888128194 seconds
Worst Speed: 3.429543972015381 seconds
Best Inference Speed: 3.0094871520996094 seconds
Average Inference Speed: 3.262221574783325 seconds
Worst Inference Speed: 3.429543972015381 seconds
Total Detections: 314
Total Inferences: 5


Model: ssd_vgg16
Best Precision: 94.76912021636963 %
Average Precision: 9.736605516859683 %
Worst Precision: 5.517248809337616 %
Best Speed: 0.4633173942565918 seconds
Average Speed: 0.5438689841278668 seconds
Worst Speed: 0.5931768417358398 seconds
Best Inference Speed: 0.4633173942565918 seconds
Average Inference Speed: 0.5431578636169434 seconds
Worst Inference Speed: 0.5931768417358398 seconds
Total Detections: 928
Total Inferences: 5


Model: ssd_mobilenet_v3
Best Precision: 98.03178310394287 %
Average Precision: 7.859095972838126 %
Worst Precision: 3.05889043956995 %
Best Speed: 0.1787426471710205 seconds
Average Speed: 0.19995710539138964 seconds
Worst Speed: 0.2336561679840088 seconds
Best Inference Speed: 0.1787426471710205 seconds
Average Inference Speed: 0.20047483444213868 seconds
Worst Inference Speed: 0.2336561679840088 seconds
Total Detections: 1159
Total Inferences: 5


Model: yolov5nu
Best Precision: 91.97649955749512 %
Average Precision: 58.57486256531307 %
Worst Precision: 28.14248502254486 %
Best Speed: 0.0981130599975586 seconds
Average Speed: 0.88833144732884 seconds
Worst Speed: 3.8585567474365234 seconds
Best Inference Speed: 0.0981130599975586 seconds
Average Inference Speed: 0.8746856689453125 seconds
Worst Inference Speed: 3.8585567474365234 seconds
Total Detections: 49
Total Inferences: 5


Model: yolov5su
Best Precision: 94.07822489738464 %
Average Precision: 64.22873812337076 %
Worst Precision: 25.000491738319397 %
Best Speed: 0.14306187629699707 seconds
Average Speed: 0.6831054379863124 seconds
Worst Speed: 3.3073654174804688 seconds
Best Inference Speed: 0.14306187629699707 seconds
Average Inference Speed: 0.8059768199920654 seconds
Worst Inference Speed: 3.3073654174804688 seconds
Total Detections: 62
Total Inferences: 5


Model: yolov5mu
Best Precision: 96.37896418571472 %
Average Precision: 66.38702933604901 %
Worst Precision: 26.028957962989807 %
Best Speed: 0.31354832649230957 seconds
Average Speed: 0.9810541929342808 seconds
Worst Speed: 3.500016689300537 seconds
Best Inference Speed: 0.31354832649230957 seconds
Average Inference Speed: 0.9652681827545166 seconds
Worst Inference Speed: 3.500016689300537 seconds
Total Detections: 78
Total Inferences: 5


Model: yolov5lu
Best Precision: 95.80461382865906 %
Average Precision: 66.45397571630257 %
Worst Precision: 26.010149717330933 %
Best Speed: 0.39235806465148926 seconds
Average Speed: 1.1470441984575848 seconds
Worst Speed: 3.957641839981079 seconds
Best Inference Speed: 0.39235806465148926 seconds
Average Inference Speed: 1.1998708248138428 seconds
Worst Inference Speed: 3.957641839981079 seconds
Total Detections: 86
Total Inferences: 5


Model: yolov5xu
Best Precision: 97.3269522190094 %
Average Precision: 69.34243030846119 %
Worst Precision: 25.085321068763733 %
Best Speed: 0.732499361038208 seconds
Average Speed: 1.6781020950187335 seconds
Worst Speed: 4.662721633911133 seconds
Best Inference Speed: 0.732499361038208 seconds
Average Inference Speed: 1.617397928237915 seconds
Worst Inference Speed: 4.662721633911133 seconds
Total Detections: 88
Total Inferences: 5


Model: yolov5n6u
Best Precision: 94.2396879196167 %
Average Precision: 64.08193600177765 %
Worst Precision: 29.559960961341858 %
Best Speed: 0.2709951400756836 seconds
Average Speed: 0.8536689186096191 seconds
Worst Speed: 3.2117486000061035 seconds
Best Inference Speed: 0.2709951400756836 seconds
Average Inference Speed: 0.8903384208679199 seconds
Worst Inference Speed: 3.2117486000061035 seconds
Total Detections: 75
Total Inferences: 5


Model: yolov5s6u
Best Precision: 95.54755091667175 %
Average Precision: 65.64587518409058 %
Worst Precision: 25.403180718421936 %
Best Speed: 0.46065783500671387 seconds
Average Speed: 1.3535711529490713 seconds
Worst Speed: 4.264893054962158 seconds
Best Inference Speed: 0.46065783500671387 seconds
Average Inference Speed: 1.2370962142944335 seconds
Worst Inference Speed: 4.264893054962158 seconds
Total Detections: 91
Total Inferences: 5


Model: yolov5m6u
Best Precision: 96.92941308021545 %
Average Precision: 68.4352623634651 %
Worst Precision: 25.797632336616516 %
Best Speed: 0.8086121082305908 seconds
Average Speed: 1.8413387243864967 seconds
Worst Speed: 5.168080806732178 seconds
Best Inference Speed: 0.8086121082305908 seconds
Average Inference Speed: 1.7893301486968993 seconds
Worst Inference Speed: 5.168080806732178 seconds
Total Detections: 122
Total Inferences: 5


Model: yolov5l6u
Best Precision: 96.77505493164062 %
Average Precision: 70.50009181907585 %
Worst Precision: 25.110462307929993 %
Best Speed: 1.2785289287567139 seconds
Average Speed: 2.4489242515048466 seconds
Worst Speed: 5.89592432975769 seconds
Best Inference Speed: 1.2785289287567139 seconds
Average Inference Speed: 2.336281728744507 seconds
Worst Inference Speed: 5.89592432975769 seconds
Total Detections: 111
Total Inferences: 5


Model: yolov5x6u
Best Precision: 96.98181748390198 %
Average Precision: 70.57452546663521 %
Worst Precision: 26.683789491653442 %
Best Speed: 1.8950295448303223 seconds
Average Speed: 3.3354815885055165 seconds
Worst Speed: 7.9948036670684814 seconds
Best Inference Speed: 1.8950295448303223 seconds
Average Inference Speed: 3.252710962295532 seconds
Worst Inference Speed: 7.9948036670684814 seconds
Total Detections: 121
Total Inferences: 5


Model: yolov8n
Best Precision: 95.45491933822632 %
Average Precision: 60.306799984895264 %
Worst Precision: 25.879019498825073 %
Best Speed: 0.10694026947021484 seconds
Average Speed: 0.8645601089184101 seconds
Worst Speed: 3.8568460941314697 seconds
Best Inference Speed: 0.10694026947021484 seconds
Average Inference Speed: 0.8903901100158691 seconds
Worst Inference Speed: 3.8568460941314697 seconds
Total Detections: 52
Total Inferences: 5


Model: yolov8s
Best Precision: 95.18426060676575 %
Average Precision: 65.38355725281167 %
Worst Precision: 27.328482270240784 %
Best Speed: 0.14949584007263184 seconds
Average Speed: 0.8322482903798422 seconds
Worst Speed: 3.609773635864258 seconds
Best Inference Speed: 0.14949584007263184 seconds
Average Inference Speed: 0.8897275924682617 seconds
Worst Inference Speed: 3.609773635864258 seconds
Total Detections: 66
Total Inferences: 5


Model: yolov8m
Best Precision: 96.5444028377533 %
Average Precision: 68.05787021533037 %
Worst Precision: 29.550635814666748 %
Best Speed: 0.2841668128967285 seconds
Average Speed: 1.1231638162564008 seconds
Worst Speed: 4.249230861663818 seconds
Best Inference Speed: 0.2841668128967285 seconds
Average Inference Speed: 1.150191879272461 seconds
Worst Inference Speed: 4.249230861663818 seconds
Total Detections: 78
Total Inferences: 5


Model: yolov8l
Best Precision: 96.16449475288391 %
Average Precision: 69.06973085431166 %
Worst Precision: 26.58754587173462 %
Best Speed: 0.39304232597351074 seconds
Average Speed: 1.1759941106618836 seconds
Worst Speed: 3.9690730571746826 seconds
Best Inference Speed: 0.39304232597351074 seconds
Average Inference Speed: 1.1841650009155273 seconds
Worst Inference Speed: 3.9690730571746826 seconds
Total Detections: 86
Total Inferences: 5


Model: yolov8x
Best Precision: 96.99693322181702 %
Average Precision: 68.54418788818603 %
Worst Precision: 25.09877383708954 %
Best Speed: 0.5446245670318604 seconds
Average Speed: 1.492222451149149 seconds
Worst Speed: 4.074252128601074 seconds
Best Inference Speed: 0.5446245670318604 seconds
Average Inference Speed: 1.3767151832580566 seconds
Worst Inference Speed: 4.074252128601074 seconds
Total Detections: 94
Total Inferences: 5


Model: yolov9c
Best Precision: 96.3855504989624 %
Average Precision: 71.11662730072321 %
Worst Precision: 25.430238246917725 %
Best Speed: 0.3612549304962158 seconds
Average Speed: 1.2335194978821145 seconds
Worst Speed: 3.502370834350586 seconds
Best Inference Speed: 0.3612549304962158 seconds
Average Inference Speed: 1.1274725437164306 seconds
Worst Inference Speed: 3.502370834350586 seconds
Total Detections: 89
Total Inferences: 5


Model: yolov9e
Best Precision: 97.04502820968628 %
Average Precision: 70.97662907271157 %
Worst Precision: 26.8461674451828 %
Best Speed: 0.7776787281036377 seconds
Average Speed: 2.3225121185893105 seconds
Worst Speed: 7.7722368240356445 seconds
Best Inference Speed: 0.7776787281036377 seconds
Average Inference Speed: 2.223586416244507 seconds
Worst Inference Speed: 7.7722368240356445 seconds
Total Detections: 84
Total Inferences: 5


Best Precision:
Model: mask_rcnn_resnet
Best Precision: 99.87894892692566 %
Average Precision: 38.42796711716801 %
Worst Precision: 5.086960643529892 %
Best Speed: 2.317978858947754 seconds
Average Speed: 2.5589838886260985 seconds
Worst Speed: 2.837881088256836 seconds
Best Inference Speed: 2.317978858947754 seconds
Average Inference Speed: 2.550578308105469 seconds
Worst Inference Speed: 2.837881088256836 seconds
Total Detections: 400
Total Inferences: 5


Best Average Precision:
Model: yolov9c
Best Precision: 96.3855504989624 %
Average Precision: 71.11662730072321 %
Worst Precision: 25.430238246917725 %
Best Speed: 0.3612549304962158 seconds
Average Speed: 1.2335194978821145 seconds
Worst Speed: 3.502370834350586 seconds
Best Inference Speed: 0.3612549304962158 seconds
Average Inference Speed: 1.1274725437164306 seconds
Worst Inference Speed: 3.502370834350586 seconds
Total Detections: 89
Total Inferences: 5


Worst Precision:
Model: ssd_mobilenet_v3
Best Precision: 98.03178310394287 %
Average Precision: 7.859095972838126 %
Worst Precision: 3.05889043956995 %
Best Speed: 0.1787426471710205 seconds
Average Speed: 0.19995710539138964 seconds
Worst Speed: 0.2336561679840088 seconds
Best Inference Speed: 0.1787426471710205 seconds
Average Inference Speed: 0.20047483444213868 seconds
Worst Inference Speed: 0.2336561679840088 seconds
Total Detections: 1159
Total Inferences: 5


Best Speed:
Model: yolov5nu
Best Precision: 91.97649955749512 %
Average Precision: 58.57486256531307 %
Worst Precision: 28.14248502254486 %
Best Speed: 0.0981130599975586 seconds
Average Speed: 0.88833144732884 seconds
Worst Speed: 3.8585567474365234 seconds
Best Inference Speed: 0.0981130599975586 seconds
Average Inference Speed: 0.8746856689453125 seconds
Worst Inference Speed: 3.8585567474365234 seconds
Total Detections: 49
Total Inferences: 5


Best Average Speed:
Model: ssd_mobilenet_v3
Best Precision: 98.03178310394287 %
Average Precision: 7.859095972838126 %
Worst Precision: 3.05889043956995 %
Best Speed: 0.1787426471710205 seconds
Average Speed: 0.19995710539138964 seconds
Worst Speed: 0.2336561679840088 seconds
Best Inference Speed: 0.1787426471710205 seconds
Average Inference Speed: 0.20047483444213868 seconds
Worst Inference Speed: 0.2336561679840088 seconds
Total Detections: 1159
Total Inferences: 5


Worst Speed:
Model: yolov5x6u
Best Precision: 96.98181748390198 %
Average Precision: 70.57452546663521 %
Worst Precision: 26.683789491653442 %
Best Speed: 1.8950295448303223 seconds
Average Speed: 3.3354815885055165 seconds
Worst Speed: 7.9948036670684814 seconds
Best Inference Speed: 1.8950295448303223 seconds
Average Inference Speed: 3.252710962295532 seconds
Worst Inference Speed: 7.9948036670684814 seconds
Total Detections: 121
Total Inferences: 5




import numpy as np

# Model names
models = [
    "fcos_resnet", "retinanet_resnet", "retinanet_resnet_v2", "faster_rcnn_resnet",
    "faster_rcnn_resnet_v2", "faster_rcnn_mobilenet_v3", "faster_rcnn_mobilenet_v3_320",
    "mask_rcnn_resnet", "mask_rcnn_resnet_v2", "ssd_vgg16", "ssd_mobilenet_v3",
    "yolov5nu", "yolov5su", "yolov5mu", "yolov5lu", "yolov5xu", "yolov5n6u", "yolov5s6u",
    "yolov5m6u", "yolov5l6u", "yolov5x6u", "yolov8n", "yolov8s", "yolov8m", "yolov8l",
    "yolov8x", "yolov9c", "yolov9e"
]

# Best, Average, and Worst Precision
best_precision = np.array([
    88.38, 94.32, 94.65, 99.81, 99.86, 99.15, 99.13, 99.88, 99.85, 94.77, 98.03,
    91.98, 94.08, 96.38, 95.80, 97.33, 94.24, 95.55, 96.93, 96.78, 96.98, 95.45,
    95.18, 96.54, 96.16, 96.99, 96.39, 97.05
])
average_precision = np.array([
    41.43, 26.67, 28.26, 34.53, 44.35, 25.37, 20.85, 38.43, 45.33, 9.74, 7.86,
    58.57, 64.23, 66.39, 66.45, 69.34, 64.08, 65.65, 68.44, 70.50, 70.57, 60.31,
    65.38, 68.06, 69.07, 68.54, 71.12, 70.98
])
worst_precision = np.array([
    25.27, 5.02, 5.09, 5.01, 5.01, 5.01, 5.04, 5.09, 5.06, 5.52, 3.06, 28.14,
    25.00, 26.03, 26.01, 25.09, 29.56, 25.40, 25.80, 25.11, 26.68, 25.88, 27.33,
    29.55, 26.59, 25.10, 25.43, 26.85
])

# Best, Average, and Worst Speed
best_speed = np.array([
    1.4784, 1.5541, 1.5577, 1.5494, 2.1854, 0.5133, 0.2207, 2.3180, 3.0095, 0.4633,
    0.1787, 0.0981, 0.1431, 0.3135, 0.3924, 0.7325, 0.2710, 0.4607, 0.8086, 1.2785,
    1.8950, 0.1069, 0.1495, 0.2842, 0.3930, 0.5446, 0.3613, 0.7777
])
average_speed = np.array([
    1.6543, 1.6321, 1.5887, 1.5758, 2.2691, 0.6018, 0.2821, 2.5590, 3.2603, 0.5439,
    0.2000, 0.8883, 0.6831, 0.9811, 1.1470, 1.6781, 0.8537, 1.3536, 1.8413, 2.4489,
    3.3355, 0.8646, 0.8322, 1.1232, 1.1760, 1.4922, 1.2335, 2.3225
])
worst_speed = np.array([
    2.1443, 1.7094, 1.6651, 1.6262, 2.3628, 0.6728, 0.3705, 2.8379, 3.4295, 0.5932,
    0.2337, 3.8586, 3.3074, 3.5000, 3.9576, 4.6627, 3.2117, 4.2649, 5.1681, 5.8959,
    7.9948, 3.8568, 3.6098, 4.2492, 3.9691, 4.0743, 3.5024, 7.7722
])

# Best, Average, and Worst Inference Speed for each model (in seconds)
best_inference_speed = np.array([
    1.4784, 1.5541, 1.5577, 1.5494, 2.1854, 0.5133, 0.2207, 2.3180, 3.0095, 0.4633,
    0.1787, 0.0981, 0.1431, 0.3135, 0.3924, 0.7325, 0.2710, 0.4607, 0.8086, 1.2785,
    1.8950, 0.1069, 0.1495, 0.2842, 0.3930, 0.5446, 0.3613, 0.7777
])
average_inference_speed = np.array([
    1.6587, 1.6301, 1.5914, 1.5767, 2.2694, 0.6020, 0.2838, 2.5506, 3.2622, 0.5432,
    0.2005, 0.8747, 0.8060, 0.9653, 1.1999, 1.6174, 0.8903, 1.2371, 1.7893, 2.3363,
    3.2527, 0.8904, 0.8897, 1.1502, 1.1842, 1.3767, 1.1275, 2.2236
])
worst_inference_speed = np.array([
    2.1443, 1.7094, 1.6651, 1.6262, 2.3628, 0.6728, 0.3705, 2.8379, 3.4295, 0.5932,
    0.2337, 3.8586, 3.3074, 3.5000, 3.9576, 4.6627, 3.2117, 4.2649, 5.1681, 5.8959,
    7.9948, 3.8568, 3.6098, 4.2492, 3.9691, 4.0743, 3.5024, 7.7722
])


import numpy as np
import matplotlib.pyplot as plt
import matplotlib

# Generate a color map
num_models = len(models)
cmap = matplotlib.colormaps.get_cmap('nipy_spectral')
colors = [cmap(i / num_models) for i in range(num_models)]

# Create a figure and plot with unique colors
plt.figure(figsize=(14, 10))
for i, model in enumerate(models):
    plt.scatter(average_speed[i], average_precision[i], color=colors[i], label=model, edgecolor='black')

print("DEVICE: CPU (Intel i7-14700KF)")
print("SOURCE: data/video/vecteezy_car-and-truck-traffic-on-the-highway-in-europe-poland_7957364.MP4 (4k30fps Video)")
print("SKIP EVERY 60 FRAMES")
print("TOTALLY PROCESSED FRAMES: 6")
print("COCO Dataset, Classes truck, bus, car, train, bicycle")
# Axis labels and plot title
plt.xlabel('Average Speed (seconds)')
plt.ylabel('Average Precision (%)')
plt.title('Model Performance: Average Detection Speed x Average Accuracy')
plt.legend(loc='upper left', bbox_to_anchor=(1.05, 1), fontsize='small')
plt.grid(True)
plt.show()

# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_precision, average_precision, worst_precision])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Precision (%)", "Average Precision (%)", "Worst Precision (%)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

plt.title("Model Precision Metrics")
plt.show()

# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_speed, average_speed, worst_speed])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Speed (s)", "Average Speed (s)", "Worst Speed (s)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

plt.title("Model Speed Metrics")
plt.show()

# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_inference_speed, average_inference_speed, worst_inference_speed])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Inference Speed (s)", "Average Inference Speed (s)", "Worst Inference Speed (s)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

plt.title("Model Inference Speed Metrics")
plt.show()


import cv2
import torch
from torchvision.models.detection import *
from torchvision.io.image import read_image
from torchvision.utils import draw_bounding_boxes
from torchvision.transforms.functional import to_pil_image
import torchvision.transforms as transforms
from PIL import Image
import IPython
from IPython.display import display, clear_output
import time
import traceback
import math
from ultralytics import YOLO

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

transform = transforms.Compose([
    #transforms.Resize((300, 300)),
    transforms.ToTensor(),
])

class DetectionModel():
    def __init__(self, concept, backbone, model, weights, reference, color=None):
        self.concept = concept
        self.backbone = backbone
        self.model = model(weights) if concept=="YOLO" else model(weights=weights.DEFAULT)
        self.weights = weights
        self.reference = reference
        self.color = color
        self.detections = []

    def addDetection(self, detection):
        self.detections.append(detection)

    def getDetections(self):
        return self.detections

    def clearDetections(self):
        self.detections = []

    def detectionsCount(self):
        return len(self.detections)

    def inferencesCount(self):
        length = 0
        addedFrames = []
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    addedFrames.append(self.detections[i].frame)
        return len(addedFrames)

    def getAveragePrecision(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].precision
            avg = sum/len(self.detections)
        return  avg.item() if torch.is_tensor(avg) else avg

    def getMaxPrecision(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].precision:
                max=self.detections[i].precision
        return max.item() if torch.is_tensor(max) else max

    def getMinPrecision(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].precision:
                min=self.detections[i].precision
        return 0.0 if min==9999999.99 else min.item() if torch.is_tensor(min) else min

    def getAverageSpeed(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].speed
            avg = sum/len(self.detections)
        return avg

    def getMaxSpeed(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].speed:
                max=self.detections[i].speed
        return max

    def getMinSpeed(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].speed:
                min=self.detections[i].speed
        return 0.0 if min==9999999.99 else min

    def getAverageInferenceSpeed(self):
        sum=0.0
        avg=0.0
        addedFrames=[]
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    sum+=self.detections[i].speed
                    addedFrames.append(self.detections[i].frame)
            avg = sum/len(addedFrames) if len(addedFrames)>0 else 0.0
        return avg

    def getMaxInferenceSpeed(self):
        max=0.0
        addedFrames=[]
        for i in range(len(self.detections)):
            if max<self.detections[i].speed and self.detections[i].frame not in addedFrames:
                max=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return max

    def getMinInferenceSpeed(self):
        min=9999999.99
        addedFrames=[]
        for i in range(len(self.detections)):
            if min>self.detections[i].speed and self.detections[i].frame not in addedFrames:
                min=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return 0.0 if min==9999999.99 else min

    def filterPrecisionsBelow(self, minPrecision):
        newDetections = []
        for i in range(len(self.detections)):
            if self.detections[i].precision>=minPrecision:
                newDetections.append(self.detections[i])
        self.detections = newDetections

    def printResults(self):
        print("Model:", self.reference,
              "\nBest Precision:", self.getMaxPrecision(),
              "%\nAverage Precision:", self.getAveragePrecision(),
              "%\nWorst Precision:", self.getMinPrecision(),
              "%\nBest Speed:", self.getMinSpeed(),
              "seconds\nAverage Speed:", self.getAverageSpeed(),
              "seconds\nWorst Speed:", self.getMaxSpeed(),
              "seconds\nBest Inference Speed:", self.getMinInferenceSpeed(),
              "seconds\nAverage Inference Speed:", self.getAverageInferenceSpeed(),
              "seconds\nWorst Inference Speed:", self.getMaxInferenceSpeed(),
              "seconds\nTotal Detections:", self.detectionsCount(),
              "\nTotal Inferences:", self.inferencesCount(),
              "\n\n")

class DetectionResult():
    def __init__(self, model, label, precision, speed, device=None, source=None, frame=None, boundingBox=None, details=None):
        self.model = model
        self.label = label
        self.precision = precision.item() if torch.is_tensor(precision) else precision
        self.speed = speed.item() if torch.is_tensor(speed) else speed
        self.device = device
        self.source = source
        self.frame = frame
        self.boundingBox = boundingBox
        self.details = details

target_classes = ["truck", "bus", "car", "train", "bicycle"]


VEHICLE_DETECTION_MODELS = [
    DetectionModel("FCOS", "resnet50_fpn", fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights, "fcos_resnet", (255,0,0)),
    DetectionModel("RetinaNet", "resnet50_fpn", retinanet_resnet50_fpn, RetinaNet_ResNet50_FPN_Weights, "retinanet_resnet", (255,255,0)),
    DetectionModel("RetinaNet", "resnet50_fpn_v2", retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights, "retinanet_resnet_v2", (0,234,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn", fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights, "faster_rcnn_resnet", (170,0,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn_v2", fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights, "faster_rcnn_resnet_v2", (255,127,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_fpn", fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights, "faster_rcnn_mobilenet_v3", (191,255,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_320_fpn", fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights, "faster_rcnn_mobilenet_v3_320", (0,149,255)),
    DetectionModel("MaskRCNN", "resnet50_fpn", maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights, "mask_rcnn_resnet", (255,0,170)),
    DetectionModel("MaskRCNN", "resnet50_fpn_v2", maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights, "mask_rcnn_resnet_v2", (255,212,0)),
    DetectionModel("SSD300", "vgg16", ssd300_vgg16, SSD300_VGG16_Weights, "ssd_vgg16", (106,255,0)),
    DetectionModel("SSDLite320", "mobilenet_v3_large", ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights, "ssd_mobilenet_v3", (0,64,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5nu.pt', "yolov5nu", (237,185,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5su.pt', "yolov5su", (185,215,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5mu.pt', "yolov5mu", (231,233,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5lu.pt', "yolov5lu", (220,185,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5xu.pt', "yolov5xu", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5n6u.pt', "yolov5n6u", (143,35,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5s6u.pt', "yolov5s6u", (35,98,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5m6u.pt', "yolov5m6u", (143,106,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5l6u.pt', "yolov5l6u", (107,35,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5x6u.pt', "yolov5x6u", (79,143,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8n.pt', "yolov8n", (0,0,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8s.pt', "yolov8s", (115,115,115)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8m.pt', "yolov8m", (204,204,204)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8l.pt', "yolov8l", (255,0,0)), # cycle restarts
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8x.pt', "yolov8x", (255,255,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9c.pt', "yolov9c", (0,234,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9e.pt', "yolov9e", (170,0,255))
]

def process(frame, frameIndex=-1, source=None):
    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        processed_frame = frame.copy()

        if currentModel.concept=="YOLO":
            start_time = time.time()
            results = currentModel.model(processed_frame, agnostic_nms=True, verbose=False)[0]
            processing_time = time.time() - start_time
            for result in results:
                detection_count = result.boxes.shape[0]
                for j in range(detection_count):
                    cls = int(result.boxes.cls[j].item())
                    name = result.names[cls]
                    confidence = float(result.boxes.conf[j].item())
                    bounding_box = result.boxes.xyxy[j].cpu().numpy()
                    if name in target_classes:
                        x1, y1, x2, y2 = [int(x) for x in bounding_box]
                        # Ensure x1 < x2 and y1 < y2
                        x1, x2 = min(x1, x2), max(x1, x2)
                        y1, y2 = min(y1, y2), max(y1, y2)
                        label = '{} ({:.2f}%) ({:.3f}s) {}'.format(name.upper(), confidence*100, processing_time, currentModel.reference)
                        print(label)
                        VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, name, confidence*100, processing_time, device, source, frameIndex, bounding_box, label))
                        cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                        cv2.putText(processed_frame, label, (x1+10, y1+25), 0, 0.8, currentModel.color, 2)
        else:
            weights = currentModel.weights.DEFAULT
            model = currentModel.model
            model.to(device)
            model.eval()

            # Convert frame to RGB and PIL Image, then apply transformation
            frame_rgb = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            image = transform(pil_image).unsqueeze(0).to(device)

            # Object detection
            start_time = time.time()
            with torch.no_grad():
                predictions = model(image)

            # Processing time
            processing_time = time.time() - start_time

            # Visualization
            scores = predictions[0]['scores']
            boxes = predictions[0]['boxes']
            labels = predictions[0]['labels']
            confidence_threshold = 0.5

            for score, box, label in zip(scores, boxes, labels):
                # and score > confidence_threshold
                if weights.meta["categories"][label.item()] in target_classes:
                    x1, y1, x2, y2 = map(int, box)
                    label_text = f'{weights.meta["categories"][label.item()].upper()} ({score*100:.2f}%) ({processing_time:.3f}s) {currentModel.reference}'
                    print(label_text)
                    VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, weights.meta["categories"][label.item()], float(score.item())*100, processing_time, device, source, frameIndex, box, label_text))
                    cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                    cv2.putText(processed_frame, label_text, (x1, y1+20), 0, 0.8, currentModel.color, 2)

        img = Image.fromarray(processed_frame, 'RGB')
        display(img)
        clear_output(wait=True)
    return frame

def handleFrame(frame, frameIndex=-1, source=None):
    processed_frame = process(frame,frameIndex,source)

def handleRelease():
    cam.release()
    print("Source released.")

    print(device)
    print(source)

    BestPrecision=None
    BestAvgPrecision=None
    WorstPrecision=None
    BestSpeed=None
    BestAvgSpeed=None
    WorstSpeed=None
    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        currentModel.printResults()
        if BestPrecision is None or BestPrecision.getMaxPrecision()<=currentModel.getMaxPrecision():
            BestPrecision = currentModel
        if BestAvgPrecision is None or BestAvgPrecision.getAveragePrecision()<=currentModel.getAveragePrecision():
            BestAvgPrecision = currentModel
        if WorstPrecision is None or WorstPrecision.getMinPrecision()>=currentModel.getMinPrecision():
            WorstPrecision = currentModel
        if BestSpeed is None or BestSpeed.getMinSpeed()>=currentModel.getMinSpeed():
            BestSpeed = currentModel
        if BestAvgSpeed is None or BestAvgSpeed.getAverageSpeed()>=currentModel.getAverageSpeed():
            BestAvgSpeed = currentModel
        if WorstSpeed is None or WorstSpeed.getMaxSpeed()<=currentModel.getMaxSpeed():
            WorstSpeed = currentModel

    if BestPrecision is not None:
        print("Best Precision:")
        BestPrecision.printResults()
    if BestAvgPrecision is not None:
        print("Best Average Precision:")
        BestAvgPrecision.printResults()
    if WorstPrecision is not None:
        print("Worst Precision:")
        WorstPrecision.printResults()
    if BestSpeed is not None:
        print("Best Speed:")
        BestSpeed.printResults()
    if BestAvgSpeed is not None:
        print("Best Average Speed:")
        BestAvgSpeed.printResults()
    if WorstSpeed is not None:
        print("Worst Speed:")
        WorstSpeed.printResults()

inputType="video"
source = "data/video/sample.MP4" if inputType=="video" else 0
cam = cv2.VideoCapture(source)
display_handle=display(None, display_id=True)
every = 60

start = 0 if source!=0 else -1
end = int(cam.get(cv2.CAP_PROP_FRAME_COUNT)) if source!=0 else -1
if source!=0: cam.set(1, start)

frameCount=start
while_safety=0
saved_count=0

if not cam.isOpened(): print("Error: Could not open source.")
else:
    try:
        while True if end<0 else frameCount<end:
            _, frame = cam.read()
            if frame is None:
                if source!=0:
                    if while_safety > 2000: break
                    while_safety += 1
                    continue
                else:
                    print("Error: Could not capture frame.")
                    cam.release()
                    break

            if every>0:
                if (frameCount+1)%math.floor(every) == 0:
                    while_safety = 0
                    handleFrame(frame, frameCount, source)
            else:
                while_safety=0
                handleFrame(frame)
            frameCount += 1
        handleRelease()
    except KeyboardInterrupt:
        handleRelease()
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Source could not be released.")
        except:
            pass
        traceback.print_exc()


Source released.
cuda:0
data/video/sample.MP4
Model: fcos_resnet
Best Precision: 84.01496410369873 %
Average Precision: 33.320979035418965 %
Worst Precision: 20.000271499156952 %
Best Speed: 0.0827782154083252 seconds
Average Speed: 0.2132601224857828 seconds
Worst Speed: 0.9202609062194824 seconds
Best Inference Speed: 0.0827782154083252 seconds
Average Inference Speed: 0.20757309595743814 seconds
Worst Inference Speed: 0.9202609062194824 seconds
Total Detections: 920
Total Inferences: 84


Model: retinanet_resnet
Best Precision: 86.20967268943787 %
Average Precision: 13.884030294133167 %
Worst Precision: 5.000218749046326 %
Best Speed: 0.09275293350219727 seconds
Average Speed: 0.11827594854480628 seconds
Worst Speed: 0.23600435256958008 seconds
Best Inference Speed: 0.09275293350219727 seconds
Average Inference Speed: 0.11878633215313866 seconds
Worst Inference Speed: 0.23600435256958008 seconds
Total Detections: 2017
Total Inferences: 84


Model: retinanet_resnet_v2
Best Precision: 96.10868096351624 %
Average Precision: 15.765904018925468 %
Worst Precision: 5.000488460063934 %
Best Speed: 0.08776545524597168 seconds
Average Speed: 0.10320771109806709 seconds
Worst Speed: 0.14960241317749023 seconds
Best Inference Speed: 0.08776545524597168 seconds
Average Inference Speed: 0.10432871750422887 seconds
Worst Inference Speed: 0.14960241317749023 seconds
Total Detections: 1614
Total Inferences: 84


Model: faster_rcnn_resnet
Best Precision: 99.2474615573883 %
Average Precision: 33.835676844348974 %
Worst Precision: 5.061367526650429 %
Best Speed: 0.08377480506896973 seconds
Average Speed: 0.09352583190463237 seconds
Worst Speed: 0.17817139625549316 seconds
Best Inference Speed: 0.08377480506896973 seconds
Average Inference Speed: 0.0943033979052589 seconds
Worst Inference Speed: 0.17817139625549316 seconds
Total Detections: 755
Total Inferences: 84


Model: faster_rcnn_resnet_v2
Best Precision: 99.60416555404663 %
Average Precision: 33.03066442840906 %
Worst Precision: 5.013740062713623 %
Best Speed: 0.10870885848999023 seconds
Average Speed: 0.11575107597278066 seconds
Worst Speed: 0.16954660415649414 seconds
Best Inference Speed: 0.10870885848999023 seconds
Average Inference Speed: 0.11539448442913237 seconds
Worst Inference Speed: 0.16954660415649414 seconds
Total Detections: 836
Total Inferences: 84


Model: faster_rcnn_mobilenet_v3
Best Precision: 99.35287833213806 %
Average Precision: 37.715451977191826 %
Worst Precision: 5.046916380524635 %
Best Speed: 0.02094292640686035 seconds
Average Speed: 0.02885772631718562 seconds
Worst Speed: 0.10901284217834473 seconds
Best Inference Speed: 0.02094292640686035 seconds
Average Inference Speed: 0.029697951816377185 seconds
Worst Inference Speed: 0.10901284217834473 seconds
Total Detections: 390
Total Inferences: 84


Model: faster_rcnn_mobilenet_v3_320
Best Precision: 98.90453815460205 %
Average Precision: 38.08975459535917 %
Worst Precision: 5.015108734369278 %
Best Speed: 0.018947839736938477 seconds
Average Speed: 0.024051254908243815 seconds
Worst Speed: 0.05285978317260742 seconds
Best Inference Speed: 0.018947839736938477 seconds
Average Inference Speed: 0.024423494225456602 seconds
Worst Inference Speed: 0.05285978317260742 seconds
Total Detections: 375
Total Inferences: 84


Model: mask_rcnn_resnet
Best Precision: 98.71110916137695 %
Average Precision: 32.75369045274182 %
Worst Precision: 5.009877309203148 %
Best Speed: 0.09973382949829102 seconds
Average Speed: 0.11404819816075354 seconds
Worst Speed: 0.1436169147491455 seconds
Best Inference Speed: 0.09973382949829102 seconds
Average Inference Speed: 0.11229550270807176 seconds
Worst Inference Speed: 0.1436169147491455 seconds
Total Detections: 757
Total Inferences: 84


Model: mask_rcnn_resnet_v2
Best Precision: 99.41350817680359 %
Average Precision: 37.252656026661306 %
Worst Precision: 5.005602538585663 %
Best Speed: 0.12566304206848145 seconds
Average Speed: 0.1425610487759574 seconds
Worst Speed: 0.18849706649780273 seconds
Best Inference Speed: 0.12566304206848145 seconds
Average Inference Speed: 0.14216835725875127 seconds
Worst Inference Speed: 0.18849706649780273 seconds
Total Detections: 647
Total Inferences: 84


Model: ssd_vgg16
Best Precision: 95.25400996208191 %
Average Precision: 6.433261671535882 %
Worst Precision: 2.1978026255965233 %
Best Speed: 0.039888620376586914 seconds
Average Speed: 0.050030622773876605 seconds
Worst Speed: 0.09773945808410645 seconds
Best Inference Speed: 0.039888620376586914 seconds
Average Inference Speed: 0.049783624353862944 seconds
Worst Inference Speed: 0.09773945808410645 seconds
Total Detections: 4808
Total Inferences: 84


Model: ssd_mobilenet_v3
Best Precision: 97.86270260810852 %
Average Precision: 3.645043508510231 %
Worst Precision: 0.7162530906498432 %
Best Speed: 0.04190540313720703 seconds
Average Speed: 0.07151326498478655 seconds
Worst Speed: 0.13720297813415527 seconds
Best Inference Speed: 0.04190540313720703 seconds
Average Inference Speed: 0.072170737243834 seconds
Worst Inference Speed: 0.13720297813415527 seconds
Total Detections: 7633
Total Inferences: 84


Model: yolov5nu
Best Precision: 89.5509660243988 %
Average Precision: 59.80521331139661 %
Worst Precision: 25.001904368400574 %
Best Speed: 0.010967493057250977 seconds
Average Speed: 0.014186200745608828 seconds
Worst Speed: 0.2256937026977539 seconds
Best Inference Speed: 0.010967493057250977 seconds
Average Inference Speed: 0.014808717980442277 seconds
Worst Inference Speed: 0.2256937026977539 seconds
Total Detections: 109
Total Inferences: 83


Model: yolov5su
Best Precision: 94.16013956069946 %
Average Precision: 65.16339865703027 %
Worst Precision: 28.9579838514328 %
Best Speed: 0.01095128059387207 seconds
Average Speed: 0.014040159947663835 seconds
Worst Speed: 0.2124335765838623 seconds
Best Inference Speed: 0.01095128059387207 seconds
Average Inference Speed: 0.014484831265040807 seconds
Worst Inference Speed: 0.2124335765838623 seconds
Total Detections: 103
Total Inferences: 84


Model: yolov5mu
Best Precision: 95.83573341369629 %
Average Precision: 62.61704099732776 %
Worst Precision: 25.004157423973083 %
Best Speed: 0.0139617919921875 seconds
Average Speed: 0.02057248307752979 seconds
Worst Speed: 0.362032413482666 seconds
Best Inference Speed: 0.0139617919921875 seconds
Average Inference Speed: 0.02213143167041597 seconds
Worst Inference Speed: 0.362032413482666 seconds
Total Detections: 129
Total Inferences: 84


Model: yolov5lu
Best Precision: 95.45990824699402 %
Average Precision: 73.18179570138454 %
Worst Precision: 25.045400857925415 %
Best Speed: 0.01994609832763672 seconds
Average Speed: 0.03423878976276943 seconds
Worst Speed: 0.7081074714660645 seconds
Best Inference Speed: 0.01994609832763672 seconds
Average Inference Speed: 0.0359394976070949 seconds
Worst Inference Speed: 0.7081074714660645 seconds
Total Detections: 112
Total Inferences: 84


Model: yolov5xu
Best Precision: 96.57882452011108 %
Average Precision: 75.30655244301106 %
Worst Precision: 25.6293386220932 %
Best Speed: 0.03191113471984863 seconds
Average Speed: 0.05898017924407433 seconds
Worst Speed: 0.8936121463775635 seconds
Best Inference Speed: 0.03191113471984863 seconds
Average Inference Speed: 0.06118170420328776 seconds
Worst Inference Speed: 0.8936121463775635 seconds
Total Detections: 116
Total Inferences: 84


Model: yolov5n6u
Best Precision: 89.94280099868774 %
Average Precision: 63.64890647303198 %
Worst Precision: 25.590893626213074 %
Best Speed: 0.019944429397583008 seconds
Average Speed: 0.024922444648349407 seconds
Worst Speed: 0.2094407081604004 seconds
Best Inference Speed: 0.019944429397583008 seconds
Average Inference Speed: 0.02520587898436047 seconds
Worst Inference Speed: 0.2094407081604004 seconds
Total Detections: 97
Total Inferences: 84


Model: yolov5s6u
Best Precision: 93.81154775619507 %
Average Precision: 68.91446464484737 %
Worst Precision: 25.863370299339294 %
Best Speed: 0.03091573715209961 seconds
Average Speed: 0.036208794962975285 seconds
Worst Speed: 0.043883562088012695 seconds
Best Inference Speed: 0.03091573715209961 seconds
Average Inference Speed: 0.03614339771040951 seconds
Worst Inference Speed: 0.043883562088012695 seconds
Total Detections: 124
Total Inferences: 83


Model: yolov5m6u
Best Precision: 96.59475088119507 %
Average Precision: 74.3742719536922 %
Worst Precision: 27.02006697654724 %
Best Speed: 0.06282997131347656 seconds
Average Speed: 0.07724395540894055 seconds
Worst Speed: 0.09674239158630371 seconds
Best Inference Speed: 0.06282997131347656 seconds
Average Inference Speed: 0.07645792271717484 seconds
Worst Inference Speed: 0.09674239158630371 seconds
Total Detections: 122
Total Inferences: 83


Model: yolov5l6u
Best Precision: 96.98339700698853 %
Average Precision: 76.79305465952042 %
Worst Precision: 25.585880875587463 %
Best Speed: 0.10471796989440918 seconds
Average Speed: 0.12722381276469077 seconds
Worst Speed: 0.16156792640686035 seconds
Best Inference Speed: 0.10471796989440918 seconds
Average Inference Speed: 0.12651456856146093 seconds
Worst Inference Speed: 0.16156792640686035 seconds
Total Detections: 124
Total Inferences: 82


Model: yolov5x6u
Best Precision: 96.82453870773315 %
Average Precision: 77.06775167813667 %
Worst Precision: 25.649303197860718 %
Best Speed: 0.13065052032470703 seconds
Average Speed: 0.1853115155146672 seconds
Worst Speed: 1.5039780139923096 seconds
Best Inference Speed: 0.13065052032470703 seconds
Average Inference Speed: 0.18957576297578357 seconds
Worst Inference Speed: 1.5039780139923096 seconds
Total Detections: 130
Total Inferences: 84


Model: yolov8n
Best Precision: 89.8779571056366 %
Average Precision: 64.94164078524618 %
Worst Precision: 25.362345576286316 %
Best Speed: 0.009966850280761719 seconds
Average Speed: 0.012542327245076498 seconds
Worst Speed: 0.13962531089782715 seconds
Best Inference Speed: 0.009966850280761719 seconds
Average Inference Speed: 0.012787401676177979 seconds
Worst Inference Speed: 0.13962531089782715 seconds
Total Detections: 99
Total Inferences: 84


Model: yolov8s
Best Precision: 96.51696681976318 %
Average Precision: 69.82135244850384 %
Worst Precision: 25.332868099212646 %
Best Speed: 0.00996708869934082 seconds
Average Speed: 0.013092942157034146 seconds
Worst Speed: 0.16954684257507324 seconds
Best Inference Speed: 0.00996708869934082 seconds
Average Inference Speed: 0.013726186184656052 seconds
Worst Inference Speed: 0.16954684257507324 seconds
Total Detections: 118
Total Inferences: 84


Model: yolov8m
Best Precision: 96.09609842300415 %
Average Precision: 75.264937850562 %
Worst Precision: 25.736242532730103 %
Best Speed: 0.014957904815673828 seconds
Average Speed: 0.02133357741615989 seconds
Worst Speed: 0.29720497131347656 seconds
Best Inference Speed: 0.014957904815673828 seconds
Average Inference Speed: 0.02210736842382522 seconds
Worst Inference Speed: 0.29720497131347656 seconds
Total Detections: 110
Total Inferences: 84


Model: yolov8l
Best Precision: 96.97031378746033 %
Average Precision: 75.66015367445193 %
Worst Precision: 26.12241506576538 %
Best Speed: 0.020935535430908203 seconds
Average Speed: 0.029911020345855178 seconds
Worst Speed: 0.44281625747680664 seconds
Best Inference Speed: 0.020935535430908203 seconds
Average Inference Speed: 0.031213942028227307 seconds
Worst Inference Speed: 0.44281625747680664 seconds
Total Detections: 114
Total Inferences: 84


Model: yolov8x
Best Precision: 96.952885389328 %
Average Precision: 82.63096769113798 %
Worst Precision: 26.093533635139465 %
Best Speed: 0.028922080993652344 seconds
Average Speed: 0.04285835360621547 seconds
Worst Speed: 0.6582400798797607 seconds
Best Inference Speed: 0.028922080993652344 seconds
Average Inference Speed: 0.04512909764335269 seconds
Worst Inference Speed: 0.6582400798797607 seconds
Total Detections: 111
Total Inferences: 84


Model: yolov9c
Best Precision: 95.78875303268433 %
Average Precision: 75.2313807211091 %
Worst Precision: 25.400808453559875 %
Best Speed: 0.018947124481201172 seconds
Average Speed: 0.0261284017984846 seconds
Worst Speed: 0.3795278072357178 seconds
Best Inference Speed: 0.018947124481201172 seconds
Average Inference Speed: 0.027051914305914016 seconds
Worst Inference Speed: 0.3795278072357178 seconds
Total Detections: 113
Total Inferences: 84


Model: yolov9e
Best Precision: 96.19094133377075 %
Average Precision: 76.31395133428795 %
Worst Precision: 25.9141743183136 %
Best Speed: 0.030917644500732422 seconds
Average Speed: 0.046473571496416434 seconds
Worst Speed: 0.7124040126800537 seconds
Best Inference Speed: 0.030917644500732422 seconds
Average Inference Speed: 0.04880040884017944 seconds
Worst Inference Speed: 0.7124040126800537 seconds
Total Detections: 129
Total Inferences: 84


Best Precision:
Model: faster_rcnn_resnet_v2
Best Precision: 99.60416555404663 %
Average Precision: 33.03066442840906 %
Worst Precision: 5.013740062713623 %
Best Speed: 0.10870885848999023 seconds
Average Speed: 0.11575107597278066 seconds
Worst Speed: 0.16954660415649414 seconds
Best Inference Speed: 0.10870885848999023 seconds
Average Inference Speed: 0.11539448442913237 seconds
Worst Inference Speed: 0.16954660415649414 seconds
Total Detections: 836
Total Inferences: 84


Best Average Precision:
Model: yolov8x
Best Precision: 96.952885389328 %
Average Precision: 82.63096769113798 %
Worst Precision: 26.093533635139465 %
Best Speed: 0.028922080993652344 seconds
Average Speed: 0.04285835360621547 seconds
Worst Speed: 0.6582400798797607 seconds
Best Inference Speed: 0.028922080993652344 seconds
Average Inference Speed: 0.04512909764335269 seconds
Worst Inference Speed: 0.6582400798797607 seconds
Total Detections: 111
Total Inferences: 84


Worst Precision:
Model: ssd_mobilenet_v3
Best Precision: 97.86270260810852 %
Average Precision: 3.645043508510231 %
Worst Precision: 0.7162530906498432 %
Best Speed: 0.04190540313720703 seconds
Average Speed: 0.07151326498478655 seconds
Worst Speed: 0.13720297813415527 seconds
Best Inference Speed: 0.04190540313720703 seconds
Average Inference Speed: 0.072170737243834 seconds
Worst Inference Speed: 0.13720297813415527 seconds
Total Detections: 7633
Total Inferences: 84


Best Speed:
Model: yolov8n
Best Precision: 89.8779571056366 %
Average Precision: 64.94164078524618 %
Worst Precision: 25.362345576286316 %
Best Speed: 0.009966850280761719 seconds
Average Speed: 0.012542327245076498 seconds
Worst Speed: 0.13962531089782715 seconds
Best Inference Speed: 0.009966850280761719 seconds
Average Inference Speed: 0.012787401676177979 seconds
Worst Inference Speed: 0.13962531089782715 seconds
Total Detections: 99
Total Inferences: 84


Best Average Speed:
Model: yolov8n
Best Precision: 89.8779571056366 %
Average Precision: 64.94164078524618 %
Worst Precision: 25.362345576286316 %
Best Speed: 0.009966850280761719 seconds
Average Speed: 0.012542327245076498 seconds
Worst Speed: 0.13962531089782715 seconds
Best Inference Speed: 0.009966850280761719 seconds
Average Inference Speed: 0.012787401676177979 seconds
Worst Inference Speed: 0.13962531089782715 seconds
Total Detections: 99
Total Inferences: 84


Worst Speed:
Model: yolov5x6u
Best Precision: 96.82453870773315 %
Average Precision: 77.06775167813667 %
Worst Precision: 25.649303197860718 %
Best Speed: 0.13065052032470703 seconds
Average Speed: 0.1853115155146672 seconds
Worst Speed: 1.5039780139923096 seconds
Best Inference Speed: 0.13065052032470703 seconds
Average Inference Speed: 0.18957576297578357 seconds
Worst Inference Speed: 1.5039780139923096 seconds
Total Detections: 130
Total Inferences: 84


import numpy as np

# Model names
models = [
    "fcos_resnet", "retinanet_resnet", "retinanet_resnet_v2", "faster_rcnn_resnet",
    "faster_rcnn_resnet_v2", "faster_rcnn_mobilenet_v3", "faster_rcnn_mobilenet_v3_320",
    "mask_rcnn_resnet", "mask_rcnn_resnet_v2", "ssd_vgg16", "ssd_mobilenet_v3",
    "yolov5nu", "yolov5su", "yolov5mu", "yolov5lu", "yolov5xu", "yolov5n6u", "yolov5s6u",
    "yolov5m6u", "yolov5l6u", "yolov5x6u", "yolov8n", "yolov8s", "yolov8m", "yolov8l",
    "yolov8x", "yolov9c", "yolov9e"
]

# Precision metrics
best_precision = np.array([
    84.014964, 86.209673, 96.108681, 99.247462, 99.604166, 99.352878, 98.904538, 98.711109, 99.413508,
    95.254010, 97.862703, 89.550966, 94.160140, 95.835733, 95.459908, 96.578825, 89.942801, 93.811548,
    96.594751, 96.983397, 96.824539, 89.877957, 96.516967, 96.096098, 96.970314, 96.952885, 95.788753,
    96.190941
])
average_precision = np.array([
    33.320979, 13.884030, 15.765904, 33.835677, 33.030664, 37.715452, 38.089755, 32.753690, 37.252656,
    6.433262, 3.645044, 59.805213, 65.163399, 62.617041, 73.181796, 75.306552, 63.648906, 68.914465,
    74.374272, 76.793055, 77.067752, 64.941641, 69.821352, 75.264938, 75.660154, 82.630968, 75.231381,
    76.313951
])
worst_precision = np.array([
    20.000271, 5.000219, 5.000488, 5.061368, 5.013740, 5.046916, 5.015109, 5.009877, 5.005603,
    2.197803, 0.716253, 25.001904, 28.957984, 25.004157, 25.045401, 25.629339, 25.590894, 25.863370,
    27.020067, 25.585881, 25.649303, 25.362346, 25.332868, 25.736243, 26.122415, 26.093534, 25.400808,
    25.914174
])

# Speed metrics converted to milliseconds
best_speed_ms = np.array([
    82.778215, 92.752934, 87.765455, 83.774805, 108.708858, 20.942926, 18.947840, 99.733829, 125.663042,
    39.888620, 41.905403, 10.967493, 10.951281, 13.961792, 19.946098, 31.911135, 19.944429, 30.915737,
    62.829971, 104.717970, 130.650520, 9.966850, 9.967089, 14.957905, 20.935535, 28.922081, 18.947124,
    30.917645
])
average_speed_ms = np.array([
    213.260122, 118.275949, 103.207711, 93.525832, 115.751076, 28.857726, 24.051255, 114.048198, 142.561049,
    50.030623, 71.513265, 14.186201, 14.040160, 20.572483, 34.238790, 58.980179, 24.922445, 36.208795,
    77.243955, 127.223813, 185.311516, 12.542327, 13.092942, 21.333577, 29.911020, 42.858354, 26.128402,
    46.473571
])
worst_speed_ms = np.array([
    920.260906, 236.004353, 149.602413, 178.171396, 169.546604, 109.012842, 52.859783, 143.616915, 188.497067,
    97.739458, 137.202978, 225.693703, 212.433577, 362.032413, 708.107471, 893.612147, 209.440708, 43.883562,
    96.742392, 161.567927, 1503.978014, 139.625311, 169.546843, 297.204971, 442.816258, 658.240080, 379.527807,
    712.404013
])

# Inference Speed metrics converted to milliseconds
best_inference_speed_ms = np.array([
    82.778215, 92.752934, 87.765455, 83.774805, 108.708858, 20.942926, 18.947840, 99.733829, 125.663042,
    39.888620, 41.905403, 10.967493, 10.951281, 13.961792, 19.946098, 31.911135, 19.944429, 30.915737,
    62.829971, 104.717970, 130.650520, 9.966850, 9.967089, 14.957905, 20.935535, 28.922081, 18.947124,
    30.917645
])
average_inference_speed_ms = np.array([
    207.573096, 118.786332, 104.328718, 94.303398, 115.394484, 29.697952, 24.423494, 112.295503, 142.168357,
    49.783624, 72.170737, 14.808718, 14.484831, 22.131432, 35.939498, 61.181704, 25.205879, 36.143398,
    76.457923, 126.514569, 189.575763, 12.787402, 13.726186, 22.107368, 31.213942, 45.129098, 27.051914,
    48.800409
])
worst_inference_speed_ms = np.array([
    920.260906, 236.004353, 149.602413, 178.171396, 169.546604, 109.012842, 52.859783, 143.616915, 188.497067,
    97.739458, 137.202978, 225.693703, 212.433577, 362.032413, 708.107471, 893.612147, 209.440708, 43.883562,
    96.742392, 161.567927, 1503.978014, 139.625311, 169.546843, 297.204971, 442.816258, 658.240080, 379.527807,
    712.404013
])


import numpy as np
import matplotlib.pyplot as plt
import matplotlib

# Generate a color map
num_models = len(models)
cmap = matplotlib.colormaps.get_cmap('nipy_spectral')
colors = [cmap(i / num_models) for i in range(num_models)]

# Create a figure and plot with unique colors
plt.figure(figsize=(14, 10))
for i, model in enumerate(models):
    plt.scatter(average_speed[i], average_precision[i], color=colors[i], label=model, edgecolor='black')

print("DEVICE: GPU (NVidia RTX 2070S Mobile)")
print("SOURCE: data/video/sample.MP4 (1080p59.94fps Video)")
print("SKIP EVERY 60 FRAMES")
print("TOTALLY PROCESSED FRAMES: 84")
print("COCO Dataset, Classes truck, bus, car, train, bicycle")
# Axis labels and plot title
plt.xlabel('Average Speed (seconds)')
plt.ylabel('Average Precision (%)')
plt.title('Model Performance: Average Detection Speed x Average Accuracy')
plt.legend(loc='upper left', bbox_to_anchor=(1.05, 1), fontsize='small')
plt.grid(True)
plt.show()

# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_precision, average_precision, worst_precision])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Precision (%)", "Average Precision (%)", "Worst Precision (%)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

plt.title("Model Precision Metrics")
plt.show()

# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_speed_ms, average_speed_ms, worst_speed_ms])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Speed (ms)", "Average Speed (ms)", "Worst Speed (ms)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

plt.title("Model Speed Metrics")
plt.show()

# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_inference_speed_ms, average_inference_speed_ms, worst_inference_speed_ms])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Inference Speed (ms)", "Average Inference Speed (ms)", "Worst Inference Speed (mss)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

plt.title("Model Inference Speed Metrics")
plt.show()


import cv2
import torch
from torchvision.models.detection import *
from torchvision.io.image import read_image
from torchvision.utils import draw_bounding_boxes
from torchvision.transforms.functional import to_pil_image
import torchvision.transforms as transforms
from PIL import Image
import IPython
from IPython.display import display, clear_output
import time
import traceback
import math
from ultralytics import YOLO

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

transform = transforms.Compose([
    #transforms.Resize((300, 300)),
    transforms.ToTensor(),
])

class DetectionModel():
    def __init__(self, concept, backbone, model, weights, reference, color=None):
        self.concept = concept
        self.backbone = backbone
        self.model = model(weights) if concept=="YOLO" else model(weights=weights.DEFAULT)
        self.weights = weights
        self.reference = reference
        self.color = color
        self.detections = []

    def addDetection(self, detection):
        self.detections.append(detection)

    def getDetections(self):
        return self.detections

    def clearDetections(self):
        self.detections = []

    def detectionsCount(self):
        return len(self.detections)

    def inferencesCount(self):
        length = 0
        addedFrames = []
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    addedFrames.append(self.detections[i].frame)
        return len(addedFrames)

    def getAveragePrecision(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].precision
            avg = sum/len(self.detections)
        return  avg.item() if torch.is_tensor(avg) else avg

    def getMaxPrecision(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].precision:
                max=self.detections[i].precision
        return max.item() if torch.is_tensor(max) else max

    def getMinPrecision(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].precision:
                min=self.detections[i].precision
        return 0.0 if min==9999999.99 else min.item() if torch.is_tensor(min) else min

    def getAverageSpeed(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].speed
            avg = sum/len(self.detections)
        return avg

    def getMaxSpeed(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].speed:
                max=self.detections[i].speed
        return max

    def getMinSpeed(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].speed:
                min=self.detections[i].speed
        return 0.0 if min==9999999.99 else min

    def getAverageInferenceSpeed(self):
        sum=0.0
        avg=0.0
        addedFrames=[]
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    sum+=self.detections[i].speed
                    addedFrames.append(self.detections[i].frame)
            avg = sum/len(addedFrames) if len(addedFrames)>0 else 0.0
        return avg

    def getMaxInferenceSpeed(self):
        max=0.0
        addedFrames=[]
        for i in range(len(self.detections)):
            if max<self.detections[i].speed and self.detections[i].frame not in addedFrames:
                max=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return max

    def getMinInferenceSpeed(self):
        min=9999999.99
        addedFrames=[]
        for i in range(len(self.detections)):
            if min>self.detections[i].speed and self.detections[i].frame not in addedFrames:
                min=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return 0.0 if min==9999999.99 else min

    def filterPrecisionsBelow(self, minPrecision):
        newDetections = []
        for i in range(len(self.detections)):
            if self.detections[i].precision>=minPrecision:
                newDetections.append(self.detections[i])
        self.detections = newDetections

    def printResults(self):
        print("Model:", self.reference,
              "\nBest Precision:", self.getMaxPrecision(),
              "%\nAverage Precision:", self.getAveragePrecision(),
              "%\nWorst Precision:", self.getMinPrecision(),
              "%\nBest Speed:", self.getMinSpeed(),
              "seconds\nAverage Speed:", self.getAverageSpeed(),
              "seconds\nWorst Speed:", self.getMaxSpeed(),
              "seconds\nBest Inference Speed:", self.getMinInferenceSpeed(),
              "seconds\nAverage Inference Speed:", self.getAverageInferenceSpeed(),
              "seconds\nWorst Inference Speed:", self.getMaxInferenceSpeed(),
              "seconds\nTotal Detections:", self.detectionsCount(),
              "\nTotal Inferences:", self.inferencesCount(),
              "\n\n")

class DetectionResult():
    def __init__(self, model, label, precision, speed, device=None, source=None, frame=None, boundingBox=None, details=None):
        self.model = model
        self.label = label
        self.precision = precision.item() if torch.is_tensor(precision) else precision
        self.speed = speed.item() if torch.is_tensor(speed) else speed
        self.device = device
        self.source = source
        self.frame = frame
        self.boundingBox = boundingBox
        self.details = details

target_classes = ["truck", "bus", "car", "train", "bicycle"]


VEHICLE_DETECTION_MODELS = [
    DetectionModel("FCOS", "resnet50_fpn", fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights, "fcos_resnet", (255,0,0)),
    DetectionModel("RetinaNet", "resnet50_fpn", retinanet_resnet50_fpn, RetinaNet_ResNet50_FPN_Weights, "retinanet_resnet", (255,255,0)),
    DetectionModel("RetinaNet", "resnet50_fpn_v2", retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights, "retinanet_resnet_v2", (0,234,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn", fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights, "faster_rcnn_resnet", (170,0,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn_v2", fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights, "faster_rcnn_resnet_v2", (255,127,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_fpn", fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights, "faster_rcnn_mobilenet_v3", (191,255,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_320_fpn", fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights, "faster_rcnn_mobilenet_v3_320", (0,149,255)),
    DetectionModel("MaskRCNN", "resnet50_fpn", maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights, "mask_rcnn_resnet", (255,0,170)),
    DetectionModel("MaskRCNN", "resnet50_fpn_v2", maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights, "mask_rcnn_resnet_v2", (255,212,0)),
    DetectionModel("SSD300", "vgg16", ssd300_vgg16, SSD300_VGG16_Weights, "ssd_vgg16", (106,255,0)),
    DetectionModel("SSDLite320", "mobilenet_v3_large", ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights, "ssd_mobilenet_v3", (0,64,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5nu.pt', "yolov5nu", (237,185,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5su.pt', "yolov5su", (185,215,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5mu.pt', "yolov5mu", (231,233,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5lu.pt', "yolov5lu", (220,185,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5xu.pt', "yolov5xu", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5n6u.pt', "yolov5n6u", (143,35,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5s6u.pt', "yolov5s6u", (35,98,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5m6u.pt', "yolov5m6u", (143,106,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5l6u.pt', "yolov5l6u", (107,35,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5x6u.pt', "yolov5x6u", (79,143,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8n.pt', "yolov8n", (0,0,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8s.pt', "yolov8s", (115,115,115)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8m.pt', "yolov8m", (204,204,204)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8l.pt', "yolov8l", (255,0,0)), # cycle restarts
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8x.pt', "yolov8x", (255,255,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9c.pt', "yolov9c", (0,234,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9e.pt', "yolov9e", (170,0,255))
]

def process(frame, frameIndex=-1, source=None):
    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        processed_frame = frame.copy()

        if currentModel.concept=="YOLO":
            start_time = time.time()
            results = currentModel.model(processed_frame, agnostic_nms=True, verbose=False)[0]
            processing_time = time.time() - start_time
            for result in results:
                detection_count = result.boxes.shape[0]
                for j in range(detection_count):
                    cls = int(result.boxes.cls[j].item())
                    name = result.names[cls]
                    confidence = float(result.boxes.conf[j].item())
                    bounding_box = result.boxes.xyxy[j].cpu().numpy()
                    if name in target_classes:
                        x1, y1, x2, y2 = [int(x) for x in bounding_box]
                        # Ensure x1 < x2 and y1 < y2
                        x1, x2 = min(x1, x2), max(x1, x2)
                        y1, y2 = min(y1, y2), max(y1, y2)
                        label = '{} ({:.2f}%) ({:.3f}s) {}'.format(name.upper(), confidence*100, processing_time, currentModel.reference)
                        print(label)
                        VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, name, confidence*100, processing_time, device, source, frameIndex, bounding_box, label))
                        cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                        cv2.putText(processed_frame, label, (x1+10, y1+25), 0, 0.8, currentModel.color, 2)
        else:
            weights = currentModel.weights.DEFAULT
            model = currentModel.model
            model.to(device)
            model.eval()

            # Convert frame to RGB and PIL Image, then apply transformation
            frame_rgb = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            image = transform(pil_image).unsqueeze(0).to(device)

            # Object detection
            start_time = time.time()
            with torch.no_grad():
                predictions = model(image)

            # Processing time
            processing_time = time.time() - start_time

            # Visualization
            scores = predictions[0]['scores']
            boxes = predictions[0]['boxes']
            labels = predictions[0]['labels']
            confidence_threshold = 0.5

            for score, box, label in zip(scores, boxes, labels):
                # and score > confidence_threshold
                if weights.meta["categories"][label.item()] in target_classes:
                    x1, y1, x2, y2 = map(int, box)
                    label_text = f'{weights.meta["categories"][label.item()].upper()} ({score*100:.2f}%) ({processing_time:.3f}s) {currentModel.reference}'
                    print(label_text)
                    VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, weights.meta["categories"][label.item()], float(score.item())*100, processing_time, device, source, frameIndex, box, label_text))
                    cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                    cv2.putText(processed_frame, label_text, (x1, y1+20), 0, 0.8, currentModel.color, 2)

        img = Image.fromarray(processed_frame, 'RGB')
        display(img)
        clear_output(wait=True)
    return frame

def handleFrame(frame, frameIndex=-1, source=None):
    processed_frame = process(frame,frameIndex,source)

def handleRelease():
    cam.release()
    print("Source released.")

    print(device)
    print(source)

    BestPrecision=None
    BestAvgPrecision=None
    WorstPrecision=None
    BestSpeed=None
    BestAvgSpeed=None
    WorstSpeed=None
    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        currentModel.printResults()
        if BestPrecision is None or BestPrecision.getMaxPrecision()<=currentModel.getMaxPrecision():
            BestPrecision = currentModel
        if BestAvgPrecision is None or BestAvgPrecision.getAveragePrecision()<=currentModel.getAveragePrecision():
            BestAvgPrecision = currentModel
        if WorstPrecision is None or WorstPrecision.getMinPrecision()>=currentModel.getMinPrecision():
            WorstPrecision = currentModel
        if BestSpeed is None or BestSpeed.getMinSpeed()>=currentModel.getMinSpeed():
            BestSpeed = currentModel
        if BestAvgSpeed is None or BestAvgSpeed.getAverageSpeed()>=currentModel.getAverageSpeed():
            BestAvgSpeed = currentModel
        if WorstSpeed is None or WorstSpeed.getMaxSpeed()<=currentModel.getMaxSpeed():
            WorstSpeed = currentModel

    if BestPrecision is not None:
        print("Best Precision:")
        BestPrecision.printResults()
    if BestAvgPrecision is not None:
        print("Best Average Precision:")
        BestAvgPrecision.printResults()
    if WorstPrecision is not None:
        print("Worst Precision:")
        WorstPrecision.printResults()
    if BestSpeed is not None:
        print("Best Speed:")
        BestSpeed.printResults()
    if BestAvgSpeed is not None:
        print("Best Average Speed:")
        BestAvgSpeed.printResults()
    if WorstSpeed is not None:
        print("Worst Speed:")
        WorstSpeed.printResults()

inputType="video"
source = "data/video/vecteezy_car-and-truck-traffic-on-the-highway-in-europe-poland_7957364.MP4" if inputType=="video" else 0
cam = cv2.VideoCapture(source)
display_handle=display(None, display_id=True)
every = 60

start = 0 if source!=0 else -1
end = int(cam.get(cv2.CAP_PROP_FRAME_COUNT)) if source!=0 else -1
if source!=0: cam.set(1, start)

frameCount=start
while_safety=0
saved_count=0

if not cam.isOpened(): print("Error: Could not open source.")
else:
    try:
        while True if end<0 else frameCount<end:
            _, frame = cam.read()
            if frame is None:
                if source!=0:
                    if while_safety > 2000: break
                    while_safety += 1
                    continue
                else:
                    print("Error: Could not capture frame.")
                    cam.release()
                    break

            if every>0:
                if (frameCount+1)%math.floor(every) == 0:
                    while_safety = 0
                    handleFrame(frame, frameCount, source)
            else:
                while_safety=0
                handleFrame(frame)
            frameCount += 1
        handleRelease()
    except KeyboardInterrupt:
        handleRelease()
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Source could not be released.")
        except:
            pass
        traceback.print_exc()


Source released.
cuda:0
data/video/vecteezy_car-and-truck-traffic-on-the-highway-in-europe-poland_7957364.MP4
Model: fcos_resnet
Best Precision: 88.38133811950684 %
Average Precision: 41.43324782981456 %
Worst Precision: 25.272265076637268 %
Best Speed: 0.2952122688293457 seconds
Average Speed: 0.370031426253828 seconds
Worst Speed: 0.42985105514526367 seconds
Best Inference Speed: 0.2952122688293457 seconds
Average Inference Speed: 0.3696131229400635 seconds
Worst Inference Speed: 0.42985105514526367 seconds
Total Detections: 412
Total Inferences: 5


Model: retinanet_resnet
Best Precision: 94.31945085525513 %
Average Precision: 26.668557842161686 %
Worst Precision: 5.019444599747658 %
Best Speed: 0.1156916618347168 seconds
Average Speed: 0.17480310631507393 seconds
Worst Speed: 0.21342921257019043 seconds
Best Inference Speed: 0.1156916618347168 seconds
Average Inference Speed: 0.17453336715698242 seconds
Worst Inference Speed: 0.21342921257019043 seconds
Total Detections: 1076
Total Inferences: 5


Model: retinanet_resnet_v2
Best Precision: 94.65104341506958 %
Average Precision: 28.26277765279433 %
Worst Precision: 5.091703310608864 %
Best Speed: 0.0937507152557373 seconds
Average Speed: 0.15874664323164686 seconds
Worst Speed: 0.20844364166259766 seconds
Best Inference Speed: 0.0937507152557373 seconds
Average Inference Speed: 0.16236743927001954 seconds
Worst Inference Speed: 0.20844364166259766 seconds
Total Detections: 808
Total Inferences: 5


Model: faster_rcnn_resnet
Best Precision: 99.81282353401184 %
Average Precision: 34.53327115616846 %
Worst Precision: 5.013693869113922 %
Best Speed: 0.10273075103759766 seconds
Average Speed: 0.15444356827003122 seconds
Worst Speed: 0.18550419807434082 seconds
Best Inference Speed: 0.10273075103759766 seconds
Average Inference Speed: 0.15518631935119628 seconds
Worst Inference Speed: 0.18550419807434082 seconds
Total Detections: 449
Total Inferences: 5


Model: faster_rcnn_resnet_v2
Best Precision: 99.86346364021301 %
Average Precision: 44.35067556433444 %
Worst Precision: 5.013251304626465 %
Best Speed: 0.12566494941711426 seconds
Average Speed: 0.19075694482676922 seconds
Worst Speed: 0.24387454986572266 seconds
Best Inference Speed: 0.12566494941711426 seconds
Average Inference Speed: 0.18640804290771484 seconds
Worst Inference Speed: 0.24387454986572266 seconds
Total Detections: 347
Total Inferences: 5


Model: faster_rcnn_mobilenet_v3
Best Precision: 99.15496110916138 %
Average Precision: 25.370939945754774 %
Worst Precision: 5.009407550096512 %
Best Speed: 0.03191542625427246 seconds
Average Speed: 0.06310564265790768 seconds
Worst Speed: 0.07879018783569336 seconds
Best Inference Speed: 0.03191542625427246 seconds
Average Inference Speed: 0.0630333423614502 seconds
Worst Inference Speed: 0.07879018783569336 seconds
Total Detections: 433
Total Inferences: 5


Model: faster_rcnn_mobilenet_v3_320
Best Precision: 99.13085699081421 %
Average Precision: 20.852532451125708 %
Worst Precision: 5.037242919206619 %
Best Speed: 0.025942564010620117 seconds
Average Speed: 0.036028368906541305 seconds
Worst Speed: 0.04089093208312988 seconds
Best Inference Speed: 0.025942564010620117 seconds
Average Inference Speed: 0.03570690155029297 seconds
Worst Inference Speed: 0.04089093208312988 seconds
Total Detections: 220
Total Inferences: 5


Model: mask_rcnn_resnet
Best Precision: 99.87896084785461 %
Average Precision: 38.427968363277614 %
Worst Precision: 5.086957663297653 %
Best Speed: 0.42885494232177734 seconds
Average Speed: 0.5186664545536042 seconds
Worst Speed: 0.7001280784606934 seconds
Best Inference Speed: 0.42885494232177734 seconds
Average Inference Speed: 0.5141184329986572 seconds
Worst Inference Speed: 0.7001280784606934 seconds
Total Detections: 400
Total Inferences: 5


Model: mask_rcnn_resnet_v2
Best Precision: 99.8472273349762 %
Average Precision: 45.33061946321065 %
Worst Precision: 5.0619907677173615 %
Best Speed: 0.2373661994934082 seconds
Average Speed: 0.4406829487745929 seconds
Worst Speed: 0.8537182807922363 seconds
Best Inference Speed: 0.2373661994934082 seconds
Average Inference Speed: 0.43427472114562987 seconds
Worst Inference Speed: 0.8537182807922363 seconds
Total Detections: 314
Total Inferences: 5


Model: ssd_vgg16
Best Precision: 94.76894736289978 %
Average Precision: 9.736606856840181 %
Worst Precision: 5.5172838270664215 %
Best Speed: 0.05186152458190918 seconds
Average Speed: 0.08539291008793075 seconds
Worst Speed: 0.09973359107971191 seconds
Best Inference Speed: 0.05186152458190918 seconds
Average Inference Speed: 0.08477387428283692 seconds
Worst Inference Speed: 0.09973359107971191 seconds
Total Detections: 928
Total Inferences: 5


Model: ssd_mobilenet_v3
Best Precision: 98.03175926208496 %
Average Precision: 7.859093187548818 %
Worst Precision: 3.058909811079502 %
Best Speed: 0.05186867713928223 seconds
Average Speed: 0.08636535787705824 seconds
Worst Speed: 0.11369585990905762 seconds
Best Inference Speed: 0.05186867713928223 seconds
Average Inference Speed: 0.08656964302062989 seconds
Worst Inference Speed: 0.11369585990905762 seconds
Total Detections: 1159
Total Inferences: 5


Model: yolov5nu
Best Precision: 91.97649955749512 %
Average Precision: 58.57487260078897 %
Worst Precision: 28.142526745796204 %
Best Speed: 0.05884051322937012 seconds
Average Speed: 0.12611162419221839 seconds
Worst Speed: 0.27726054191589355 seconds
Best Inference Speed: 0.05884051322937012 seconds
Average Inference Speed: 0.12406802177429199 seconds
Worst Inference Speed: 0.27726054191589355 seconds
Total Detections: 49
Total Inferences: 5


Model: yolov5su
Best Precision: 94.07821297645569 %
Average Precision: 64.22873437404633 %
Worst Precision: 25.000473856925964 %
Best Speed: 0.07380175590515137 seconds
Average Speed: 0.2245871482356902 seconds
Worst Speed: 0.39594244956970215 seconds
Best Inference Speed: 0.07380175590515137 seconds
Average Inference Speed: 0.23496627807617188 seconds
Worst Inference Speed: 0.39594244956970215 seconds
Total Detections: 62
Total Inferences: 5


Model: yolov5mu
Best Precision: 96.37896418571472 %
Average Precision: 66.38702585911139 %
Worst Precision: 26.028919219970703 %
Best Speed: 0.08477282524108887 seconds
Average Speed: 0.21376337760534042 seconds
Worst Speed: 0.5954077243804932 seconds
Best Inference Speed: 0.08477282524108887 seconds
Average Inference Speed: 0.2092421054840088 seconds
Worst Inference Speed: 0.5954077243804932 seconds
Total Detections: 78
Total Inferences: 5


Model: yolov5lu
Best Precision: 95.80461382865906 %
Average Precision: 66.45396899345309 %
Worst Precision: 26.01023018360138 %
Best Speed: 0.1655576229095459 seconds
Average Speed: 0.2654524847518566 seconds
Worst Speed: 0.5954070091247559 seconds
Best Inference Speed: 0.1655576229095459 seconds
Average Inference Speed: 0.27167305946350095 seconds
Worst Inference Speed: 0.5954070091247559 seconds
Total Detections: 86
Total Inferences: 5


Model: yolov5xu
Best Precision: 97.32694029808044 %
Average Precision: 69.34244239872152 %
Worst Precision: 25.08540153503418 %
Best Speed: 0.11070418357849121 seconds
Average Speed: 0.3247107592496005 seconds
Worst Speed: 1.0641543865203857 seconds
Best Inference Speed: 0.11070418357849121 seconds
Average Inference Speed: 0.3095716953277588 seconds
Worst Inference Speed: 1.0641543865203857 seconds
Total Detections: 88
Total Inferences: 5


Model: yolov5n6u
Best Precision: 94.23967599868774 %
Average Precision: 64.08193226655324 %
Worst Precision: 29.55993115901947 %
Best Speed: 0.1027231216430664 seconds
Average Speed: 0.11868117968241373 seconds
Worst Speed: 0.1685473918914795 seconds
Best Inference Speed: 0.1027231216430664 seconds
Average Inference Speed: 0.11927952766418456 seconds
Worst Inference Speed: 0.1685473918914795 seconds
Total Detections: 75
Total Inferences: 5


Model: yolov5s6u
Best Precision: 95.54756283760071 %
Average Precision: 65.645873939598 %
Worst Precision: 25.403085350990295 %
Best Speed: 0.07178878784179688 seconds
Average Speed: 0.15559118134634836 seconds
Worst Speed: 0.42985057830810547 seconds
Best Inference Speed: 0.07178878784179688 seconds
Average Inference Speed: 0.14460930824279786 seconds
Worst Inference Speed: 0.42985057830810547 seconds
Total Detections: 91
Total Inferences: 5


Model: yolov5m6u
Best Precision: 96.92941308021545 %
Average Precision: 68.4352594809454 %
Worst Precision: 25.7977694272995 %
Best Speed: 0.1545872688293457 seconds
Average Speed: 0.3111925867737317 seconds
Worst Speed: 0.7868955135345459 seconds
Best Inference Speed: 0.1545872688293457 seconds
Average Inference Speed: 0.30318942070007326 seconds
Worst Inference Speed: 0.7868955135345459 seconds
Total Detections: 122
Total Inferences: 5


Model: yolov5l6u
Best Precision: 96.77504301071167 %
Average Precision: 70.50009568532307 %
Worst Precision: 25.110426545143127 %
Best Speed: 0.1795203685760498 seconds
Average Speed: 0.3441328186172623 seconds
Worst Speed: 0.9005923271179199 seconds
Best Inference Speed: 0.1795203685760498 seconds
Average Inference Speed: 0.3259274005889893 seconds
Worst Inference Speed: 0.9005923271179199 seconds
Total Detections: 111
Total Inferences: 5


Model: yolov5x6u
Best Precision: 96.98180556297302 %
Average Precision: 70.57451889042026 %
Worst Precision: 26.68379843235016 %
Best Speed: 0.31017041206359863 seconds
Average Speed: 0.5707826259707617 seconds
Worst Speed: 1.5029950141906738 seconds
Best Inference Speed: 0.31017041206359863 seconds
Average Inference Speed: 0.5533233165740967 seconds
Worst Inference Speed: 1.5029950141906738 seconds
Total Detections: 121
Total Inferences: 5


Model: yolov8n
Best Precision: 95.45490741729736 %
Average Precision: 60.30679929714937 %
Worst Precision: 25.879091024398804 %
Best Speed: 0.05485057830810547 seconds
Average Speed: 0.06675835297657894 seconds
Worst Speed: 0.11367917060852051 seconds
Best Inference Speed: 0.05485057830810547 seconds
Average Inference Speed: 0.06721458435058594 seconds
Worst Inference Speed: 0.11367917060852051 seconds
Total Detections: 52
Total Inferences: 5


Model: yolov8s
Best Precision: 95.18426060676575 %
Average Precision: 65.38355345978881 %
Worst Precision: 27.328526973724365 %
Best Speed: 0.06482529640197754 seconds
Average Speed: 0.08678184856068004 seconds
Worst Speed: 0.18350839614868164 seconds
Best Inference Speed: 0.06482529640197754 seconds
Average Inference Speed: 0.08896079063415527 seconds
Worst Inference Speed: 0.18350839614868164 seconds
Total Detections: 66
Total Inferences: 5


Model: yolov8m
Best Precision: 96.5444028377533 %
Average Precision: 68.05786876342235 %
Worst Precision: 29.550659656524658 %
Best Speed: 0.09474515914916992 seconds
Average Speed: 0.17481354566720816 seconds
Worst Speed: 0.36003756523132324 seconds
Best Inference Speed: 0.09474515914916992 seconds
Average Inference Speed: 0.17513060569763184 seconds
Worst Inference Speed: 0.36003756523132324 seconds
Total Detections: 78
Total Inferences: 5


Model: yolov8l
Best Precision: 96.16448879241943 %
Average Precision: 69.06972021557564 %
Worst Precision: 26.587432622909546 %
Best Speed: 0.15059709548950195 seconds
Average Speed: 0.31129552042761516 seconds
Worst Speed: 0.7699415683746338 seconds
Best Inference Speed: 0.15059709548950195 seconds
Average Inference Speed: 0.3123647689819336 seconds
Worst Inference Speed: 0.7699415683746338 seconds
Total Detections: 86
Total Inferences: 5


Model: yolov8x
Best Precision: 96.99692130088806 %
Average Precision: 68.54419124887345 %
Worst Precision: 25.098952651023865 %
Best Speed: 0.10571527481079102 seconds
Average Speed: 0.26730590931912684 seconds
Worst Speed: 0.6751947402954102 seconds
Best Inference Speed: 0.10571527481079102 seconds
Average Inference Speed: 0.24773693084716797 seconds
Worst Inference Speed: 0.6751947402954102 seconds
Total Detections: 94
Total Inferences: 5


Model: yolov9c
Best Precision: 96.3855504989624 %
Average Precision: 71.11663131901388 %
Worst Precision: 25.430256128311157 %
Best Speed: 0.1545870304107666 seconds
Average Speed: 0.2124204073059425 seconds
Worst Speed: 0.39294910430908203 seconds
Best Inference Speed: 0.1545870304107666 seconds
Average Inference Speed: 0.20385456085205078 seconds
Worst Inference Speed: 0.39294910430908203 seconds
Total Detections: 89
Total Inferences: 5


Model: yolov9e
Best Precision: 97.04502820968628 %
Average Precision: 70.97663858107158 %
Worst Precision: 26.846206188201904 %
Best Speed: 0.29122042655944824 seconds
Average Speed: 0.4400404464630854 seconds
Worst Speed: 0.9773995876312256 seconds
Best Inference Speed: 0.29122042655944824 seconds
Average Inference Speed: 0.4302521705627441 seconds
Worst Inference Speed: 0.9773995876312256 seconds
Total Detections: 84
Total Inferences: 5


Best Precision:
Model: mask_rcnn_resnet
Best Precision: 99.87896084785461 %
Average Precision: 38.427968363277614 %
Worst Precision: 5.086957663297653 %
Best Speed: 0.42885494232177734 seconds
Average Speed: 0.5186664545536042 seconds
Worst Speed: 0.7001280784606934 seconds
Best Inference Speed: 0.42885494232177734 seconds
Average Inference Speed: 0.5141184329986572 seconds
Worst Inference Speed: 0.7001280784606934 seconds
Total Detections: 400
Total Inferences: 5


Best Average Precision:
Model: yolov9c
Best Precision: 96.3855504989624 %
Average Precision: 71.11663131901388 %
Worst Precision: 25.430256128311157 %
Best Speed: 0.1545870304107666 seconds
Average Speed: 0.2124204073059425 seconds
Worst Speed: 0.39294910430908203 seconds
Best Inference Speed: 0.1545870304107666 seconds
Average Inference Speed: 0.20385456085205078 seconds
Worst Inference Speed: 0.39294910430908203 seconds
Total Detections: 89
Total Inferences: 5


Worst Precision:
Model: ssd_mobilenet_v3
Best Precision: 98.03175926208496 %
Average Precision: 7.859093187548818 %
Worst Precision: 3.058909811079502 %
Best Speed: 0.05186867713928223 seconds
Average Speed: 0.08636535787705824 seconds
Worst Speed: 0.11369585990905762 seconds
Best Inference Speed: 0.05186867713928223 seconds
Average Inference Speed: 0.08656964302062989 seconds
Worst Inference Speed: 0.11369585990905762 seconds
Total Detections: 1159
Total Inferences: 5


Best Speed:
Model: faster_rcnn_mobilenet_v3_320
Best Precision: 99.13085699081421 %
Average Precision: 20.852532451125708 %
Worst Precision: 5.037242919206619 %
Best Speed: 0.025942564010620117 seconds
Average Speed: 0.036028368906541305 seconds
Worst Speed: 0.04089093208312988 seconds
Best Inference Speed: 0.025942564010620117 seconds
Average Inference Speed: 0.03570690155029297 seconds
Worst Inference Speed: 0.04089093208312988 seconds
Total Detections: 220
Total Inferences: 5


Best Average Speed:
Model: faster_rcnn_mobilenet_v3_320
Best Precision: 99.13085699081421 %
Average Precision: 20.852532451125708 %
Worst Precision: 5.037242919206619 %
Best Speed: 0.025942564010620117 seconds
Average Speed: 0.036028368906541305 seconds
Worst Speed: 0.04089093208312988 seconds
Best Inference Speed: 0.025942564010620117 seconds
Average Inference Speed: 0.03570690155029297 seconds
Worst Inference Speed: 0.04089093208312988 seconds
Total Detections: 220
Total Inferences: 5


Worst Speed:
Model: yolov5x6u
Best Precision: 96.98180556297302 %
Average Precision: 70.57451889042026 %
Worst Precision: 26.68379843235016 %
Best Speed: 0.31017041206359863 seconds
Average Speed: 0.5707826259707617 seconds
Worst Speed: 1.5029950141906738 seconds
Best Inference Speed: 0.31017041206359863 seconds
Average Inference Speed: 0.5533233165740967 seconds
Worst Inference Speed: 1.5029950141906738 seconds
Total Detections: 121
Total Inferences: 5


import numpy as np

# Model names
models = [
    "fcos_resnet", "retinanet_resnet", "retinanet_resnet_v2", "faster_rcnn_resnet",
    "faster_rcnn_resnet_v2", "faster_rcnn_mobilenet_v3", "faster_rcnn_mobilenet_v3_320",
    "mask_rcnn_resnet", "mask_rcnn_resnet_v2", "ssd_vgg16", "ssd_mobilenet_v3",
    "yolov5nu", "yolov5su", "yolov5mu", "yolov5lu", "yolov5xu", "yolov5n6u", "yolov5s6u",
    "yolov5m6u", "yolov5l6u", "yolov5x6u", "yolov8n", "yolov8s", "yolov8m", "yolov8l",
    "yolov8x", "yolov9c", "yolov9e"
]

# Precision metrics with six decimal places
best_precision = np.array([
    88.381338, 94.319451, 94.651043, 99.812824, 99.863464, 99.154961, 99.130857, 99.878961,
    99.847227, 94.768947, 98.031759, 91.976500, 94.078213, 96.378964, 95.804614, 97.326940,
    94.239676, 95.547563, 96.929413, 96.775043, 96.981806, 95.454907, 95.184261, 96.544403,
    96.164489, 96.996921, 96.385550, 97.045028
])
average_precision = np.array([
    41.433248, 26.668558, 28.262778, 34.533271, 44.350676, 25.370940, 20.852532, 38.427968,
    45.330619, 9.736607, 7.859093, 58.574873, 64.228734, 66.387026, 66.453969, 69.342442,
    64.081932, 65.645874, 68.435260, 70.500096, 70.574519, 60.306799, 65.383553, 68.057869,
    69.069720, 68.544191, 71.116631, 70.976639
])
worst_precision = np.array([
    25.272265, 5.019445, 5.091703, 5.013694, 5.013251, 5.009408, 5.037243, 5.086958,
    5.061991, 5.517284, 3.058910, 28.142527, 25.000474, 26.028919, 26.010230, 25.085402,
    29.559931, 25.403085, 25.797770, 25.110427, 26.683798, 25.879091, 27.328527, 29.550660,
    26.587433, 25.098953, 25.430256, 26.846206
])

# Speed metrics directly converted to milliseconds
best_speed_ms = np.array([
    295.212269, 115.691662, 93.750715, 102.730751, 125.664949, 31.915426, 25.942564,
    428.854942, 237.366199, 51.861525, 51.868677, 58.840513, 73.801756, 84.772825,
    165.557623, 110.704184, 102.723122, 71.788788, 154.587269, 179.520369, 310.170412,
    54.850578, 64.825296, 94.745159, 150.597095, 105.715275, 154.587030, 291.220427
])
average_speed_ms = np.array([
    370.031426, 174.803106, 158.746643, 154.443568, 190.756945, 63.105643, 36.028369,
    518.666455, 440.682949, 85.392910, 86.365358, 126.111624, 224.587148, 213.763378,
    265.452485, 324.710759, 118.681180, 155.591181, 311.192587, 344.132819, 570.782626,
    66.758353, 86.781849, 174.813546, 311.295520, 267.305909, 212.420407, 440.040446
])
worst_speed_ms = np.array([
    429.851055, 213.429213, 208.443642, 185.504198, 243.874550, 78.790188, 40.890932,
    700.128078, 853.718281, 99.733591, 113.695860, 277.260542, 395.942450, 595.407724,
    595.407009, 1064.154387, 168.547392, 429.850578, 786.895514, 900.592327, 1502.995014,
    113.679171, 183.508396, 360.037565, 769.941568, 675.194740, 392.949104, 977.399588
])

# Inference Speed metrics converted to milliseconds
best_inference_speed_ms = np.array([
    295.212269, 115.691662, 93.750715, 102.730751, 125.664949, 31.915426, 25.942564,
    428.854942, 237.366199, 51.861525, 51.868677, 58.840513, 73.801756, 84.772825,
    165.557623, 110.704184, 102.723122, 71.788788, 154.587269, 179.520369, 310.170412,
    54.850578, 64.825296, 94.745159, 150.597095, 105.715275, 154.587030, 291.220427
])
average_inference_speed_ms = np.array([
    369.613123, 174.533367, 162.367439, 155.186319, 186.408043, 63.033342, 35.706902,
    514.118433, 434.274721, 84.773874, 86.569643, 124.068022, 234.966278, 209.242105,
    271.673060, 309.571695, 119.279528, 144.609308, 303.189421, 325.927401, 553.323317,
    67.214585, 88.960791, 175.130606, 312.364769, 247.736931, 203.854561, 430.252171
])
worst_inference_speed_ms = np.array([
    429.851055, 213.429213, 208.443642, 185.504198, 243.874550, 78.790188, 40.890932,
    700.128078, 853.718281, 99.733591, 113.695860, 277.260542, 395.942450, 595.407724,
    595.407009, 1064.154387, 168.547392, 429.850578, 786.895514, 900.592327, 1502.995014,
    113.679171, 183.508396, 360.037565, 769.941568, 675.194740, 392.949104, 977.399588
])


import numpy as np
import matplotlib.pyplot as plt
import matplotlib

# Generate a color map
num_models = len(models)
cmap = matplotlib.colormaps.get_cmap('nipy_spectral')
colors = [cmap(i / num_models) for i in range(num_models)]

# Create a figure and plot with unique colors
plt.figure(figsize=(14, 10))
for i, model in enumerate(models):
    plt.scatter(average_speed[i], average_precision[i], color=colors[i], label=model, edgecolor='black')

print("DEVICE: GPU (NVidia RTX 2070S Mobile)")
print("SOURCE: data/video/vecteezy_car-and-truck-traffic-on-the-highway-in-europe-poland_7957364.MP4 (4k30fps Video)")
print("SKIP EVERY 60 FRAMES")
print("TOTALLY PROCESSED FRAMES: 6")
print("COCO Dataset, Classes truck, bus, car, train, bicycle")
# Axis labels and plot title
plt.xlabel('Average Speed (seconds)')
plt.ylabel('Average Precision (%)')
plt.title('Model Performance: Average Detection Speed x Average Accuracy')
plt.legend(loc='upper left', bbox_to_anchor=(1.05, 1), fontsize='small')
plt.grid(True)
plt.show()

# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_precision, average_precision, worst_precision])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Precision (%)", "Average Precision (%)", "Worst Precision (%)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

plt.title("Model Precision Metrics")
plt.show()

# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_speed_ms, average_speed_ms, worst_speed_ms])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Speed (ms)", "Average Speed (ms)", "Worst Speed (ms)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

plt.title("Model Speed Metrics")
plt.show()

# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_inference_speed_ms, average_inference_speed_ms, worst_inference_speed_ms])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Inference Speed (ms)", "Average Inference Speed (ms)", "Worst Inference Speed (mss)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

plt.title("Model Inference Speed Metrics")
plt.show()





import cv2
import torch
from torchvision.models.detection import *
from torchvision.io.image import read_image
from torchvision.utils import draw_bounding_boxes
from torchvision.transforms.functional import to_pil_image
import torchvision.transforms as transforms
from PIL import Image
import IPython
from IPython.display import display, clear_output
import time
import traceback
import math
from ultralytics import YOLO

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

transform = transforms.Compose([
    #transforms.Resize((300, 300)),
    transforms.ToTensor(),
])

class DetectionModel():
    def __init__(self, concept, backbone, model, weights, reference, color=None):
        self.concept = concept
        self.backbone = backbone
        self.model = model(weights) if concept=="YOLO" else model(weights=weights.DEFAULT)
        self.weights = weights
        self.reference = reference
        self.color = color
        self.detections = []

    def addDetection(self, detection):
        self.detections.append(detection)

    def getDetections(self):
        return self.detections

    def clearDetections(self):
        self.detections = []

    def detectionsCount(self):
        return len(self.detections)

    def inferencesCount(self):
        length = 0
        addedFrames = []
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    addedFrames.append(self.detections[i].frame)
        return len(addedFrames)

    def getAveragePrecision(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].precision
            avg = sum/len(self.detections)
        return  avg.item() if torch.is_tensor(avg) else avg

    def getMaxPrecision(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].precision:
                max=self.detections[i].precision
        return max.item() if torch.is_tensor(max) else max

    def getMinPrecision(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].precision:
                min=self.detections[i].precision
        return 0.0 if min==9999999.99 else min.item() if torch.is_tensor(min) else min

    def getAverageSpeed(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].speed
            avg = sum/len(self.detections)
        return avg

    def getMaxSpeed(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].speed:
                max=self.detections[i].speed
        return max

    def getMinSpeed(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].speed:
                min=self.detections[i].speed
        return 0.0 if min==9999999.99 else min

    def getAverageInferenceSpeed(self):
        sum=0.0
        avg=0.0
        addedFrames=[]
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    sum+=self.detections[i].speed
                    addedFrames.append(self.detections[i].frame)
            avg = sum/len(addedFrames) if len(addedFrames)>0 else 0.0
        return avg

    def getMaxInferenceSpeed(self):
        max=0.0
        addedFrames=[]
        for i in range(len(self.detections)):
            if max<self.detections[i].speed and self.detections[i].frame not in addedFrames:
                max=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return max

    def getMinInferenceSpeed(self):
        min=9999999.99
        addedFrames=[]
        for i in range(len(self.detections)):
            if min>self.detections[i].speed and self.detections[i].frame not in addedFrames:
                min=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return 0.0 if min==9999999.99 else min

    def filterPrecisionsBelow(self, minPrecision):
        newDetections = []
        for i in range(len(self.detections)):
            if self.detections[i].precision>=minPrecision:
                newDetections.append(self.detections[i])
        self.detections = newDetections

    def printResults(self):
        print("Model:", self.reference,
              "\nBest Precision:", self.getMaxPrecision(),
              "%\nAverage Precision:", self.getAveragePrecision(),
              "%\nWorst Precision:", self.getMinPrecision(),
              "%\nBest Speed:", self.getMinSpeed(),
              "seconds\nAverage Speed:", self.getAverageSpeed(),
              "seconds\nWorst Speed:", self.getMaxSpeed(),
              "seconds\nBest Inference Speed:", self.getMinInferenceSpeed(),
              "seconds\nAverage Inference Speed:", self.getAverageInferenceSpeed(),
              "seconds\nWorst Inference Speed:", self.getMaxInferenceSpeed(),
              "seconds\nTotal Detections:", self.detectionsCount(),
              "\nTotal Inferences:", self.inferencesCount(),
              "\n\n")

class DetectionResult():
    def __init__(self, model, label, precision, speed, device=None, source=None, frame=None, boundingBox=None, details=None):
        self.model = model
        self.label = label
        self.precision = precision.item() if torch.is_tensor(precision) else precision
        self.speed = speed.item() if torch.is_tensor(speed) else speed
        self.device = device
        self.source = source
        self.frame = frame
        self.boundingBox = boundingBox
        self.details = details

target_classes = ["truck", "bus", "car", "train", "bicycle"]


VEHICLE_DETECTION_MODELS = [
    DetectionModel("FCOS", "resnet50_fpn", fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights, "fcos_resnet", (255,0,0)),
    DetectionModel("RetinaNet", "resnet50_fpn", retinanet_resnet50_fpn, RetinaNet_ResNet50_FPN_Weights, "retinanet_resnet", (255,255,0)),
    DetectionModel("RetinaNet", "resnet50_fpn_v2", retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights, "retinanet_resnet_v2", (0,234,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn", fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights, "faster_rcnn_resnet", (170,0,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn_v2", fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights, "faster_rcnn_resnet_v2", (255,127,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_fpn", fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights, "faster_rcnn_mobilenet_v3", (191,255,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_320_fpn", fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights, "faster_rcnn_mobilenet_v3_320", (0,149,255)),
    DetectionModel("MaskRCNN", "resnet50_fpn", maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights, "mask_rcnn_resnet", (255,0,170)),
    DetectionModel("MaskRCNN", "resnet50_fpn_v2", maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights, "mask_rcnn_resnet_v2", (255,212,0)),
    DetectionModel("SSD300", "vgg16", ssd300_vgg16, SSD300_VGG16_Weights, "ssd_vgg16", (106,255,0)),
    DetectionModel("SSDLite320", "mobilenet_v3_large", ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights, "ssd_mobilenet_v3", (0,64,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5nu.pt', "yolov5nu", (237,185,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5su.pt', "yolov5su", (185,215,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5mu.pt', "yolov5mu", (231,233,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5lu.pt', "yolov5lu", (220,185,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5xu.pt', "yolov5xu", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5n6u.pt', "yolov5n6u", (143,35,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5s6u.pt', "yolov5s6u", (35,98,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5m6u.pt', "yolov5m6u", (143,106,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5l6u.pt', "yolov5l6u", (107,35,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5x6u.pt', "yolov5x6u", (79,143,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8n.pt', "yolov8n", (0,0,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8s.pt', "yolov8s", (115,115,115)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8m.pt', "yolov8m", (204,204,204)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8l.pt', "yolov8l", (255,0,0)), # cycle restarts
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8x.pt', "yolov8x", (255,255,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9c.pt', "yolov9c", (0,234,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9e.pt', "yolov9e", (170,0,255))
]

def process(frame, frameIndex=-1, source=None):
    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        processed_frame = frame.copy()

        if currentModel.concept=="YOLO":
            start_time = time.time()
            results = currentModel.model(processed_frame, agnostic_nms=True, verbose=False)[0]
            processing_time = time.time() - start_time
            for result in results:
                detection_count = result.boxes.shape[0]
                for j in range(detection_count):
                    cls = int(result.boxes.cls[j].item())
                    name = result.names[cls]
                    confidence = float(result.boxes.conf[j].item())
                    bounding_box = result.boxes.xyxy[j].cpu().numpy()
                    if name in target_classes:
                        x1, y1, x2, y2 = [int(x) for x in bounding_box]
                        # Ensure x1 < x2 and y1 < y2
                        x1, x2 = min(x1, x2), max(x1, x2)
                        y1, y2 = min(y1, y2), max(y1, y2)
                        label = '{} ({:.2f}%) ({:.3f}s) {}'.format(name.upper(), confidence*100, processing_time, currentModel.reference)
                        print(label)
                        VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, name, confidence*100, processing_time, device, source, frameIndex, bounding_box, label))
                        cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                        cv2.putText(processed_frame, label, (x1+10, y1+25), 0, 0.8, currentModel.color, 2)
        else:
            weights = currentModel.weights.DEFAULT
            model = currentModel.model
            model.to(device)
            model.eval()

            # Convert frame to RGB and PIL Image, then apply transformation
            frame_rgb = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            image = transform(pil_image).unsqueeze(0).to(device)

            # Object detection
            start_time = time.time()
            with torch.no_grad():
                predictions = model(image)

            # Processing time
            processing_time = time.time() - start_time

            # Visualization
            scores = predictions[0]['scores']
            boxes = predictions[0]['boxes']
            labels = predictions[0]['labels']
            confidence_threshold = 0.5

            for score, box, label in zip(scores, boxes, labels):
                # and score > confidence_threshold
                if weights.meta["categories"][label.item()] in target_classes and score > confidence_threshold:
                    x1, y1, x2, y2 = map(int, box)
                    label_text = f'{weights.meta["categories"][label.item()].upper()} ({score*100:.2f}%) ({processing_time:.3f}s) {currentModel.reference}'
                    print(label_text)
                    VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, weights.meta["categories"][label.item()], float(score.item())*100, processing_time, device, source, frameIndex, box, label_text))
                    cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                    cv2.putText(processed_frame, label_text, (x1, y1+20), 0, 0.8, currentModel.color, 2)

        img = Image.fromarray(processed_frame, 'RGB')
        display(img)
        clear_output(wait=True)
    return frame

def handleFrame(frame, frameIndex=-1, source=None):
    processed_frame = process(frame,frameIndex,source)

def handleRelease():
    cam.release()
    print("Source released.")

    print(device)
    print(source)

    BestPrecision=None
    BestAvgPrecision=None
    WorstPrecision=None
    BestSpeed=None
    BestAvgSpeed=None
    WorstSpeed=None

    Models=[]
    BestPrecisions=[]
    AvgPrecisions=[]
    WorstPrecisions=[]
    BestSpeeds=[]
    AvgSpeeds=[]
    WorstSpeeds=[]
    BestInferenceSpeeds=[]
    AvgInferenceSpeeds=[]
    WorstInferenceSpeeds=[]
    TotalDetectionsArr=[]
    TotalInferencesArr=[]

    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        currentModel.printResults()

        Models.append(currentModel.reference)
        BestPrecisions.append(currentModel.getMaxPrecision())
        AvgPrecisions.append(currentModel.getAveragePrecision())
        WorstPrecisions.append(currentModel.getMinPrecision())
        BestSpeeds.append(currentModel.getMinSpeed())
        AvgSpeeds.append(currentModel.getAverageSpeed())
        WorstSpeeds.append(currentModel.getMaxSpeed())
        BestInferenceSpeeds.append(currentModel.getMinInferenceSpeed())
        AvgInferenceSpeeds.append(currentModel.getAverageInferenceSpeed())
        WorstInferenceSpeeds.append(currentModel.getMaxInferenceSpeed())
        TotalDetectionsArr.append(currentModel.detectionsCount())
        TotalInferencesArr.append(currentModel.inferencesCount())

        if BestPrecision is None or BestPrecision.getMaxPrecision()<=currentModel.getMaxPrecision():
            BestPrecision = currentModel
        if BestAvgPrecision is None or BestAvgPrecision.getAveragePrecision()<=currentModel.getAveragePrecision():
            BestAvgPrecision = currentModel
        if WorstPrecision is None or WorstPrecision.getMinPrecision()>=currentModel.getMinPrecision():
            WorstPrecision = currentModel
        if BestSpeed is None or BestSpeed.getMinSpeed()>=currentModel.getMinSpeed():
            BestSpeed = currentModel
        if BestAvgSpeed is None or BestAvgSpeed.getAverageSpeed()>=currentModel.getAverageSpeed():
            BestAvgSpeed = currentModel
        if WorstSpeed is None or WorstSpeed.getMaxSpeed()<=currentModel.getMaxSpeed():
            WorstSpeed = currentModel

    if BestPrecision is not None:
        print("Best Precision:")
        BestPrecision.printResults()
    if BestAvgPrecision is not None:
        print("Best Average Precision:")
        BestAvgPrecision.printResults()
    if WorstPrecision is not None:
        print("Worst Precision:")
        WorstPrecision.printResults()
    if BestSpeed is not None:
        print("Best Speed:")
        BestSpeed.printResults()
    if BestAvgSpeed is not None:
        print("Best Average Speed:")
        BestAvgSpeed.printResults()
    if WorstSpeed is not None:
        print("Worst Speed:")
        WorstSpeed.printResults()

    print("models=",Models)
    print("best_precisions=",BestPrecisions)
    print("avg_precisions=",AvgPrecisions)
    print("worst_precisions=",WorstPrecisions)
    print("best_speeds=",BestSpeeds)
    print("avg_speeds=",AvgSpeeds)
    print("worst_speeds=",WorstSpeeds)
    print("best_inference_speeds=",BestInferenceSpeeds)
    print("avg_inference_speeds=",AvgInferenceSpeeds)
    print("worst_inference_speeds=",WorstInferenceSpeeds)
    print("total_detections=",TotalDetectionsArr)
    print("total_inferences=",TotalInferencesArr)

inputType="video"
source = "data/video/sample.MP4" if inputType=="video" else 0
cam = cv2.VideoCapture(source)
display_handle=display(None, display_id=True)
every = 60

start = 0 if source!=0 else -1
end = int(cam.get(cv2.CAP_PROP_FRAME_COUNT)) if source!=0 else -1
if source!=0: cam.set(1, start)

frameCount=start
while_safety=0
saved_count=0

if not cam.isOpened(): print("Error: Could not open source.")
else:
    try:
        while True if end<0 else frameCount<end:
            _, frame = cam.read()
            if frame is None:
                if source!=0:
                    if while_safety > 2000: break
                    while_safety += 1
                    continue
                else:
                    print("Error: Could not capture frame.")
                    cam.release()
                    break

            if every>0:
                if (frameCount+1)%math.floor(every) == 0:
                    while_safety = 0
                    handleFrame(frame, frameCount, source)
            else:
                while_safety=0
                handleFrame(frame)
            frameCount += 1
        handleRelease()
    except KeyboardInterrupt:
        handleRelease()
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Source could not be released.")
        except:
            pass
        traceback.print_exc()


Source released.
cpu
data/video/sample.MP4
Model: fcos_resnet
Best Precision: 84.01497006416321 %
Average Precision: 65.37111498588739 %
Worst Precision: 50.03466010093689 %
Best Speed: 1.3539996147155762 seconds
Average Speed: 1.5900508451831432 seconds
Worst Speed: 1.9969995021820068 seconds
Best Inference Speed: 1.3539996147155762 seconds
Average Inference Speed: 1.58190915096237 seconds
Worst Inference Speed: 1.9969995021820068 seconds
Total Detections: 129
Total Inferences: 83


Model: retinanet_resnet
Best Precision: 86.20967268943787 %
Average Precision: 69.96585425642348 %
Worst Precision: 51.02284550666809 %
Best Speed: 1.5019989013671875 seconds
Average Speed: 1.7201893993259705 seconds
Worst Speed: 1.9779984951019287 seconds
Best Inference Speed: 1.5019989013671875 seconds
Average Inference Speed: 1.720051841858106 seconds
Worst Inference Speed: 1.9779984951019287 seconds
Total Detections: 97
Total Inferences: 78


Model: retinanet_resnet_v2
Best Precision: 96.10866904258728 %
Average Precision: 76.44101748562822 %
Worst Precision: 50.60317516326904 %
Best Speed: 1.5300014019012451 seconds
Average Speed: 1.6965783485258468 seconds
Worst Speed: 2.057000160217285 seconds
Best Inference Speed: 1.5300014019012451 seconds
Average Inference Speed: 1.7037758558988572 seconds
Worst Inference Speed: 2.057000160217285 seconds
Total Detections: 99
Total Inferences: 80


Model: faster_rcnn_resnet
Best Precision: 99.24747347831726 %
Average Precision: 79.92927298188847 %
Worst Precision: 50.00736713409424 %
Best Speed: 1.5599994659423828 seconds
Average Speed: 1.6948788663282752 seconds
Worst Speed: 1.8570001125335693 seconds
Best Inference Speed: 1.5599994659423828 seconds
Average Inference Speed: 1.690338404781847 seconds
Worst Inference Speed: 1.8570001125335693 seconds
Total Detections: 187
Total Inferences: 83


Model: faster_rcnn_resnet_v2
Best Precision: 99.60418939590454 %
Average Precision: 80.5721729032455 %
Worst Precision: 50.113362073898315 %
Best Speed: 2.264000415802002 seconds
Average Speed: 2.449766608427197 seconds
Worst Speed: 2.6440017223358154 seconds
Best Inference Speed: 2.264000415802002 seconds
Average Inference Speed: 2.4464489760852994 seconds
Worst Inference Speed: 2.6440017223358154 seconds
Total Detections: 217
Total Inferences: 84


Model: faster_rcnn_mobilenet_v3
Best Precision: 99.35287833213806 %
Average Precision: 78.78277043209357 %
Worst Precision: 50.14830827713013 %
Best Speed: 0.31999921798706055 seconds
Average Speed: 0.4300350476713741 seconds
Worst Speed: 0.6099998950958252 seconds
Best Inference Speed: 0.31999921798706055 seconds
Average Inference Speed: 0.42600051561991376 seconds
Worst Inference Speed: 0.6099998950958252 seconds
Total Detections: 136
Total Inferences: 84


Model: faster_rcnn_mobilenet_v3_320
Best Precision: 98.90456199645996 %
Average Precision: 78.05265253409743 %
Worst Precision: 52.156418561935425 %
Best Speed: 0.1419980525970459 seconds
Average Speed: 0.2042175717651844 seconds
Worst Speed: 0.3490004539489746 seconds
Best Inference Speed: 0.1419980525970459 seconds
Average Inference Speed: 0.20521727609045712 seconds
Worst Inference Speed: 0.3490004539489746 seconds
Total Detections: 128
Total Inferences: 81


Model: mask_rcnn_resnet
Best Precision: 98.711097240448 %
Average Precision: 78.70424458855076 %
Worst Precision: 50.028592348098755 %
Best Speed: 1.5999958515167236 seconds
Average Speed: 1.8232942179629676 seconds
Worst Speed: 2.0370004177093506 seconds
Best Inference Speed: 1.5999958515167236 seconds
Average Inference Speed: 1.8135415798141843 seconds
Worst Inference Speed: 2.0370004177093506 seconds
Total Detections: 190
Total Inferences: 84


Model: mask_rcnn_resnet_v2
Best Precision: 99.41349625587463 %
Average Precision: 78.58292045186481 %
Worst Precision: 50.22304058074951 %
Best Speed: 2.3830008506774902 seconds
Average Speed: 2.60096769536276 seconds
Worst Speed: 2.816997766494751 seconds
Best Inference Speed: 2.3830008506774902 seconds
Average Inference Speed: 2.601034700870514 seconds
Worst Inference Speed: 2.816997766494751 seconds
Total Detections: 211
Total Inferences: 84


Model: ssd_vgg16
Best Precision: 95.25397419929504 %
Average Precision: 78.78161410251296 %
Worst Precision: 51.88506245613098 %
Best Speed: 0.2420032024383545 seconds
Average Speed: 0.3163974198950342 seconds
Worst Speed: 0.5239999294281006 seconds
Best Inference Speed: 0.2420032024383545 seconds
Average Inference Speed: 0.3179741761623285 seconds
Worst Inference Speed: 0.5239999294281006 seconds
Total Detections: 83
Total Inferences: 78


Model: ssd_mobilenet_v3
Best Precision: 97.86267876625061 %
Average Precision: 78.83799374103546 %
Worst Precision: 52.02542543411255 %
Best Speed: 0.10300064086914062 seconds
Average Speed: 0.12453579416080397 seconds
Worst Speed: 0.1680004596710205 seconds
Best Inference Speed: 0.10300064086914062 seconds
Average Inference Speed: 0.12368244148162474 seconds
Worst Inference Speed: 0.1680004596710205 seconds
Total Detections: 98
Total Inferences: 83


Model: yolov5nu
Best Precision: 89.55094814300537 %
Average Precision: 59.80522941558733 %
Worst Precision: 25.00203847885132 %
Best Speed: 0.07900047302246094 seconds
Average Speed: 0.15297493147193839 seconds
Worst Speed: 3.3814072608947754 seconds
Best Inference Speed: 0.07900047302246094 seconds
Average Inference Speed: 0.16270243116171965 seconds
Worst Inference Speed: 3.3814072608947754 seconds
Total Detections: 109
Total Inferences: 83


Model: yolov5su
Best Precision: 94.16013956069946 %
Average Precision: 65.16339556106085 %
Worst Precision: 28.957846760749817 %
Best Speed: 0.10699963569641113 seconds
Average Speed: 0.1763820694488229 seconds
Worst Speed: 2.9385104179382324 seconds
Best Inference Speed: 0.10699963569641113 seconds
Average Inference Speed: 0.18419471525010608 seconds
Worst Inference Speed: 2.9385104179382324 seconds
Total Detections: 103
Total Inferences: 84


Model: yolov5mu
Best Precision: 95.83572149276733 %
Average Precision: 62.61705411959064 %
Worst Precision: 25.00402331352234 %
Best Speed: 0.2089977264404297 seconds
Average Speed: 0.2967791390973468 seconds
Worst Speed: 3.3595077991485596 seconds
Best Inference Speed: 0.2089977264404297 seconds
Average Inference Speed: 0.30788696663720266 seconds
Worst Inference Speed: 3.3595077991485596 seconds
Total Detections: 129
Total Inferences: 84


Model: yolov5lu
Best Precision: 95.45990824699402 %
Average Precision: 73.18180666438171 %
Worst Precision: 25.045570731163025 %
Best Speed: 0.31800150871276855 seconds
Average Speed: 0.4719558592353548 seconds
Worst Speed: 3.823509693145752 seconds
Best Inference Speed: 0.31800150871276855 seconds
Average Inference Speed: 0.4812829153878348 seconds
Worst Inference Speed: 3.823509693145752 seconds
Total Detections: 112
Total Inferences: 84


Model: yolov5xu
Best Precision: 96.57882452011108 %
Average Precision: 75.30654810112098 %
Worst Precision: 25.6292462348938 %
Best Speed: 0.5419995784759521 seconds
Average Speed: 0.7510631721595238 seconds
Worst Speed: 4.539067983627319 seconds
Best Inference Speed: 0.5419995784759521 seconds
Average Inference Speed: 0.7577934889566331 seconds
Worst Inference Speed: 4.539067983627319 seconds
Total Detections: 116
Total Inferences: 84


Model: yolov5n6u
Best Precision: 89.94278907775879 %
Average Precision: 63.64890622723963 %
Worst Precision: 25.59078335762024 %
Best Speed: 0.16900110244750977 seconds
Average Speed: 0.2531993684080458 seconds
Worst Speed: 3.1432266235351562 seconds
Best Inference Speed: 0.16900110244750977 seconds
Average Inference Speed: 0.2604563179470244 seconds
Worst Inference Speed: 3.1432266235351562 seconds
Total Detections: 97
Total Inferences: 84


Model: yolov5s6u
Best Precision: 93.81153583526611 %
Average Precision: 68.91447240787167 %
Worst Precision: 25.863394141197205 %
Best Speed: 0.2979624271392822 seconds
Average Speed: 0.36630508592051847 seconds
Worst Speed: 0.5580019950866699 seconds
Best Inference Speed: 0.2979624271392822 seconds
Average Inference Speed: 0.36485346254095974 seconds
Worst Inference Speed: 0.5580019950866699 seconds
Total Detections: 124
Total Inferences: 83


Model: yolov5m6u
Best Precision: 96.59476280212402 %
Average Precision: 74.37427713245641 %
Worst Precision: 27.020099759101868 %
Best Speed: 0.6189985275268555 seconds
Average Speed: 0.786983353192689 seconds
Worst Speed: 1.033001184463501 seconds
Best Inference Speed: 0.6189985275268555 seconds
Average Inference Speed: 0.7825815103140222 seconds
Worst Inference Speed: 1.033001184463501 seconds
Total Detections: 122
Total Inferences: 83


Model: yolov5l6u
Best Precision: 96.98338508605957 %
Average Precision: 76.79304788189549 %
Worst Precision: 25.58591663837433 %
Best Speed: 1.178002119064331 seconds
Average Speed: 1.340057905643217 seconds
Worst Speed: 1.5633089542388916 seconds
Best Inference Speed: 1.178002119064331 seconds
Average Inference Speed: 1.3347263859539498 seconds
Worst Inference Speed: 1.5633089542388916 seconds
Total Detections: 124
Total Inferences: 82


Model: yolov5x6u
Best Precision: 96.82453870773315 %
Average Precision: 77.06774789553423 %
Worst Precision: 25.649192929267883 %
Best Speed: 1.849999189376831 seconds
Average Speed: 2.149621004324693 seconds
Worst Speed: 7.2852160930633545 seconds
Best Inference Speed: 1.849999189376831 seconds
Average Inference Speed: 2.168102888833909 seconds
Worst Inference Speed: 7.2852160930633545 seconds
Total Detections: 130
Total Inferences: 84


Model: yolov8n
Best Precision: 89.87798690795898 %
Average Precision: 64.94164858201538 %
Worst Precision: 25.362318754196167 %
Best Speed: 0.07900071144104004 seconds
Average Speed: 0.1458940746808293 seconds
Worst Speed: 3.268512725830078 seconds
Best Inference Speed: 0.07900071144104004 seconds
Average Inference Speed: 0.15061255863734654 seconds
Worst Inference Speed: 3.268512725830078 seconds
Total Detections: 99
Total Inferences: 84


Model: yolov8s
Best Precision: 96.51697874069214 %
Average Precision: 69.82134878635406 %
Worst Precision: 25.33295452594757 %
Best Speed: 0.11600041389465332 seconds
Average Speed: 0.18567290144451595 seconds
Worst Speed: 3.3655169010162354 seconds
Best Inference Speed: 0.11600041389465332 seconds
Average Inference Speed: 0.1982314529873076 seconds
Worst Inference Speed: 3.3655169010162354 seconds
Total Detections: 118
Total Inferences: 84


Model: yolov8m
Best Precision: 96.09609842300415 %
Average Precision: 75.26495310393247 %
Worst Precision: 25.73632299900055 %
Best Speed: 0.2160015106201172 seconds
Average Speed: 0.35385025414553556 seconds
Worst Speed: 3.959514856338501 seconds
Best Inference Speed: 0.2160015106201172 seconds
Average Inference Speed: 0.36191155513127643 seconds
Worst Inference Speed: 3.959514856338501 seconds
Total Detections: 110
Total Inferences: 84


Model: yolov8l
Best Precision: 96.97030186653137 %
Average Precision: 75.66016041918805 %
Worst Precision: 26.122286915779114 %
Best Speed: 0.3510007858276367 seconds
Average Speed: 0.5758424244428936 seconds
Worst Speed: 4.32851767539978 seconds
Best Inference Speed: 0.3510007858276367 seconds
Average Inference Speed: 0.5852860098793393 seconds
Worst Inference Speed: 4.32851767539978 seconds
Total Detections: 114
Total Inferences: 84


Model: yolov8x
Best Precision: 96.95289731025696 %
Average Precision: 82.63096755689328 %
Worst Precision: 26.093509793281555 %
Best Speed: 0.5150008201599121 seconds
Average Speed: 0.7371541036141885 seconds
Worst Speed: 4.544516086578369 seconds
Best Inference Speed: 0.5150008201599121 seconds
Average Inference Speed: 0.7440934635344005 seconds
Worst Inference Speed: 4.544516086578369 seconds
Total Detections: 111
Total Inferences: 84


Model: yolov9c
Best Precision: 95.78874111175537 %
Average Precision: 75.23137887494754 %
Worst Precision: 25.40087103843689 %
Best Speed: 0.3299996852874756 seconds
Average Speed: 0.49190587069080993 seconds
Worst Speed: 3.4925150871276855 seconds
Best Inference Speed: 0.3299996852874756 seconds
Average Inference Speed: 0.4948988273030236 seconds
Worst Inference Speed: 3.4925150871276855 seconds
Total Detections: 113
Total Inferences: 84


Model: yolov9e
Best Precision: 96.19094133377075 %
Average Precision: 76.31394955538964 %
Worst Precision: 25.914165377616882 %
Best Speed: 0.6600019931793213 seconds
Average Speed: 0.919382045435351 seconds
Worst Speed: 4.214514970779419 seconds
Best Inference Speed: 0.6600019931793213 seconds
Average Inference Speed: 0.9257406876200721 seconds
Worst Inference Speed: 4.214514970779419 seconds
Total Detections: 129
Total Inferences: 84


Best Precision:
Model: faster_rcnn_resnet_v2
Best Precision: 99.60418939590454 %
Average Precision: 80.5721729032455 %
Worst Precision: 50.113362073898315 %
Best Speed: 2.264000415802002 seconds
Average Speed: 2.449766608427197 seconds
Worst Speed: 2.6440017223358154 seconds
Best Inference Speed: 2.264000415802002 seconds
Average Inference Speed: 2.4464489760852994 seconds
Worst Inference Speed: 2.6440017223358154 seconds
Total Detections: 217
Total Inferences: 84


Best Average Precision:
Model: yolov8x
Best Precision: 96.95289731025696 %
Average Precision: 82.63096755689328 %
Worst Precision: 26.093509793281555 %
Best Speed: 0.5150008201599121 seconds
Average Speed: 0.7371541036141885 seconds
Worst Speed: 4.544516086578369 seconds
Best Inference Speed: 0.5150008201599121 seconds
Average Inference Speed: 0.7440934635344005 seconds
Worst Inference Speed: 4.544516086578369 seconds
Total Detections: 111
Total Inferences: 84


Worst Precision:
Model: yolov5nu
Best Precision: 89.55094814300537 %
Average Precision: 59.80522941558733 %
Worst Precision: 25.00203847885132 %
Best Speed: 0.07900047302246094 seconds
Average Speed: 0.15297493147193839 seconds
Worst Speed: 3.3814072608947754 seconds
Best Inference Speed: 0.07900047302246094 seconds
Average Inference Speed: 0.16270243116171965 seconds
Worst Inference Speed: 3.3814072608947754 seconds
Total Detections: 109
Total Inferences: 83


Best Speed:
Model: yolov5nu
Best Precision: 89.55094814300537 %
Average Precision: 59.80522941558733 %
Worst Precision: 25.00203847885132 %
Best Speed: 0.07900047302246094 seconds
Average Speed: 0.15297493147193839 seconds
Worst Speed: 3.3814072608947754 seconds
Best Inference Speed: 0.07900047302246094 seconds
Average Inference Speed: 0.16270243116171965 seconds
Worst Inference Speed: 3.3814072608947754 seconds
Total Detections: 109
Total Inferences: 83


Best Average Speed:
Model: ssd_mobilenet_v3
Best Precision: 97.86267876625061 %
Average Precision: 78.83799374103546 %
Worst Precision: 52.02542543411255 %
Best Speed: 0.10300064086914062 seconds
Average Speed: 0.12453579416080397 seconds
Worst Speed: 0.1680004596710205 seconds
Best Inference Speed: 0.10300064086914062 seconds
Average Inference Speed: 0.12368244148162474 seconds
Worst Inference Speed: 0.1680004596710205 seconds
Total Detections: 98
Total Inferences: 83


Worst Speed:
Model: yolov5x6u
Best Precision: 96.82453870773315 %
Average Precision: 77.06774789553423 %
Worst Precision: 25.649192929267883 %
Best Speed: 1.849999189376831 seconds
Average Speed: 2.149621004324693 seconds
Worst Speed: 7.2852160930633545 seconds
Best Inference Speed: 1.849999189376831 seconds
Average Inference Speed: 2.168102888833909 seconds
Worst Inference Speed: 7.2852160930633545 seconds
Total Detections: 130
Total Inferences: 84


models= ['fcos_resnet', 'retinanet_resnet', 'retinanet_resnet_v2', 'faster_rcnn_resnet', 'faster_rcnn_resnet_v2', 'faster_rcnn_mobilenet_v3', 'faster_rcnn_mobilenet_v3_320', 'mask_rcnn_resnet', 'mask_rcnn_resnet_v2', 'ssd_vgg16', 'ssd_mobilenet_v3', 'yolov5nu', 'yolov5su', 'yolov5mu', 'yolov5lu', 'yolov5xu', 'yolov5n6u', 'yolov5s6u', 'yolov5m6u', 'yolov5l6u', 'yolov5x6u', 'yolov8n', 'yolov8s', 'yolov8m', 'yolov8l', 'yolov8x', 'yolov9c', 'yolov9e']
best_precisions= [84.01497006416321, 86.20967268943787, 96.10866904258728, 99.24747347831726, 99.60418939590454, 99.35287833213806, 98.90456199645996, 98.711097240448, 99.41349625587463, 95.25397419929504, 97.86267876625061, 89.55094814300537, 94.16013956069946, 95.83572149276733, 95.45990824699402, 96.57882452011108, 89.94278907775879, 93.81153583526611, 96.59476280212402, 96.98338508605957, 96.82453870773315, 89.87798690795898, 96.51697874069214, 96.09609842300415, 96.97030186653137, 96.95289731025696, 95.78874111175537, 96.19094133377075]
avg_precisions= [65.37111498588739, 69.96585425642348, 76.44101748562822, 79.92927298188847, 80.5721729032455, 78.78277043209357, 78.05265253409743, 78.70424458855076, 78.58292045186481, 78.78161410251296, 78.83799374103546, 59.80522941558733, 65.16339556106085, 62.61705411959064, 73.18180666438171, 75.30654810112098, 63.64890622723963, 68.91447240787167, 74.37427713245641, 76.79304788189549, 77.06774789553423, 64.94164858201538, 69.82134878635406, 75.26495310393247, 75.66016041918805, 82.63096755689328, 75.23137887494754, 76.31394955538964]
worst_precisions= [50.03466010093689, 51.02284550666809, 50.60317516326904, 50.00736713409424, 50.113362073898315, 50.14830827713013, 52.156418561935425, 50.028592348098755, 50.22304058074951, 51.88506245613098, 52.02542543411255, 25.00203847885132, 28.957846760749817, 25.00402331352234, 25.045570731163025, 25.6292462348938, 25.59078335762024, 25.863394141197205, 27.020099759101868, 25.58591663837433, 25.649192929267883, 25.362318754196167, 25.33295452594757, 25.73632299900055, 26.122286915779114, 26.093509793281555, 25.40087103843689, 25.914165377616882]
best_speeds= [1.3539996147155762, 1.5019989013671875, 1.5300014019012451, 1.5599994659423828, 2.264000415802002, 0.31999921798706055, 0.1419980525970459, 1.5999958515167236, 2.3830008506774902, 0.2420032024383545, 0.10300064086914062, 0.07900047302246094, 0.10699963569641113, 0.2089977264404297, 0.31800150871276855, 0.5419995784759521, 0.16900110244750977, 0.2979624271392822, 0.6189985275268555, 1.178002119064331, 1.849999189376831, 0.07900071144104004, 0.11600041389465332, 0.2160015106201172, 0.3510007858276367, 0.5150008201599121, 0.3299996852874756, 0.6600019931793213]
avg_speeds= [1.5900508451831432, 1.7201893993259705, 1.6965783485258468, 1.6948788663282752, 2.449766608427197, 0.4300350476713741, 0.2042175717651844, 1.8232942179629676, 2.60096769536276, 0.3163974198950342, 0.12453579416080397, 0.15297493147193839, 0.1763820694488229, 0.2967791390973468, 0.4719558592353548, 0.7510631721595238, 0.2531993684080458, 0.36630508592051847, 0.786983353192689, 1.340057905643217, 2.149621004324693, 0.1458940746808293, 0.18567290144451595, 0.35385025414553556, 0.5758424244428936, 0.7371541036141885, 0.49190587069080993, 0.919382045435351]
worst_speeds= [1.9969995021820068, 1.9779984951019287, 2.057000160217285, 1.8570001125335693, 2.6440017223358154, 0.6099998950958252, 0.3490004539489746, 2.0370004177093506, 2.816997766494751, 0.5239999294281006, 0.1680004596710205, 3.3814072608947754, 2.9385104179382324, 3.3595077991485596, 3.823509693145752, 4.539067983627319, 3.1432266235351562, 0.5580019950866699, 1.033001184463501, 1.5633089542388916, 7.2852160930633545, 3.268512725830078, 3.3655169010162354, 3.959514856338501, 4.32851767539978, 4.544516086578369, 3.4925150871276855, 4.214514970779419]
best_inference_speeds= [1.3539996147155762, 1.5019989013671875, 1.5300014019012451, 1.5599994659423828, 2.264000415802002, 0.31999921798706055, 0.1419980525970459, 1.5999958515167236, 2.3830008506774902, 0.2420032024383545, 0.10300064086914062, 0.07900047302246094, 0.10699963569641113, 0.2089977264404297, 0.31800150871276855, 0.5419995784759521, 0.16900110244750977, 0.2979624271392822, 0.6189985275268555, 1.178002119064331, 1.849999189376831, 0.07900071144104004, 0.11600041389465332, 0.2160015106201172, 0.3510007858276367, 0.5150008201599121, 0.3299996852874756, 0.6600019931793213]
avg_inference_speeds= [1.58190915096237, 1.720051841858106, 1.7037758558988572, 1.690338404781847, 2.4464489760852994, 0.42600051561991376, 0.20521727609045712, 1.8135415798141843, 2.601034700870514, 0.3179741761623285, 0.12368244148162474, 0.16270243116171965, 0.18419471525010608, 0.30788696663720266, 0.4812829153878348, 0.7577934889566331, 0.2604563179470244, 0.36485346254095974, 0.7825815103140222, 1.3347263859539498, 2.168102888833909, 0.15061255863734654, 0.1982314529873076, 0.36191155513127643, 0.5852860098793393, 0.7440934635344005, 0.4948988273030236, 0.9257406876200721]
worst_inference_speeds= [1.9969995021820068, 1.9779984951019287, 2.057000160217285, 1.8570001125335693, 2.6440017223358154, 0.6099998950958252, 0.3490004539489746, 2.0370004177093506, 2.816997766494751, 0.5239999294281006, 0.1680004596710205, 3.3814072608947754, 2.9385104179382324, 3.3595077991485596, 3.823509693145752, 4.539067983627319, 3.1432266235351562, 0.5580019950866699, 1.033001184463501, 1.5633089542388916, 7.2852160930633545, 3.268512725830078, 3.3655169010162354, 3.959514856338501, 4.32851767539978, 4.544516086578369, 3.4925150871276855, 4.214514970779419]
total_detections= [129, 97, 99, 187, 217, 136, 128, 190, 211, 83, 97.86267876625061, 89.55094814300537, 94.16013956069946, 95.83572149276733, 95.45990824699402, 96.57882452011108, 89.94278907775879, 93.81153583526611, 96.59476280212402, 96.98338508605957, 96.82453870773315, 89.87798690795898, 96.51697874069214, 96.09609842300415, 96.97030186653137, 96.95289731025696, 95.78874111175537, 96.19094133377075]
total_inferences= [83, 78, 80, 83, 84, 84, 84, 84, 78, 95.25397419929504, 97.86267876625061, 89.55094814300537, 94.16013956069946, 95.83572149276733, 95.45990824699402, 96.57882452011108, 89.94278907775879, 93.81153583526611, 96.59476280212402, 96.98338508605957, 96.82453870773315, 89.87798690795898, 96.51697874069214, 96.09609842300415, 96.97030186653137, 96.95289731025696, 95.78874111175537, 96.19094133377075]



models= ['fcos_resnet', 'retinanet_resnet', 'retinanet_resnet_v2', 'faster_rcnn_resnet', 'faster_rcnn_resnet_v2', 'faster_rcnn_mobilenet_v3', 'faster_rcnn_mobilenet_v3_320', 'mask_rcnn_resnet', 'mask_rcnn_resnet_v2', 'ssd_vgg16', 'ssd_mobilenet_v3', 'yolov5nu', 'yolov5su', 'yolov5mu', 'yolov5lu', 'yolov5xu', 'yolov5n6u', 'yolov5s6u', 'yolov5m6u', 'yolov5l6u', 'yolov5x6u', 'yolov8n', 'yolov8s', 'yolov8m', 'yolov8l', 'yolov8x', 'yolov9c', 'yolov9e']
best_precisions= [84.01497006416321, 86.20967268943787, 96.10866904258728, 99.24747347831726, 99.60418939590454, 99.35287833213806, 98.90456199645996, 98.711097240448, 99.41349625587463, 95.25397419929504, 97.86267876625061, 89.55094814300537, 94.16013956069946, 95.83572149276733, 95.45990824699402, 96.57882452011108, 89.94278907775879, 93.81153583526611, 96.59476280212402, 96.98338508605957, 96.82453870773315, 89.87798690795898, 96.51697874069214, 96.09609842300415, 96.97030186653137, 96.95289731025696, 95.78874111175537, 96.19094133377075]
avg_precisions= [65.37111498588739, 69.96585425642348, 76.44101748562822, 79.92927298188847, 80.5721729032455, 78.78277043209357, 78.05265253409743, 78.70424458855076, 78.58292045186481, 78.78161410251296, 78.83799374103546, 59.80522941558733, 65.16339556106085, 62.61705411959064, 73.18180666438171, 75.30654810112098, 63.64890622723963, 68.91447240787167, 74.37427713245641, 76.79304788189549, 77.06774789553423, 64.94164858201538, 69.82134878635406, 75.26495310393247, 75.66016041918805, 82.63096755689328, 75.23137887494754, 76.31394955538964]
worst_precisions= [50.03466010093689, 51.02284550666809, 50.60317516326904, 50.00736713409424, 50.113362073898315, 50.14830827713013, 52.156418561935425, 50.028592348098755, 50.22304058074951, 51.88506245613098, 52.02542543411255, 25.00203847885132, 28.957846760749817, 25.00402331352234, 25.045570731163025, 25.6292462348938, 25.59078335762024, 25.863394141197205, 27.020099759101868, 25.58591663837433, 25.649192929267883, 25.362318754196167, 25.33295452594757, 25.73632299900055, 26.122286915779114, 26.093509793281555, 25.40087103843689, 25.914165377616882]
best_speeds= [1.3539996147155762, 1.5019989013671875, 1.5300014019012451, 1.5599994659423828, 2.264000415802002, 0.31999921798706055, 0.1419980525970459, 1.5999958515167236, 2.3830008506774902, 0.2420032024383545, 0.10300064086914062, 0.07900047302246094, 0.10699963569641113, 0.2089977264404297, 0.31800150871276855, 0.5419995784759521, 0.16900110244750977, 0.2979624271392822, 0.6189985275268555, 1.178002119064331, 1.849999189376831, 0.07900071144104004, 0.11600041389465332, 0.2160015106201172, 0.3510007858276367, 0.5150008201599121, 0.3299996852874756, 0.6600019931793213]
avg_speeds= [1.5900508451831432, 1.7201893993259705, 1.6965783485258468, 1.6948788663282752, 2.449766608427197, 0.4300350476713741, 0.2042175717651844, 1.8232942179629676, 2.60096769536276, 0.3163974198950342, 0.12453579416080397, 0.15297493147193839, 0.1763820694488229, 0.2967791390973468, 0.4719558592353548, 0.7510631721595238, 0.2531993684080458, 0.36630508592051847, 0.786983353192689, 1.340057905643217, 2.149621004324693, 0.1458940746808293, 0.18567290144451595, 0.35385025414553556, 0.5758424244428936, 0.7371541036141885, 0.49190587069080993, 0.919382045435351]
worst_speeds= [1.9969995021820068, 1.9779984951019287, 2.057000160217285, 1.8570001125335693, 2.6440017223358154, 0.6099998950958252, 0.3490004539489746, 2.0370004177093506, 2.816997766494751, 0.5239999294281006, 0.1680004596710205, 3.3814072608947754, 2.9385104179382324, 3.3595077991485596, 3.823509693145752, 4.539067983627319, 3.1432266235351562, 0.5580019950866699, 1.033001184463501, 1.5633089542388916, 7.2852160930633545, 3.268512725830078, 3.3655169010162354, 3.959514856338501, 4.32851767539978, 4.544516086578369, 3.4925150871276855, 4.214514970779419]
best_inference_speeds= [1.3539996147155762, 1.5019989013671875, 1.5300014019012451, 1.5599994659423828, 2.264000415802002, 0.31999921798706055, 0.1419980525970459, 1.5999958515167236, 2.3830008506774902, 0.2420032024383545, 0.10300064086914062, 0.07900047302246094, 0.10699963569641113, 0.2089977264404297, 0.31800150871276855, 0.5419995784759521, 0.16900110244750977, 0.2979624271392822, 0.6189985275268555, 1.178002119064331, 1.849999189376831, 0.07900071144104004, 0.11600041389465332, 0.2160015106201172, 0.3510007858276367, 0.5150008201599121, 0.3299996852874756, 0.6600019931793213]
avg_inference_speeds= [1.58190915096237, 1.720051841858106, 1.7037758558988572, 1.690338404781847, 2.4464489760852994, 0.42600051561991376, 0.20521727609045712, 1.8135415798141843, 2.601034700870514, 0.3179741761623285, 0.12368244148162474, 0.16270243116171965, 0.18419471525010608, 0.30788696663720266, 0.4812829153878348, 0.7577934889566331, 0.2604563179470244, 0.36485346254095974, 0.7825815103140222, 1.3347263859539498, 2.168102888833909, 0.15061255863734654, 0.1982314529873076, 0.36191155513127643, 0.5852860098793393, 0.7440934635344005, 0.4948988273030236, 0.9257406876200721]
worst_inference_speeds= [1.9969995021820068, 1.9779984951019287, 2.057000160217285, 1.8570001125335693, 2.6440017223358154, 0.6099998950958252, 0.3490004539489746, 2.0370004177093506, 2.816997766494751, 0.5239999294281006, 0.1680004596710205, 3.3814072608947754, 2.9385104179382324, 3.3595077991485596, 3.823509693145752, 4.539067983627319, 3.1432266235351562, 0.5580019950866699, 1.033001184463501, 1.5633089542388916, 7.2852160930633545, 3.268512725830078, 3.3655169010162354, 3.959514856338501, 4.32851767539978, 4.544516086578369, 3.4925150871276855, 4.214514970779419]
total_detections= [129, 97, 99, 187, 217, 136, 128, 190, 211, 83, 97.86267876625061, 89.55094814300537, 94.16013956069946, 95.83572149276733, 95.45990824699402, 96.57882452011108, 89.94278907775879, 93.81153583526611, 96.59476280212402, 96.98338508605957, 96.82453870773315, 89.87798690795898, 96.51697874069214, 96.09609842300415, 96.97030186653137, 96.95289731025696, 95.78874111175537, 96.19094133377075]
total_inferences= [83, 78, 80, 83, 84, 84, 84, 84, 78, 95.25397419929504, 97.86267876625061, 89.55094814300537, 94.16013956069946, 95.83572149276733, 95.45990824699402, 96.57882452011108, 89.94278907775879, 93.81153583526611, 96.59476280212402, 96.98338508605957, 96.82453870773315, 89.87798690795898, 96.51697874069214, 96.09609842300415, 96.97030186653137, 96.95289731025696, 95.78874111175537, 96.19094133377075]



import numpy as np
import matplotlib.pyplot as plt
import matplotlib

# Generate a color map
num_models = len(models)
cmap = matplotlib.colormaps.get_cmap('nipy_spectral')
colors = [cmap(i / num_models) for i in range(num_models)]

# Create a figure and plot with unique colors
plt.figure(figsize=(14, 10))
for i, model in enumerate(models):
    plt.scatter(avg_speeds[i], avg_precisions[i], color=colors[i], label=model, edgecolor='black')

print("DEVICE: CPU (Intel Core i7-14700KF)")
print("SOURCE: data/video/sample.MP4 (1080p60fps Video)")
print("SKIP EVERY 60 FRAMES")
print("TOTALLY PROCESSED FRAMES: 84")
print("COCO Dataset, Classes truck, bus, car, train, bicycle")
print("All >50% except YOLO")
# Axis labels and plot title
plt.xlabel('Average Speed (seconds)')
plt.ylabel('Average Precision (%)')
plt.title('Model Performance: Average Detection Speed x Average Accuracy')
plt.legend(loc='upper left', bbox_to_anchor=(1.05, 1), fontsize='small')
plt.grid(True)
plt.show()

# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_precisions, avg_precisions, worst_precisions])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Precision (%)", "Average Precision (%)", "Worst Precision (%)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

plt.title("Model Precision Metrics")
plt.show()

# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_speeds, avg_speeds, worst_speeds])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Speed (ms)", "Average Speed (ms)", "Worst Speed (ms)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

plt.title("Model Speed Metrics")
plt.show()

# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_inference_speeds, avg_inference_speeds, worst_inference_speeds])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Inference Speed (ms)", "Average Inference Speed (ms)", "Worst Inference Speed (mss)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

plt.title("Model Inference Speed Metrics")
plt.show()





import cv2
import torch
from torchvision.models.detection import *
from torchvision.io.image import read_image
from torchvision.utils import draw_bounding_boxes
from torchvision.transforms.functional import to_pil_image
import torchvision.transforms as transforms
from PIL import Image
import IPython
from IPython.display import display, clear_output
import time
import traceback
import math
from ultralytics import YOLO

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

transform = transforms.Compose([
    #transforms.Resize((300, 300)),
    transforms.ToTensor(),
])

class DetectionModel():
    def __init__(self, concept, backbone, model, weights, reference, color=None):
        self.concept = concept
        self.backbone = backbone
        self.model = model(weights) if concept=="YOLO" else model(weights=weights.DEFAULT)
        self.weights = weights
        self.reference = reference
        self.color = color
        self.detections = []

    def addDetection(self, detection):
        self.detections.append(detection)

    def getDetections(self):
        return self.detections

    def clearDetections(self):
        self.detections = []

    def detectionsCount(self):
        return len(self.detections)

    def inferencesCount(self):
        length = 0
        addedFrames = []
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    addedFrames.append(self.detections[i].frame)
        return len(addedFrames)

    def getAveragePrecision(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].precision
            avg = sum/len(self.detections)
        return  avg.item() if torch.is_tensor(avg) else avg

    def getMaxPrecision(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].precision:
                max=self.detections[i].precision
        return max.item() if torch.is_tensor(max) else max

    def getMinPrecision(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].precision:
                min=self.detections[i].precision
        return 0.0 if min==9999999.99 else min.item() if torch.is_tensor(min) else min

    def getAverageSpeed(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].speed
            avg = sum/len(self.detections)
        return avg

    def getMaxSpeed(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].speed:
                max=self.detections[i].speed
        return max

    def getMinSpeed(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].speed:
                min=self.detections[i].speed
        return 0.0 if min==9999999.99 else min

    def getAverageInferenceSpeed(self):
        sum=0.0
        avg=0.0
        addedFrames=[]
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    sum+=self.detections[i].speed
                    addedFrames.append(self.detections[i].frame)
            avg = sum/len(addedFrames) if len(addedFrames)>0 else 0.0
        return avg

    def getMaxInferenceSpeed(self):
        max=0.0
        addedFrames=[]
        for i in range(len(self.detections)):
            if max<self.detections[i].speed and self.detections[i].frame not in addedFrames:
                max=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return max

    def getMinInferenceSpeed(self):
        min=9999999.99
        addedFrames=[]
        for i in range(len(self.detections)):
            if min>self.detections[i].speed and self.detections[i].frame not in addedFrames:
                min=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return 0.0 if min==9999999.99 else min

    def filterPrecisionsBelow(self, minPrecision):
        newDetections = []
        for i in range(len(self.detections)):
            if self.detections[i].precision>=minPrecision:
                newDetections.append(self.detections[i])
        self.detections = newDetections

    def printResults(self):
        print("Model:", self.reference,
              "\nBest Precision:", self.getMaxPrecision(),
              "%\nAverage Precision:", self.getAveragePrecision(),
              "%\nWorst Precision:", self.getMinPrecision(),
              "%\nBest Speed:", self.getMinSpeed(),
              "seconds\nAverage Speed:", self.getAverageSpeed(),
              "seconds\nWorst Speed:", self.getMaxSpeed(),
              "seconds\nBest Inference Speed:", self.getMinInferenceSpeed(),
              "seconds\nAverage Inference Speed:", self.getAverageInferenceSpeed(),
              "seconds\nWorst Inference Speed:", self.getMaxInferenceSpeed(),
              "seconds\nTotal Detections:", self.detectionsCount(),
              "\nTotal Inferences:", self.inferencesCount(),
              "\n\n")

class DetectionResult():
    def __init__(self, model, label, precision, speed, device=None, source=None, frame=None, boundingBox=None, details=None):
        self.model = model
        self.label = label
        self.precision = precision.item() if torch.is_tensor(precision) else precision
        self.speed = speed.item() if torch.is_tensor(speed) else speed
        self.device = device
        self.source = source
        self.frame = frame
        self.boundingBox = boundingBox
        self.details = details

target_classes = ["truck", "bus", "car", "train", "bicycle"]


VEHICLE_DETECTION_MODELS = [
    DetectionModel("FCOS", "resnet50_fpn", fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights, "fcos_resnet", (255,0,0)),
    DetectionModel("RetinaNet", "resnet50_fpn", retinanet_resnet50_fpn, RetinaNet_ResNet50_FPN_Weights, "retinanet_resnet", (255,255,0)),
    DetectionModel("RetinaNet", "resnet50_fpn_v2", retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights, "retinanet_resnet_v2", (0,234,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn", fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights, "faster_rcnn_resnet", (170,0,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn_v2", fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights, "faster_rcnn_resnet_v2", (255,127,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_fpn", fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights, "faster_rcnn_mobilenet_v3", (191,255,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_320_fpn", fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights, "faster_rcnn_mobilenet_v3_320", (0,149,255)),
    DetectionModel("MaskRCNN", "resnet50_fpn", maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights, "mask_rcnn_resnet", (255,0,170)),
    DetectionModel("MaskRCNN", "resnet50_fpn_v2", maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights, "mask_rcnn_resnet_v2", (255,212,0)),
    DetectionModel("SSD300", "vgg16", ssd300_vgg16, SSD300_VGG16_Weights, "ssd_vgg16", (106,255,0)),
    DetectionModel("SSDLite320", "mobilenet_v3_large", ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights, "ssd_mobilenet_v3", (0,64,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5nu.pt', "yolov5nu", (237,185,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5su.pt', "yolov5su", (185,215,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5mu.pt', "yolov5mu", (231,233,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5lu.pt', "yolov5lu", (220,185,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5xu.pt', "yolov5xu", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5n6u.pt', "yolov5n6u", (143,35,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5s6u.pt', "yolov5s6u", (35,98,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5m6u.pt', "yolov5m6u", (143,106,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5l6u.pt', "yolov5l6u", (107,35,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5x6u.pt', "yolov5x6u", (79,143,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8n.pt', "yolov8n", (0,0,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8s.pt', "yolov8s", (115,115,115)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8m.pt', "yolov8m", (204,204,204)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8l.pt', "yolov8l", (255,0,0)), # cycle restarts
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8x.pt', "yolov8x", (255,255,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9c.pt', "yolov9c", (0,234,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9e.pt', "yolov9e", (170,0,255))
]

def process(frame, frameIndex=-1, source=None):
    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        processed_frame = frame.copy()

        if currentModel.concept=="YOLO":
            start_time = time.time()
            results = currentModel.model(processed_frame, agnostic_nms=True, verbose=False)[0]
            processing_time = time.time() - start_time
            for result in results:
                detection_count = result.boxes.shape[0]
                for j in range(detection_count):
                    cls = int(result.boxes.cls[j].item())
                    name = result.names[cls]
                    confidence = float(result.boxes.conf[j].item())
                    bounding_box = result.boxes.xyxy[j].cpu().numpy()
                    if name in target_classes and confidence>0.5:
                        x1, y1, x2, y2 = [int(x) for x in bounding_box]
                        # Ensure x1 < x2 and y1 < y2
                        x1, x2 = min(x1, x2), max(x1, x2)
                        y1, y2 = min(y1, y2), max(y1, y2)
                        label = '{} ({:.2f}%) ({:.3f}s) {}'.format(name.upper(), confidence*100, processing_time, currentModel.reference)
                        print(label)
                        VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, name, confidence*100, processing_time, device, source, frameIndex, bounding_box, label))
                        cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                        cv2.putText(processed_frame, label, (x1+10, y1+25), 0, 0.8, currentModel.color, 2)
        else:
            weights = currentModel.weights.DEFAULT
            model = currentModel.model
            model.to(device)
            model.eval()

            # Convert frame to RGB and PIL Image, then apply transformation
            frame_rgb = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            image = transform(pil_image).unsqueeze(0).to(device)

            # Object detection
            start_time = time.time()
            with torch.no_grad():
                predictions = model(image)

            # Processing time
            processing_time = time.time() - start_time

            # Visualization
            scores = predictions[0]['scores']
            boxes = predictions[0]['boxes']
            labels = predictions[0]['labels']
            confidence_threshold = 0.5

            for score, box, label in zip(scores, boxes, labels):
                # and score > confidence_threshold
                if weights.meta["categories"][label.item()] in target_classes and score > confidence_threshold:
                    x1, y1, x2, y2 = map(int, box)
                    label_text = f'{weights.meta["categories"][label.item()].upper()} ({score*100:.2f}%) ({processing_time:.3f}s) {currentModel.reference}'
                    print(label_text)
                    VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, weights.meta["categories"][label.item()], float(score.item())*100, processing_time, device, source, frameIndex, box, label_text))
                    cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                    cv2.putText(processed_frame, label_text, (x1, y1+20), 0, 0.8, currentModel.color, 2)

        img = Image.fromarray(processed_frame, 'RGB')
        display(img)
        clear_output(wait=True)
    return frame

def handleFrame(frame, frameIndex=-1, source=None):
    processed_frame = process(frame,frameIndex,source)

def handleRelease():
    cam.release()
    print("Source released.")

    print(device)
    print(source)

    BestPrecision=None
    BestAvgPrecision=None
    WorstPrecision=None
    BestSpeed=None
    BestAvgSpeed=None
    WorstSpeed=None

    Models=[]
    BestPrecisions=[]
    AvgPrecisions=[]
    WorstPrecisions=[]
    BestSpeeds=[]
    AvgSpeeds=[]
    WorstSpeeds=[]
    BestInferenceSpeeds=[]
    AvgInferenceSpeeds=[]
    WorstInferenceSpeeds=[]
    TotalDetectionsArr=[]
    TotalInferencesArr=[]

    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        currentModel.printResults()

        Models.append(currentModel.reference)
        BestPrecisions.append(currentModel.getMaxPrecision())
        AvgPrecisions.append(currentModel.getAveragePrecision())
        WorstPrecisions.append(currentModel.getMinPrecision())
        BestSpeeds.append(currentModel.getMinSpeed())
        AvgSpeeds.append(currentModel.getAverageSpeed())
        WorstSpeeds.append(currentModel.getMaxSpeed())
        BestInferenceSpeeds.append(currentModel.getMinInferenceSpeed())
        AvgInferenceSpeeds.append(currentModel.getAverageInferenceSpeed())
        WorstInferenceSpeeds.append(currentModel.getMaxInferenceSpeed())
        TotalDetectionsArr.append(currentModel.detectionsCount())
        TotalInferencesArr.append(currentModel.inferencesCount())

        if BestPrecision is None or BestPrecision.getMaxPrecision()<=currentModel.getMaxPrecision():
            BestPrecision = currentModel
        if BestAvgPrecision is None or BestAvgPrecision.getAveragePrecision()<=currentModel.getAveragePrecision():
            BestAvgPrecision = currentModel
        if WorstPrecision is None or WorstPrecision.getMinPrecision()>=currentModel.getMinPrecision():
            WorstPrecision = currentModel
        if BestSpeed is None or BestSpeed.getMinSpeed()>=currentModel.getMinSpeed():
            BestSpeed = currentModel
        if BestAvgSpeed is None or BestAvgSpeed.getAverageSpeed()>=currentModel.getAverageSpeed():
            BestAvgSpeed = currentModel
        if WorstSpeed is None or WorstSpeed.getMaxSpeed()<=currentModel.getMaxSpeed():
            WorstSpeed = currentModel

    if BestPrecision is not None:
        print("Best Precision:")
        BestPrecision.printResults()
    if BestAvgPrecision is not None:
        print("Best Average Precision:")
        BestAvgPrecision.printResults()
    if WorstPrecision is not None:
        print("Worst Precision:")
        WorstPrecision.printResults()
    if BestSpeed is not None:
        print("Best Speed:")
        BestSpeed.printResults()
    if BestAvgSpeed is not None:
        print("Best Average Speed:")
        BestAvgSpeed.printResults()
    if WorstSpeed is not None:
        print("Worst Speed:")
        WorstSpeed.printResults()

    print("models=",Models)
    print("best_precisions=",BestPrecisions)
    print("avg_precisions=",AvgPrecisions)
    print("worst_precisions=",WorstPrecisions)
    print("best_speeds=",BestSpeeds)
    print("avg_speeds=",AvgSpeeds)
    print("worst_speeds=",WorstSpeeds)
    print("best_inference_speeds=",BestInferenceSpeeds)
    print("avg_inference_speeds=",AvgInferenceSpeeds)
    print("worst_inference_speeds=",WorstInferenceSpeeds)
    print("total_detections=",TotalDetectionsArr)
    print("total_inferences=",TotalInferencesArr)

inputType="video"
source = "data/video/sample.MP4" if inputType=="video" else 0
cam = cv2.VideoCapture(source)
display_handle=display(None, display_id=True)
every = 60

start = 0 if source!=0 else -1
end = int(cam.get(cv2.CAP_PROP_FRAME_COUNT)) if source!=0 else -1
if source!=0: cam.set(1, start)

frameCount=start
while_safety=0
saved_count=0

if not cam.isOpened(): print("Error: Could not open source.")
else:
    try:
        while True if end<0 else frameCount<end:
            _, frame = cam.read()
            if frame is None:
                if source!=0:
                    if while_safety > 2000: break
                    while_safety += 1
                    continue
                else:
                    print("Error: Could not capture frame.")
                    cam.release()
                    break

            if every>0:
                if (frameCount+1)%math.floor(every) == 0:
                    while_safety = 0
                    handleFrame(frame, frameCount, source)
            else:
                while_safety=0
                handleFrame(frame)
            frameCount += 1
        handleRelease()
    except KeyboardInterrupt:
        handleRelease()
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Source could not be released.")
        except:
            pass
        traceback.print_exc()


Source released.
cpu
data/video/sample.MP4
Model: fcos_resnet
Best Precision: 84.01497006416321 %
Average Precision: 65.37111498588739 %
Worst Precision: 50.03466010093689 %
Best Speed: 1.2720437049865723 seconds
Average Speed: 1.4949889700542125 seconds
Worst Speed: 2.1510627269744873 seconds
Best Inference Speed: 1.2720437049865723 seconds
Average Inference Speed: 1.4883765542363545 seconds
Worst Inference Speed: 2.1510627269744873 seconds
Total Detections: 129
Total Inferences: 83


Model: retinanet_resnet
Best Precision: 86.20967268943787 %
Average Precision: 69.96585425642348 %
Worst Precision: 51.02284550666809 %
Best Speed: 1.4448177814483643 seconds
Average Speed: 1.6377926929709838 seconds
Worst Speed: 1.8269693851470947 seconds
Best Inference Speed: 1.4448177814483643 seconds
Average Inference Speed: 1.6396800676981609 seconds
Worst Inference Speed: 1.8269693851470947 seconds
Total Detections: 97
Total Inferences: 78


Model: retinanet_resnet_v2
Best Precision: 96.10866904258728 %
Average Precision: 76.44101748562822 %
Worst Precision: 50.60317516326904 %
Best Speed: 1.399595022201538 seconds
Average Speed: 1.6253563996517297 seconds
Worst Speed: 1.7923152446746826 seconds
Best Inference Speed: 1.399595022201538 seconds
Average Inference Speed: 1.6254408210515976 seconds
Worst Inference Speed: 1.7923152446746826 seconds
Total Detections: 99
Total Inferences: 80


Model: faster_rcnn_resnet
Best Precision: 99.24747347831726 %
Average Precision: 79.92927298188847 %
Worst Precision: 50.00736713409424 %
Best Speed: 1.4460835456848145 seconds
Average Speed: 1.5897577000174292 seconds
Worst Speed: 1.7271442413330078 seconds
Best Inference Speed: 1.4460835456848145 seconds
Average Inference Speed: 1.57893981991044 seconds
Worst Inference Speed: 1.7271442413330078 seconds
Total Detections: 187
Total Inferences: 83


Model: faster_rcnn_resnet_v2
Best Precision: 99.60418939590454 %
Average Precision: 80.5721729032455 %
Worst Precision: 50.113362073898315 %
Best Speed: 2.1240551471710205 seconds
Average Speed: 2.304647766500025 seconds
Worst Speed: 2.563425302505493 seconds
Best Inference Speed: 2.1240551471710205 seconds
Average Inference Speed: 2.295697598230271 seconds
Worst Inference Speed: 2.563425302505493 seconds
Total Detections: 217
Total Inferences: 84


Model: faster_rcnn_mobilenet_v3
Best Precision: 99.35287833213806 %
Average Precision: 78.78277043209357 %
Worst Precision: 50.14830827713013 %
Best Speed: 0.3063955307006836 seconds
Average Speed: 0.3926812410354614 seconds
Worst Speed: 0.5385572910308838 seconds
Best Inference Speed: 0.3063955307006836 seconds
Average Inference Speed: 0.39175031582514447 seconds
Worst Inference Speed: 0.5385572910308838 seconds
Total Detections: 136
Total Inferences: 84


Model: faster_rcnn_mobilenet_v3_320
Best Precision: 98.90456199645996 %
Average Precision: 78.05265253409743 %
Worst Precision: 52.156418561935425 %
Best Speed: 0.13848137855529785 seconds
Average Speed: 0.1891824509948492 seconds
Worst Speed: 0.28697919845581055 seconds
Best Inference Speed: 0.13848137855529785 seconds
Average Inference Speed: 0.19077127951162834 seconds
Worst Inference Speed: 0.28697919845581055 seconds
Total Detections: 128
Total Inferences: 81


Model: mask_rcnn_resnet
Best Precision: 98.711097240448 %
Average Precision: 78.70424458855076 %
Worst Precision: 50.028592348098755 %
Best Speed: 1.524181842803955 seconds
Average Speed: 1.6876290158221596 seconds
Worst Speed: 1.9129271507263184 seconds
Best Inference Speed: 1.524181842803955 seconds
Average Inference Speed: 1.6773213545481365 seconds
Worst Inference Speed: 1.9129271507263184 seconds
Total Detections: 190
Total Inferences: 84


Model: mask_rcnn_resnet_v2
Best Precision: 99.41349625587463 %
Average Precision: 78.58292045186481 %
Worst Precision: 50.22304058074951 %
Best Speed: 2.180260419845581 seconds
Average Speed: 2.4327406871940287 seconds
Worst Speed: 2.857478618621826 seconds
Best Inference Speed: 2.180260419845581 seconds
Average Inference Speed: 2.4373938185828075 seconds
Worst Inference Speed: 2.857478618621826 seconds
Total Detections: 211
Total Inferences: 84


Model: ssd_vgg16
Best Precision: 95.25397419929504 %
Average Precision: 78.78161410251296 %
Worst Precision: 51.88506245613098 %
Best Speed: 0.23613309860229492 seconds
Average Speed: 0.31705104873841067 seconds
Worst Speed: 0.42488861083984375 seconds
Best Inference Speed: 0.23613309860229492 seconds
Average Inference Speed: 0.31731603695796085 seconds
Worst Inference Speed: 0.42488861083984375 seconds
Total Detections: 83
Total Inferences: 78


Model: ssd_mobilenet_v3
Best Precision: 97.86267876625061 %
Average Precision: 78.83799374103546 %
Worst Precision: 52.02542543411255 %
Best Speed: 0.10332202911376953 seconds
Average Speed: 0.12003087754152258 seconds
Worst Speed: 0.19156908988952637 seconds
Best Inference Speed: 0.10332202911376953 seconds
Average Inference Speed: 0.11947197799223015 seconds
Worst Inference Speed: 0.19156908988952637 seconds
Total Detections: 98
Total Inferences: 83


Model: yolov5nu
Best Precision: 89.55094814300537 %
Average Precision: 70.05796279662695 %
Worst Precision: 51.16927623748779 %
Best Speed: 0.08222007751464844 seconds
Average Speed: 0.10888811563834166 seconds
Worst Speed: 0.1825263500213623 seconds
Best Inference Speed: 0.08222007751464844 seconds
Average Inference Speed: 0.10882100993639802 seconds
Worst Inference Speed: 0.1825263500213623 seconds
Total Detections: 78
Total Inferences: 73


Model: yolov5su
Best Precision: 94.16013956069946 %
Average Precision: 74.8091949991984 %
Worst Precision: 51.45619511604309 %
Best Speed: 0.11613249778747559 seconds
Average Speed: 0.18591181219440617 seconds
Worst Speed: 3.022650718688965 seconds
Best Inference Speed: 0.11613249778747559 seconds
Average Inference Speed: 0.18933773040771484 seconds
Worst Inference Speed: 3.022650718688965 seconds
Total Detections: 73
Total Inferences: 67


Model: yolov5mu
Best Precision: 95.83572149276733 %
Average Precision: 75.29266642969708 %
Worst Precision: 50.144755840301514 %
Best Speed: 0.19797611236572266 seconds
Average Speed: 0.31989358469497325 seconds
Worst Speed: 3.325303077697754 seconds
Best Inference Speed: 0.19797611236572266 seconds
Average Inference Speed: 0.32346053703411204 seconds
Worst Inference Speed: 3.325303077697754 seconds
Total Detections: 86
Total Inferences: 74


Model: yolov5lu
Best Precision: 95.45990824699402 %
Average Precision: 79.60579587767522 %
Worst Precision: 50.27608871459961 %
Best Speed: 0.3309202194213867 seconds
Average Speed: 0.48138875265916187 seconds
Worst Speed: 3.7562239170074463 seconds
Best Inference Speed: 0.3309202194213867 seconds
Average Inference Speed: 0.48505886395772296 seconds
Worst Inference Speed: 3.7562239170074463 seconds
Total Detections: 96
Total Inferences: 84


Model: yolov5xu
Best Precision: 96.57882452011108 %
Average Precision: 82.98254124820232 %
Worst Precision: 51.19180083274841 %
Best Speed: 0.562084436416626 seconds
Average Speed: 0.7432504718502363 seconds
Worst Speed: 4.245865345001221 seconds
Best Inference Speed: 0.562084436416626 seconds
Average Inference Speed: 0.7481199332645961 seconds
Worst Inference Speed: 4.245865345001221 seconds
Total Detections: 96
Total Inferences: 84


Model: yolov5n6u
Best Precision: 89.94278907775879 %
Average Precision: 72.58705219752352 %
Worst Precision: 51.13379955291748 %
Best Speed: 0.1697080135345459 seconds
Average Speed: 0.2570595103250423 seconds
Worst Speed: 3.1746139526367188 seconds
Best Inference Speed: 0.1697080135345459 seconds
Average Inference Speed: 0.25913474179696344 seconds
Worst Inference Speed: 3.1746139526367188 seconds
Total Detections: 71
Total Inferences: 69


Model: yolov5s6u
Best Precision: 93.81153583526611 %
Average Precision: 81.56128210967846 %
Worst Precision: 50.647759437561035 %
Best Speed: 0.2895989418029785 seconds
Average Speed: 0.3691249027680815 seconds
Worst Speed: 0.4763762950897217 seconds
Best Inference Speed: 0.2895989418029785 seconds
Average Inference Speed: 0.36834600097254705 seconds
Worst Inference Speed: 0.4763762950897217 seconds
Total Detections: 89
Total Inferences: 76


Model: yolov5m6u
Best Precision: 96.59476280212402 %
Average Precision: 79.9654956091018 %
Worst Precision: 50.96123814582825 %
Best Speed: 0.6143221855163574 seconds
Average Speed: 0.7486283869970413 seconds
Worst Speed: 0.9879467487335205 seconds
Best Inference Speed: 0.6143221855163574 seconds
Average Inference Speed: 0.7479389045811906 seconds
Worst Inference Speed: 0.9879467487335205 seconds
Total Detections: 105
Total Inferences: 79


Model: yolov5l6u
Best Precision: 96.98338508605957 %
Average Precision: 81.12766295671463 %
Worst Precision: 54.25065755844116 %
Best Speed: 1.0677080154418945 seconds
Average Speed: 1.2959490226847785 seconds
Worst Speed: 1.6681768894195557 seconds
Best Inference Speed: 1.0677080154418945 seconds
Average Inference Speed: 1.2984245240688324 seconds
Worst Inference Speed: 1.6681768894195557 seconds
Total Detections: 112
Total Inferences: 80


Model: yolov5x6u
Best Precision: 96.82453870773315 %
Average Precision: 83.55558562491622 %
Worst Precision: 51.130348443984985 %
Best Speed: 1.8273804187774658 seconds
Average Speed: 2.0421804764441083 seconds
Worst Speed: 2.3031165599823 seconds
Best Inference Speed: 1.8273804187774658 seconds
Average Inference Speed: 2.0422085332281794 seconds
Worst Inference Speed: 2.3031165599823 seconds
Total Detections: 112
Total Inferences: 81


Model: yolov8n
Best Precision: 89.87798690795898 %
Average Precision: 74.6314052078459 %
Worst Precision: 50.188612937927246 %
Best Speed: 0.07887768745422363 seconds
Average Speed: 0.14441520306799147 seconds
Worst Speed: 3.2307446002960205 seconds
Best Inference Speed: 0.07887768745422363 seconds
Average Inference Speed: 0.14748788230559406 seconds
Worst Inference Speed: 3.2307446002960205 seconds
Total Detections: 72
Total Inferences: 68


Model: yolov8s
Best Precision: 96.51697874069214 %
Average Precision: 79.01419977987966 %
Worst Precision: 50.04092454910278 %
Best Speed: 0.11524343490600586 seconds
Average Speed: 0.18739666990054551 seconds
Worst Speed: 3.3558781147003174 seconds
Best Inference Speed: 0.11524343490600586 seconds
Average Inference Speed: 0.19226905776233208 seconds
Worst Inference Speed: 3.3558781147003174 seconds
Total Detections: 93
Total Inferences: 82


Model: yolov8m
Best Precision: 96.09609842300415 %
Average Precision: 82.52582819231095 %
Worst Precision: 51.17872357368469 %
Best Speed: 0.21206974983215332 seconds
Average Speed: 0.31889999297357374 seconds
Worst Speed: 3.6732428073883057 seconds
Best Inference Speed: 0.21206974983215332 seconds
Average Inference Speed: 0.3261521293456296 seconds
Worst Inference Speed: 3.6732428073883057 seconds
Total Detections: 93
Total Inferences: 83


Model: yolov8l
Best Precision: 96.97030186653137 %
Average Precision: 84.48680088571881 %
Worst Precision: 53.085654973983765 %
Best Speed: 0.34369421005249023 seconds
Average Speed: 0.5172231197357178 seconds
Worst Speed: 3.9640824794769287 seconds
Best Inference Speed: 0.34369421005249023 seconds
Average Inference Speed: 0.5184700977371399 seconds
Worst Inference Speed: 3.9640824794769287 seconds
Total Detections: 92
Total Inferences: 83


Model: yolov8x
Best Precision: 96.95289731025696 %
Average Precision: 87.62376296638263 %
Worst Precision: 56.22338652610779 %
Best Speed: 0.5109748840332031 seconds
Average Speed: 0.6785352820217019 seconds
Worst Speed: 4.547873020172119 seconds
Best Inference Speed: 0.5109748840332031 seconds
Average Inference Speed: 0.6834358345894587 seconds
Worst Inference Speed: 4.547873020172119 seconds
Total Detections: 101
Total Inferences: 84


Model: yolov9c
Best Precision: 95.78874111175537 %
Average Precision: 82.92970513042651 %
Worst Precision: 52.18837857246399 %
Best Speed: 0.3110494613647461 seconds
Average Speed: 0.4290722972468326 seconds
Worst Speed: 3.924682378768921 seconds
Best Inference Speed: 0.3110494613647461 seconds
Average Inference Speed: 0.43184173107147217 seconds
Worst Inference Speed: 3.924682378768921 seconds
Total Detections: 95
Total Inferences: 84


Model: yolov9e
Best Precision: 96.19094133377075 %
Average Precision: 87.14067172078254 %
Worst Precision: 51.280707120895386 %
Best Speed: 0.6172654628753662 seconds
Average Speed: 0.828186729579296 seconds
Worst Speed: 4.319556713104248 seconds
Best Inference Speed: 0.6172654628753662 seconds
Average Inference Speed: 0.8299128299667722 seconds
Worst Inference Speed: 4.319556713104248 seconds
Total Detections: 103
Total Inferences: 84


Best Precision:
Model: faster_rcnn_resnet_v2
Best Precision: 99.60418939590454 %
Average Precision: 80.5721729032455 %
Worst Precision: 50.113362073898315 %
Best Speed: 2.1240551471710205 seconds
Average Speed: 2.304647766500025 seconds
Worst Speed: 2.563425302505493 seconds
Best Inference Speed: 2.1240551471710205 seconds
Average Inference Speed: 2.295697598230271 seconds
Worst Inference Speed: 2.563425302505493 seconds
Total Detections: 217
Total Inferences: 84


Best Average Precision:
Model: yolov8x
Best Precision: 96.95289731025696 %
Average Precision: 87.62376296638263 %
Worst Precision: 56.22338652610779 %
Best Speed: 0.5109748840332031 seconds
Average Speed: 0.6785352820217019 seconds
Worst Speed: 4.547873020172119 seconds
Best Inference Speed: 0.5109748840332031 seconds
Average Inference Speed: 0.6834358345894587 seconds
Worst Inference Speed: 4.547873020172119 seconds
Total Detections: 101
Total Inferences: 84


Worst Precision:
Model: faster_rcnn_resnet
Best Precision: 99.24747347831726 %
Average Precision: 79.92927298188847 %
Worst Precision: 50.00736713409424 %
Best Speed: 1.4460835456848145 seconds
Average Speed: 1.5897577000174292 seconds
Worst Speed: 1.7271442413330078 seconds
Best Inference Speed: 1.4460835456848145 seconds
Average Inference Speed: 1.57893981991044 seconds
Worst Inference Speed: 1.7271442413330078 seconds
Total Detections: 187
Total Inferences: 83


Best Speed:
Model: yolov8n
Best Precision: 89.87798690795898 %
Average Precision: 74.6314052078459 %
Worst Precision: 50.188612937927246 %
Best Speed: 0.07887768745422363 seconds
Average Speed: 0.14441520306799147 seconds
Worst Speed: 3.2307446002960205 seconds
Best Inference Speed: 0.07887768745422363 seconds
Average Inference Speed: 0.14748788230559406 seconds
Worst Inference Speed: 3.2307446002960205 seconds
Total Detections: 72
Total Inferences: 68


Best Average Speed:
Model: yolov5nu
Best Precision: 89.55094814300537 %
Average Precision: 70.05796279662695 %
Worst Precision: 51.16927623748779 %
Best Speed: 0.08222007751464844 seconds
Average Speed: 0.10888811563834166 seconds
Worst Speed: 0.1825263500213623 seconds
Best Inference Speed: 0.08222007751464844 seconds
Average Inference Speed: 0.10882100993639802 seconds
Worst Inference Speed: 0.1825263500213623 seconds
Total Detections: 78
Total Inferences: 73


Worst Speed:
Model: yolov8x
Best Precision: 96.95289731025696 %
Average Precision: 87.62376296638263 %
Worst Precision: 56.22338652610779 %
Best Speed: 0.5109748840332031 seconds
Average Speed: 0.6785352820217019 seconds
Worst Speed: 4.547873020172119 seconds
Best Inference Speed: 0.5109748840332031 seconds
Average Inference Speed: 0.6834358345894587 seconds
Worst Inference Speed: 4.547873020172119 seconds
Total Detections: 101
Total Inferences: 84


models= ['fcos_resnet', 'retinanet_resnet', 'retinanet_resnet_v2', 'faster_rcnn_resnet', 'faster_rcnn_resnet_v2', 'faster_rcnn_mobilenet_v3', 'faster_rcnn_mobilenet_v3_320', 'mask_rcnn_resnet', 'mask_rcnn_resnet_v2', 'ssd_vgg16', 'ssd_mobilenet_v3', 'yolov5nu', 'yolov5su', 'yolov5mu', 'yolov5lu', 'yolov5xu', 'yolov5n6u', 'yolov5s6u', 'yolov5m6u', 'yolov5l6u', 'yolov5x6u', 'yolov8n', 'yolov8s', 'yolov8m', 'yolov8l', 'yolov8x', 'yolov9c', 'yolov9e']
best_precisions= [84.01497006416321, 86.20967268943787, 96.10866904258728, 99.24747347831726, 99.60418939590454, 99.35287833213806, 98.90456199645996, 98.711097240448, 99.41349625587463, 95.25397419929504, 97.86267876625061, 89.55094814300537, 94.16013956069946, 95.83572149276733, 95.45990824699402, 96.57882452011108, 89.94278907775879, 93.81153583526611, 96.59476280212402, 96.98338508605957, 96.82453870773315, 89.87798690795898, 96.51697874069214, 96.09609842300415, 96.97030186653137, 96.95289731025696, 95.78874111175537, 96.19094133377075]
avg_precisions= [65.37111498588739, 69.96585425642348, 76.44101748562822, 79.92927298188847, 80.5721729032455, 78.78277043209357, 78.05265253409743, 78.70424458855076, 78.58292045186481, 78.78161410251296, 78.83799374103546, 70.05796279662695, 74.8091949991984, 75.29266642969708, 79.60579587767522, 82.98254124820232, 72.58705219752352, 81.56128210967846, 79.9654956091018, 81.12766295671463, 83.55558562491622, 74.6314052078459, 79.01419977987966, 82.52582819231095, 84.48680088571881, 87.62376296638263, 82.92970513042651, 87.14067172078254]
worst_precisions= [50.03466010093689, 51.02284550666809, 50.60317516326904, 50.00736713409424, 50.113362073898315, 50.14830827713013, 52.156418561935425, 50.028592348098755, 50.22304058074951, 51.88506245613098, 52.02542543411255, 51.16927623748779, 51.45619511604309, 50.144755840301514, 50.27608871459961, 51.19180083274841, 51.13379955291748, 50.647759437561035, 50.96123814582825, 54.25065755844116, 51.130348443984985, 50.188612937927246, 50.04092454910278, 51.17872357368469, 53.085654973983765, 56.22338652610779, 52.18837857246399, 51.280707120895386]
best_speeds= [1.2720437049865723, 1.4448177814483643, 1.399595022201538, 1.4460835456848145, 2.1240551471710205, 0.3063955307006836, 0.13848137855529785, 1.524181842803955, 2.180260419845581, 0.23613309860229492, 0.10332202911376953, 0.08222007751464844, 0.11613249778747559, 0.19797611236572266, 0.3309202194213867, 0.562084436416626, 0.1697080135345459, 0.2895989418029785, 0.6143221855163574, 1.0677080154418945, 1.8273804187774658, 0.07887768745422363, 0.11524343490600586, 0.21206974983215332, 0.34369421005249023, 0.5109748840332031, 0.3110494613647461, 0.6172654628753662]
avg_speeds= [1.4949889700542125, 1.6377926929709838, 1.6253563996517297, 1.5897577000174292, 2.304647766500025, 0.3926812410354614, 0.1891824509948492, 1.6876290158221596, 2.4327406871940287, 0.31705104873841067, 0.12003087754152258, 0.10888811563834166, 0.18591181219440617, 0.31989358469497325, 0.48138875265916187, 0.7432504718502363, 0.2570595103250423, 0.3691249027680815, 0.7486283869970413, 1.2959490226847785, 2.0421804764441083, 0.14441520306799147, 0.18739666990054551, 0.31889999297357374, 0.5172231197357178, 0.6785352820217019, 0.4290722972468326, 0.828186729579296]
worst_speeds= [2.1510627269744873, 1.8269693851470947, 1.7923152446746826, 1.7271442413330078, 2.563425302505493, 0.5385572910308838, 0.28697919845581055, 1.9129271507263184, 2.857478618621826, 0.42488861083984375, 0.19156908988952637, 0.1825263500213623, 3.022650718688965, 3.325303077697754, 3.7562239170074463, 4.245865345001221, 3.1746139526367188, 0.4763762950897217, 0.9879467487335205, 1.6681768894195557, 2.3031165599823, 3.2307446002960205, 3.3558781147003174, 3.6732428073883057, 3.9640824794769287, 4.547873020172119, 3.924682378768921, 4.319556713104248]
best_inference_speeds= [1.2720437049865723, 1.4448177814483643, 1.399595022201538, 1.4460835456848145, 2.1240551471710205, 0.3063955307006836, 0.13848137855529785, 1.524181842803955, 2.180260419845581, 0.23613309860229492, 0.10332202911376953, 0.08222007751464844, 0.11613249778747559, 0.19797611236572266, 0.3309202194213867, 0.562084436416626, 0.1697080135345459, 0.2895989418029785, 0.6143221855163574, 1.0677080154418945, 1.8273804187774658, 0.07887768745422363, 0.11524343490600586, 0.21206974983215332, 0.34369421005249023, 0.5109748840332031, 0.3110494613647461, 0.6172654628753662]
avg_inference_speeds= [1.4883765542363545, 1.6396800676981609, 1.6254408210515976, 1.57893981991044, 2.295697598230271, 0.39175031582514447, 0.19077127951162834, 1.6773213545481365, 2.4373938185828075, 0.31731603695796085, 0.11947197799223015, 0.10882100993639802, 0.18933773040771484, 0.32346053703411204, 0.48505886395772296, 0.7481199332645961, 0.25913474179696344, 0.36834600097254705, 0.7479389045811906, 1.2984245240688324, 2.0422085332281794, 0.14748788230559406, 0.19226905776233208, 0.3261521293456296, 0.5184700977371399, 0.6834358345894587, 0.43184173107147217, 0.8299128299667722]
worst_inference_speeds= [2.1510627269744873, 1.8269693851470947, 1.7923152446746826, 1.7271442413330078, 2.563425302505493, 0.5385572910308838, 0.28697919845581055, 1.9129271507263184, 2.857478618621826, 0.42488861083984375, 0.19156908988952637, 0.1825263500213623, 3.022650718688965, 3.325303077697754, 3.7562239170074463, 4.245865345001221, 3.1746139526367188, 0.4763762950897217, 0.9879467487335205, 1.6681768894195557, 2.3031165599823, 3.2307446002960205, 3.3558781147003174, 3.6732428073883057, 3.9640824794769287, 4.547873020172119, 3.924682378768921, 4.319556713104248]
total_detections= [129, 97, 99, 187, 217, 136, 128, 190, 211, 83, 98, 78, 73, 86, 96, 96, 71, 89, 105, 112, 112, 72, 93, 93, 92, 101, 95, 103]
total_inferences= [83, 78, 80, 83, 84, 84, 81, 84, 84, 78, 83, 73, 67, 74, 84, 84, 69, 76, 79, 80, 81, 68, 82, 83, 83, 84, 84, 84]


models= ['fcos_resnet', 'retinanet_resnet', 'retinanet_resnet_v2', 'faster_rcnn_resnet', 'faster_rcnn_resnet_v2', 'faster_rcnn_mobilenet_v3', 'faster_rcnn_mobilenet_v3_320', 'mask_rcnn_resnet', 'mask_rcnn_resnet_v2', 'ssd_vgg16', 'ssd_mobilenet_v3', 'yolov5nu', 'yolov5su', 'yolov5mu', 'yolov5lu', 'yolov5xu', 'yolov5n6u', 'yolov5s6u', 'yolov5m6u', 'yolov5l6u', 'yolov5x6u', 'yolov8n', 'yolov8s', 'yolov8m', 'yolov8l', 'yolov8x', 'yolov9c', 'yolov9e']
best_precisions= [84.01497006416321, 86.20967268943787, 96.10866904258728, 99.24747347831726, 99.60418939590454, 99.35287833213806, 98.90456199645996, 98.711097240448, 99.41349625587463, 95.25397419929504, 97.86267876625061, 89.55094814300537, 94.16013956069946, 95.83572149276733, 95.45990824699402, 96.57882452011108, 89.94278907775879, 93.81153583526611, 96.59476280212402, 96.98338508605957, 96.82453870773315, 89.87798690795898, 96.51697874069214, 96.09609842300415, 96.97030186653137, 96.95289731025696, 95.78874111175537, 96.19094133377075]
avg_precisions= [65.37111498588739, 69.96585425642348, 76.44101748562822, 79.92927298188847, 80.5721729032455, 78.78277043209357, 78.05265253409743, 78.70424458855076, 78.58292045186481, 78.78161410251296, 78.83799374103546, 70.05796279662695, 74.8091949991984, 75.29266642969708, 79.60579587767522, 82.98254124820232, 72.58705219752352, 81.56128210967846, 79.9654956091018, 81.12766295671463, 83.55558562491622, 74.6314052078459, 79.01419977987966, 82.52582819231095, 84.48680088571881, 87.62376296638263, 82.92970513042651, 87.14067172078254]
worst_precisions= [50.03466010093689, 51.02284550666809, 50.60317516326904, 50.00736713409424, 50.113362073898315, 50.14830827713013, 52.156418561935425, 50.028592348098755, 50.22304058074951, 51.88506245613098, 52.02542543411255, 51.16927623748779, 51.45619511604309, 50.144755840301514, 50.27608871459961, 51.19180083274841, 51.13379955291748, 50.647759437561035, 50.96123814582825, 54.25065755844116, 51.130348443984985, 50.188612937927246, 50.04092454910278, 51.17872357368469, 53.085654973983765, 56.22338652610779, 52.18837857246399, 51.280707120895386]
best_speeds= [1.2720437049865723, 1.4448177814483643, 1.399595022201538, 1.4460835456848145, 2.1240551471710205, 0.3063955307006836, 0.13848137855529785, 1.524181842803955, 2.180260419845581, 0.23613309860229492, 0.10332202911376953, 0.08222007751464844, 0.11613249778747559, 0.19797611236572266, 0.3309202194213867, 0.562084436416626, 0.1697080135345459, 0.2895989418029785, 0.6143221855163574, 1.0677080154418945, 1.8273804187774658, 0.07887768745422363, 0.11524343490600586, 0.21206974983215332, 0.34369421005249023, 0.5109748840332031, 0.3110494613647461, 0.6172654628753662]
avg_speeds= [1.4949889700542125, 1.6377926929709838, 1.6253563996517297, 1.5897577000174292, 2.304647766500025, 0.3926812410354614, 0.1891824509948492, 1.6876290158221596, 2.4327406871940287, 0.31705104873841067, 0.12003087754152258, 0.10888811563834166, 0.18591181219440617, 0.31989358469497325, 0.48138875265916187, 0.7432504718502363, 0.2570595103250423, 0.3691249027680815, 0.7486283869970413, 1.2959490226847785, 2.0421804764441083, 0.14441520306799147, 0.18739666990054551, 0.31889999297357374, 0.5172231197357178, 0.6785352820217019, 0.4290722972468326, 0.828186729579296]
worst_speeds= [2.1510627269744873, 1.8269693851470947, 1.7923152446746826, 1.7271442413330078, 2.563425302505493, 0.5385572910308838, 0.28697919845581055, 1.9129271507263184, 2.857478618621826, 0.42488861083984375, 0.19156908988952637, 0.1825263500213623, 3.022650718688965, 3.325303077697754, 3.7562239170074463, 4.245865345001221, 3.1746139526367188, 0.4763762950897217, 0.9879467487335205, 1.6681768894195557, 2.3031165599823, 3.2307446002960205, 3.3558781147003174, 3.6732428073883057, 3.9640824794769287, 4.547873020172119, 3.924682378768921, 4.319556713104248]
best_inference_speeds= [1.2720437049865723, 1.4448177814483643, 1.399595022201538, 1.4460835456848145, 2.1240551471710205, 0.3063955307006836, 0.13848137855529785, 1.524181842803955, 2.180260419845581, 0.23613309860229492, 0.10332202911376953, 0.08222007751464844, 0.11613249778747559, 0.19797611236572266, 0.3309202194213867, 0.562084436416626, 0.1697080135345459, 0.2895989418029785, 0.6143221855163574, 1.0677080154418945, 1.8273804187774658, 0.07887768745422363, 0.11524343490600586, 0.21206974983215332, 0.34369421005249023, 0.5109748840332031, 0.3110494613647461, 0.6172654628753662]
avg_inference_speeds= [1.4883765542363545, 1.6396800676981609, 1.6254408210515976, 1.57893981991044, 2.295697598230271, 0.39175031582514447, 0.19077127951162834, 1.6773213545481365, 2.4373938185828075, 0.31731603695796085, 0.11947197799223015, 0.10882100993639802, 0.18933773040771484, 0.32346053703411204, 0.48505886395772296, 0.7481199332645961, 0.25913474179696344, 0.36834600097254705, 0.7479389045811906, 1.2984245240688324, 2.0422085332281794, 0.14748788230559406, 0.19226905776233208, 0.3261521293456296, 0.5184700977371399, 0.6834358345894587, 0.43184173107147217, 0.8299128299667722]
worst_inference_speeds= [2.1510627269744873, 1.8269693851470947, 1.7923152446746826, 1.7271442413330078, 2.563425302505493, 0.5385572910308838, 0.28697919845581055, 1.9129271507263184, 2.857478618621826, 0.42488861083984375, 0.19156908988952637, 0.1825263500213623, 3.022650718688965, 3.325303077697754, 3.7562239170074463, 4.245865345001221, 3.1746139526367188, 0.4763762950897217, 0.9879467487335205, 1.6681768894195557, 2.3031165599823, 3.2307446002960205, 3.3558781147003174, 3.6732428073883057, 3.9640824794769287, 4.547873020172119, 3.924682378768921, 4.319556713104248]
total_detections= [129, 97, 99, 187, 217, 136, 128, 190, 211, 83, 98, 78, 73, 86, 96, 96, 71, 89, 105, 112, 112, 72, 93, 93, 92, 101, 95, 103]
total_inferences= [83, 78, 80, 83, 84, 84, 81, 84, 84, 78, 83, 73, 67, 74, 84, 84, 69, 76, 79, 80, 81, 68, 82, 83, 83, 84, 84, 84]


import numpy as np
import matplotlib.pyplot as plt
import matplotlib

# Generate a color map
num_models = len(models)
cmap = matplotlib.colormaps.get_cmap('nipy_spectral')
colors = [cmap(i / num_models) for i in range(num_models)]

# Create a figure and plot with unique colors
plt.figure(figsize=(14, 10))
for i, model in enumerate(models):
    plt.scatter(avg_speeds[i], avg_precisions[i], color=colors[i], label=model, edgecolor='black')

print("DEVICE: CPU (Intel Core i7-14700KF)")
print("SOURCE: data/video/sample.MP4 (1080p60fps Video)")
print("SKIP EVERY 60 FRAMES")
print("TOTALLY PROCESSED FRAMES: 84")
print("COCO Dataset, Classes truck, bus, car, train, bicycle")
print("ALL >50%")
# Axis labels and plot title
plt.xlabel('Average Speed (seconds)')
plt.ylabel('Average Precision (%)')
plt.title('Model Performance: Average Detection Speed x Average Accuracy')
plt.legend(loc='upper left', bbox_to_anchor=(1.05, 1), fontsize='small')
plt.grid(True)
plt.show()

# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_precisions, avg_precisions, worst_precisions])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Precision (%)", "Average Precision (%)", "Worst Precision (%)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

plt.title("Model Precision Metrics")
plt.show()

# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_speeds, avg_speeds, worst_speeds])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Speed (s)", "Average Speed (s)", "Worst Speed (s)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

plt.title("Model Speed Metrics")
plt.show()

# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_inference_speeds, avg_inference_speeds, worst_inference_speeds])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Inference Speed (s)", "Average Inference Speed (s)", "Worst Inference Speed (s)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

plt.title("Model Inference Speed Metrics")
plt.show()


import cv2
import torch
from torchvision.models.detection import *
from torchvision.io.image import read_image
from torchvision.utils import draw_bounding_boxes
from torchvision.transforms.functional import to_pil_image
import torchvision.transforms as transforms
from PIL import Image
import IPython
from IPython.display import display, clear_output
import time
import traceback
import math
from ultralytics import YOLO

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

transform = transforms.Compose([
    #transforms.Resize((300, 300)),
    transforms.ToTensor(),
])

class DetectionModel():
    def __init__(self, concept, backbone, model, weights, reference, color=None):
        self.concept = concept
        self.backbone = backbone
        self.model = model(weights, device=device) if concept=="YOLO" else model(weights=weights.DEFAULT)
        self.weights = weights if concept=="YOLO" else weights.DEFAULT
        self.reference = reference
        self.color = color
        self.detections = []

        if concept!="YOLO":
            model.to(device)
            model.eval()

    def addDetection(self, detection):
        self.detections.append(detection)

    def getDetections(self):
        return self.detections

    def clearDetections(self):
        self.detections = []

    def detectionsCount(self):
        return len(self.detections)

    def inferencesCount(self):
        length = 0
        addedFrames = []
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    addedFrames.append(self.detections[i].frame)
        return len(addedFrames)

    def getAveragePrecision(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].precision
            avg = sum/len(self.detections)
        return  avg.item() if torch.is_tensor(avg) else avg

    def getMaxPrecision(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].precision:
                max=self.detections[i].precision
        return max.item() if torch.is_tensor(max) else max

    def getMinPrecision(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].precision:
                min=self.detections[i].precision
        return 0.0 if min==9999999.99 else min.item() if torch.is_tensor(min) else min

    def getAverageSpeed(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].speed
            avg = sum/len(self.detections)
        return avg

    def getMaxSpeed(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].speed:
                max=self.detections[i].speed
        return max

    def getMinSpeed(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].speed:
                min=self.detections[i].speed
        return 0.0 if min==9999999.99 else min

    def getAverageInferenceSpeed(self):
        sum=0.0
        avg=0.0
        addedFrames=[]
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    sum+=self.detections[i].speed
                    addedFrames.append(self.detections[i].frame)
            avg = sum/len(addedFrames) if len(addedFrames)>0 else 0.0
        return avg

    def getMaxInferenceSpeed(self):
        max=0.0
        addedFrames=[]
        for i in range(len(self.detections)):
            if max<self.detections[i].speed and self.detections[i].frame not in addedFrames:
                max=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return max

    def getMinInferenceSpeed(self):
        min=9999999.99
        addedFrames=[]
        for i in range(len(self.detections)):
            if min>self.detections[i].speed and self.detections[i].frame not in addedFrames:
                min=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return 0.0 if min==9999999.99 else min

    def filterPrecisionsBelow(self, minPrecision):
        newDetections = []
        for i in range(len(self.detections)):
            if self.detections[i].precision>=minPrecision:
                newDetections.append(self.detections[i])
        self.detections = newDetections

    def printResults(self):
        print("Model:", self.reference,
              "\nBest Precision:", self.getMaxPrecision(),
              "%\nAverage Precision:", self.getAveragePrecision(),
              "%\nWorst Precision:", self.getMinPrecision(),
              "%\nBest Speed:", self.getMinSpeed(),
              "seconds\nAverage Speed:", self.getAverageSpeed(),
              "seconds\nWorst Speed:", self.getMaxSpeed(),
              "seconds\nBest Inference Speed:", self.getMinInferenceSpeed(),
              "seconds\nAverage Inference Speed:", self.getAverageInferenceSpeed(),
              "seconds\nWorst Inference Speed:", self.getMaxInferenceSpeed(),
              "seconds\nTotal Detections:", self.detectionsCount(),
              "\nTotal Inferences:", self.inferencesCount(),
              "\n\n")

class DetectionResult():
    def __init__(self, model, label, precision, speed, device=None, source=None, frame=None, boundingBox=None, details=None):
        self.model = model
        self.label = label
        self.precision = precision.item() if torch.is_tensor(precision) else precision
        self.speed = speed.item() if torch.is_tensor(speed) else speed
        self.device = device
        self.source = source
        self.frame = frame
        self.boundingBox = boundingBox
        self.details = details

class Experiment():
    def __init__(self, name, description, params, device, source, every, minimum_precision = 0.0, minimum_yolo_precision = 0.0, speed_unit = "seconds", models = None):
        self.name = name
        self.deccription = description
        self.params = params
        self.device = device
        self.source = source
        self.every = every

        self.minimum_precision = minimum_precision
        self.minimum_yolo_precision = minimum_yolo_precision
        self.speed_unit = speed_unit

        self.models = models if models not None else ['fcos_resnet', 'retinanet_resnet', 'retinanet_resnet_v2', 'faster_rcnn_resnet',
                                                      'faster_rcnn_resnet_v2', 'faster_rcnn_mobilenet_v3', 'faster_rcnn_mobilenet_v3_320', 'mask_rcnn_resnet',
                                                      'mask_rcnn_resnet_v2', 'ssd_vgg16', 'ssd_mobilenet_v3', 'yolov5nu',
                                                      'yolov5su', 'yolov5mu', 'yolov5lu', 'yolov5xu',
                                                      'yolov5n6u', 'yolov5s6u', 'yolov5m6u', 'yolov5l6u',
                                                      'yolov5x6u', 'yolov8n', 'yolov8s', 'yolov8m',
                                                      'yolov8l', 'yolov8x', 'yolov9c', 'yolov9e']
        self.best_precisions = []
        self.average_precisions = []
        self.worst_precisions = []
        self.best_speeds = []
        self.average_speeds = []
        self.worst_speeds = []
        self.best_inference_speeds = []
        self.average_inference_speeds = []
        self.worst_inference_speeds = []
        self.total_detections = []
        self.total_inferences = []

    def addExperimentData(self, best_precisions, average_precisions, worst_precisions,
                          best_speeds, average_speeds, worst_speeds,
                          best_inference_speeds, average_inference_speeds, worst_inference_speeds,
                          total_detections, total_inferences):
        self.best_precisions = best_precisions
        self.average_precisions = average_precisions
        self.worst_precisions = worst_precisions
        self.best_speeds = best_speeds
        self.average_speeds = average_speeds
        self.worst_speeds = worst_speeds
        self.best_inference_speeds = best_inference_speeds
        self.average_inference_speeds = average_inference_speeds
        self.worst_inference_speeds = worst_inference_speeds
        self.total_detections = total_detections
        self.total_inferences = total_inferences

    def printExperiment(self):
        print("models =", self.models)
        print("best_precisions =", self.best_precisions)
        print("avg_precisions =", self.average_precisions)
        print("worst_precisions =", self.worst_precisions)
        print("best_speeds =", self.best_speeds)
        print("avg_speeds =",self.average_speeds)
        print("worst_speeds =",self.worst_speeds)
        print("best_inference_speeds =",self.best_inference_speeds)
        print("avg_inference_speeds =",self.average_inference_speeds)
        print("worst_inference_speeds =",self.worst_inference_speeds)
        print("speed_unit", self.speed_unit)
        print("total_detections =",self.total_detections)
        print("total_inferences =",self.total_inferences)

target_classes = ["truck", "bus", "car", "train", "bicycle"]

EXPERIMENTS = [
    Experiment("Experiment 1", "All confidences", "0.0", device, "data/video/sample.MP4", 60),
    Experiment("Experiment 2", "All confidences above 50%, except YOLO", "0.5", device, "data/video/sample.MP4", 60, 0.5),
    Experiment("Experiment 3", "All confidences above 50%", "0.5", device, "data/video/sample.MP4", 60, 0.5, 0.5),
    Experiment("Experiment 4", "All confidences above 70%", "0.7", device, "data/video/sample.MP4", 60, 0.7, 0.7),
]

# EXPERIMENTS = [
#     Experiment("Experiment 1", "All confidences", "0.0", device, "data/video/vecteezy_car-and-truck-traffic-on-the-highway-in-europe-poland_7957364.MP4", 30),
#     Experiment("Experiment 2", "All confidences above 50%, except YOLO", "0.5", device, "data/video/vecteezy_car-and-truck-traffic-on-the-highway-in-europe-poland_7957364.MP4", 30, 0.5),
#     Experiment("Experiment 3", "All confidences above 50%", "0.5", device, "data/video/vecteezy_car-and-truck-traffic-on-the-highway-in-europe-poland_7957364.MP4", 30, 0.5, 0.5),
#     Experiment("Experiment 4", "All confidences above 70%", "0.7", device, "data/video/vecteezy_car-and-truck-traffic-on-the-highway-in-europe-poland_7957364.MP4", 30, 0.7, 0.7),
# ]

VEHICLE_DETECTION_MODELS = [
    DetectionModel("FCOS", "resnet50_fpn", fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights, "fcos_resnet", (255,0,0)),
    DetectionModel("RetinaNet", "resnet50_fpn", retinanet_resnet50_fpn, RetinaNet_ResNet50_FPN_Weights, "retinanet_resnet", (255,255,0)),
    DetectionModel("RetinaNet", "resnet50_fpn_v2", retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights, "retinanet_resnet_v2", (0,234,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn", fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights, "faster_rcnn_resnet", (170,0,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn_v2", fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights, "faster_rcnn_resnet_v2", (255,127,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_fpn", fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights, "faster_rcnn_mobilenet_v3", (191,255,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_320_fpn", fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights, "faster_rcnn_mobilenet_v3_320", (0,149,255)),
    DetectionModel("MaskRCNN", "resnet50_fpn", maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights, "mask_rcnn_resnet", (255,0,170)),
    DetectionModel("MaskRCNN", "resnet50_fpn_v2", maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights, "mask_rcnn_resnet_v2", (255,212,0)),
    DetectionModel("SSD300", "vgg16", ssd300_vgg16, SSD300_VGG16_Weights, "ssd_vgg16", (106,255,0)),
    DetectionModel("SSDLite320", "mobilenet_v3_large", ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights, "ssd_mobilenet_v3", (0,64,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5nu.pt', "yolov5nu", (237,185,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5su.pt', "yolov5su", (185,215,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5mu.pt', "yolov5mu", (231,233,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5lu.pt', "yolov5lu", (220,185,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5xu.pt', "yolov5xu", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5n6u.pt', "yolov5n6u", (143,35,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5s6u.pt', "yolov5s6u", (35,98,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5m6u.pt', "yolov5m6u", (143,106,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5l6u.pt', "yolov5l6u", (107,35,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5x6u.pt', "yolov5x6u", (79,143,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8n.pt', "yolov8n", (0,0,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8s.pt', "yolov8s", (115,115,115)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8m.pt', "yolov8m", (204,204,204)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8l.pt', "yolov8l", (255,0,0)), # cycle restarts
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8x.pt', "yolov8x", (255,255,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9c.pt', "yolov9c", (0,234,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9e.pt', "yolov9e", (170,0,255))
]

def process(frame, frameIndex=-1, source=None):
    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        processed_frame = frame.copy()
        confidence_threshold = 0.5

        if currentModel.concept=="YOLO":
            start_time = time.time()
            results = currentModel.model(processed_frame, agnostic_nms=True, verbose=False)[0]
            processing_time = time.time() - start_time

            for result in results:
                detection_count = result.boxes.shape[0]
                for j in range(detection_count):
                    cls = int(result.boxes.cls[j].item())
                    name = result.names[cls]
                    confidence = float(result.boxes.conf[j].item())
                    bounding_box = result.boxes.xyxy[j].cpu().numpy()
                    if name in target_classes and confidence>confidence_threshold:
                        x1, y1, x2, y2 = [int(x) for x in bounding_box]
                        # Ensure x1 < x2 and y1 < y2
                        x1, x2 = min(x1, x2), max(x1, x2)
                        y1, y2 = min(y1, y2), max(y1, y2)
                        label = '{} ({:.2f}%) ({:.3f}s) {}'.format(name.upper(), confidence*100, processing_time, currentModel.reference)
                        print(label)
                        VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, name, confidence*100, processing_time, device, source, frameIndex, bounding_box, label))
                        cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                        cv2.putText(processed_frame, label, (x1+10, y1+25), 0, 0.8, currentModel.color, 2)
        else:
            # Convert frame to RGB and PIL Image, then apply transformation
            frame_rgb = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            image = transform(pil_image).unsqueeze(0).to(device)

            # Object detection
            start_time = time.time()
            with torch.no_grad():
                predictions = currentModel.model(image)
            processing_time = time.time() - start_time

            # Visualization
            scores = predictions[0]['scores']
            boxes = predictions[0]['boxes']
            labels = predictions[0]['labels']

            for score, box, label in zip(scores, boxes, labels):
                # and score > confidence_threshold
                if weights.meta["categories"][label.item()] in target_classes and score > confidence_threshold:
                    x1, y1, x2, y2 = map(int, box)
                    label_text = f'{weights.meta["categories"][label.item()].upper()} ({score*100:.2f}%) ({processing_time:.3f}s) {currentModel.reference}'
                    print(label_text)
                    VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, weights.meta["categories"][label.item()], float(score.item())*100, processing_time, device, source, frameIndex, box, label_text))
                    cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                    cv2.putText(processed_frame, label_text, (x1, y1+20), 0, 0.8, currentModel.color, 2)

        img = Image.fromarray(processed_frame, 'RGB')
        display(img)
        clear_output(wait=True)
    return frame

def handleFrame(frame, frameIndex=-1, source=None):
    processed_frame = process(frame,frameIndex,source)

def handleRelease():
    cam.release()
    print("Source released.\n")

    print(device)
    print(source)

    BestPrecision=None
    BestAvgPrecision=None
    WorstPrecision=None
    BestSpeed=None
    BestAvgSpeed=None
    WorstSpeed=None

    Models=[]
    BestPrecisions=[]
    AvgPrecisions=[]
    WorstPrecisions=[]
    BestSpeeds=[]
    AvgSpeeds=[]
    WorstSpeeds=[]
    BestInferenceSpeeds=[]
    AvgInferenceSpeeds=[]
    WorstInferenceSpeeds=[]
    TotalDetectionsArr=[]
    TotalInferencesArr=[]

    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        currentModel.printResults()

        Models.append(currentModel.reference)
        BestPrecisions.append(currentModel.getMaxPrecision())
        AvgPrecisions.append(currentModel.getAveragePrecision())
        WorstPrecisions.append(currentModel.getMinPrecision())
        BestSpeeds.append(currentModel.getMinSpeed())
        AvgSpeeds.append(currentModel.getAverageSpeed())
        WorstSpeeds.append(currentModel.getMaxSpeed())
        BestInferenceSpeeds.append(currentModel.getMinInferenceSpeed())
        AvgInferenceSpeeds.append(currentModel.getAverageInferenceSpeed())
        WorstInferenceSpeeds.append(currentModel.getMaxInferenceSpeed())
        TotalDetectionsArr.append(currentModel.detectionsCount())
        TotalInferencesArr.append(currentModel.inferencesCount())

        if BestPrecision is None or BestPrecision.getMaxPrecision()<=currentModel.getMaxPrecision():
            BestPrecision = currentModel
        if BestAvgPrecision is None or BestAvgPrecision.getAveragePrecision()<=currentModel.getAveragePrecision():
            BestAvgPrecision = currentModel
        if WorstPrecision is None or WorstPrecision.getMinPrecision()>=currentModel.getMinPrecision():
            WorstPrecision = currentModel
        if BestSpeed is None or BestSpeed.getMinSpeed()>=currentModel.getMinSpeed():
            BestSpeed = currentModel
        if BestAvgSpeed is None or BestAvgSpeed.getAverageSpeed()>=currentModel.getAverageSpeed():
            BestAvgSpeed = currentModel
        if WorstSpeed is None or WorstSpeed.getMaxSpeed()<=currentModel.getMaxSpeed():
            WorstSpeed = currentModel

    if BestPrecision is not None:
        print("Best Precision:")
        BestPrecision.printResults()
    if BestAvgPrecision is not None:
        print("Best Average Precision:")
        BestAvgPrecision.printResults()
    if WorstPrecision is not None:
        print("Worst Precision:")
        WorstPrecision.printResults()
    if BestSpeed is not None:
        print("Best Speed:")
        BestSpeed.printResults()
    if BestAvgSpeed is not None:
        print("Best Average Speed:")
        BestAvgSpeed.printResults()
    if WorstSpeed is not None:
        print("Worst Speed:")
        WorstSpeed.printResults()

    print("models=",Models)
    print("best_precisions=",BestPrecisions)
    print("avg_precisions=",AvgPrecisions)
    print("worst_precisions=",WorstPrecisions)
    print("best_speeds=",BestSpeeds)
    print("avg_speeds=",AvgSpeeds)
    print("worst_speeds=",WorstSpeeds)
    print("best_inference_speeds=",BestInferenceSpeeds)
    print("avg_inference_speeds=",AvgInferenceSpeeds)
    print("worst_inference_speeds=",WorstInferenceSpeeds)
    print("total_detections=",TotalDetectionsArr)
    print("total_inferences=",TotalInferencesArr)

inputType="video"
source = "data/video/sample.MP4" if inputType=="video" else 0
cam = cv2.VideoCapture(source)
display_handle=display(None, display_id=True)
every = 60

start = 0 if source!=0 else -1
end = int(cam.get(cv2.CAP_PROP_FRAME_COUNT)) if source!=0 else -1
if source!=0: cam.set(1, start)

frameCount=start
while_safety=0
saved_count=0

if not cam.isOpened(): print("Error: Could not open source.")
else:
    try:
        while True if end<0 else frameCount<end:
            _, frame = cam.read()
            if frame is None:
                if source!=0:
                    if while_safety > 2000: break
                    while_safety += 1
                    continue
                else:
                    print("Error: Could not capture frame.")
                    cam.release()
                    break

            if every>0:
                if (frameCount+1)%math.floor(every) == 0:
                    while_safety = 0
                    handleFrame(frame, frameCount, source)
            else:
                while_safety=0
                handleFrame(frame)
            frameCount += 1
        handleRelease()
    except KeyboardInterrupt:
        handleRelease()
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Source could not be released.")
        except:
            pass
        traceback.print_exc()





import cv2
import torch
from torchvision.models.detection import *
from torchvision.io.image import read_image
from torchvision.utils import draw_bounding_boxes
from torchvision.transforms.functional import to_pil_image
import torchvision.transforms as transforms
from PIL import Image
import IPython
from IPython.display import display, clear_output
import time
import traceback
import math
from ultralytics import YOLO

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

transform = transforms.Compose([
    #transforms.Resize((300, 300)),
    transforms.ToTensor(),
])

class DetectionModel():
    def __init__(self, concept, backbone, model, weights, reference, color=None):
        self.concept = concept
        self.backbone = backbone
        self.model = model(weights) if concept=="YOLO" else model(weights=weights.DEFAULT)
        self.weights = weights
        self.reference = reference
        self.color = color
        self.detections = []

    def addDetection(self, detection):
        self.detections.append(detection)

    def getDetections(self):
        return self.detections

    def clearDetections(self):
        self.detections = []

    def detectionsCount(self):
        return len(self.detections)

    def inferencesCount(self):
        length = 0
        addedFrames = []
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    addedFrames.append(self.detections[i].frame)
        return len(addedFrames)

    def getAveragePrecision(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].precision
            avg = sum/len(self.detections)
        return  avg.item() if torch.is_tensor(avg) else avg

    def getMaxPrecision(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].precision:
                max=self.detections[i].precision
        return max.item() if torch.is_tensor(max) else max

    def getMinPrecision(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].precision:
                min=self.detections[i].precision
        return 0.0 if min==9999999.99 else min.item() if torch.is_tensor(min) else min

    def getAverageSpeed(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].speed
            avg = sum/len(self.detections)
        return avg

    def getMaxSpeed(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].speed:
                max=self.detections[i].speed
        return max

    def getMinSpeed(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].speed:
                min=self.detections[i].speed
        return 0.0 if min==9999999.99 else min

    def getAverageInferenceSpeed(self):
        sum=0.0
        avg=0.0
        addedFrames=[]
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    sum+=self.detections[i].speed
                    addedFrames.append(self.detections[i].frame)
            avg = sum/len(addedFrames) if len(addedFrames)>0 else 0.0
        return avg

    def getMaxInferenceSpeed(self):
        max=0.0
        addedFrames=[]
        for i in range(len(self.detections)):
            if max<self.detections[i].speed and self.detections[i].frame not in addedFrames:
                max=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return max

    def getMinInferenceSpeed(self):
        min=9999999.99
        addedFrames=[]
        for i in range(len(self.detections)):
            if min>self.detections[i].speed and self.detections[i].frame not in addedFrames:
                min=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return 0.0 if min==9999999.99 else min

    def filterPrecisionsBelow(self, minPrecision):
        newDetections = []
        for i in range(len(self.detections)):
            if self.detections[i].precision>=minPrecision:
                newDetections.append(self.detections[i])
        self.detections = newDetections

    def printResults(self):
        print("Model:", self.reference,
              "\nBest Precision:", self.getMaxPrecision(),
              "%\nAverage Precision:", self.getAveragePrecision(),
              "%\nWorst Precision:", self.getMinPrecision(),
              "%\nBest Speed:", self.getMinSpeed(),
              "seconds\nAverage Speed:", self.getAverageSpeed(),
              "seconds\nWorst Speed:", self.getMaxSpeed(),
              "seconds\nBest Inference Speed:", self.getMinInferenceSpeed(),
              "seconds\nAverage Inference Speed:", self.getAverageInferenceSpeed(),
              "seconds\nWorst Inference Speed:", self.getMaxInferenceSpeed(),
              "seconds\nTotal Detections:", self.detectionsCount(),
              "\nTotal Inferences:", self.inferencesCount(),
              "\n\n")

class DetectionResult():
    def __init__(self, model, label, precision, speed, device=None, source=None, frame=None, boundingBox=None, details=None):
        self.model = model
        self.label = label
        self.precision = precision.item() if torch.is_tensor(precision) else precision
        self.speed = speed.item() if torch.is_tensor(speed) else speed
        self.device = device
        self.source = source
        self.frame = frame
        self.boundingBox = boundingBox
        self.details = details

target_classes = ["truck", "bus", "car", "train", "bicycle"]


VEHICLE_DETECTION_MODELS = [
    DetectionModel("FCOS", "resnet50_fpn", fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights, "fcos_resnet", (255,0,0)),
    DetectionModel("RetinaNet", "resnet50_fpn", retinanet_resnet50_fpn, RetinaNet_ResNet50_FPN_Weights, "retinanet_resnet", (255,255,0)),
    DetectionModel("RetinaNet", "resnet50_fpn_v2", retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights, "retinanet_resnet_v2", (0,234,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn", fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights, "faster_rcnn_resnet", (170,0,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn_v2", fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights, "faster_rcnn_resnet_v2", (255,127,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_fpn", fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights, "faster_rcnn_mobilenet_v3", (191,255,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_320_fpn", fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights, "faster_rcnn_mobilenet_v3_320", (0,149,255)),
    DetectionModel("MaskRCNN", "resnet50_fpn", maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights, "mask_rcnn_resnet", (255,0,170)),
    DetectionModel("MaskRCNN", "resnet50_fpn_v2", maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights, "mask_rcnn_resnet_v2", (255,212,0)),
    DetectionModel("SSD300", "vgg16", ssd300_vgg16, SSD300_VGG16_Weights, "ssd_vgg16", (106,255,0)),
    DetectionModel("SSDLite320", "mobilenet_v3_large", ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights, "ssd_mobilenet_v3", (0,64,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5nu.pt', "yolov5nu", (237,185,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5su.pt', "yolov5su", (185,215,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5mu.pt', "yolov5mu", (231,233,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5lu.pt', "yolov5lu", (220,185,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5xu.pt', "yolov5xu", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5n6u.pt', "yolov5n6u", (143,35,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5s6u.pt', "yolov5s6u", (35,98,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5m6u.pt', "yolov5m6u", (143,106,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5l6u.pt', "yolov5l6u", (107,35,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5x6u.pt', "yolov5x6u", (79,143,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8n.pt', "yolov8n", (0,0,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8s.pt', "yolov8s", (115,115,115)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8m.pt', "yolov8m", (204,204,204)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8l.pt', "yolov8l", (255,0,0)), # cycle restarts
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8x.pt', "yolov8x", (255,255,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9c.pt', "yolov9c", (0,234,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9e.pt', "yolov9e", (170,0,255))
]

def process(frame, frameIndex=-1, source=None):
    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        processed_frame = frame.copy()

        if currentModel.concept=="YOLO":
            start_time = time.time()
            results = currentModel.model(processed_frame, agnostic_nms=True, verbose=False)[0]
            processing_time = time.time() - start_time
            for result in results:
                detection_count = result.boxes.shape[0]
                for j in range(detection_count):
                    cls = int(result.boxes.cls[j].item())
                    name = result.names[cls]
                    confidence = float(result.boxes.conf[j].item())
                    bounding_box = result.boxes.xyxy[j].cpu().numpy()
                    if name in target_classes and confidence>0.5:
                        x1, y1, x2, y2 = [int(x) for x in bounding_box]
                        # Ensure x1 < x2 and y1 < y2
                        x1, x2 = min(x1, x2), max(x1, x2)
                        y1, y2 = min(y1, y2), max(y1, y2)
                        label = '{} ({:.2f}%) ({:.3f}s) {}'.format(name.upper(), confidence*100, processing_time, currentModel.reference)
                        print(label)
                        VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, name, confidence*100, processing_time, device, source, frameIndex, bounding_box, label))
                        cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                        cv2.putText(processed_frame, label, (x1+10, y1+25), 0, 0.8, currentModel.color, 2)
        else:
            weights = currentModel.weights.DEFAULT
            model = currentModel.model
            model.to(device)
            model.eval()

            # Convert frame to RGB and PIL Image, then apply transformation
            frame_rgb = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            image = transform(pil_image).unsqueeze(0).to(device)

            # Object detection
            start_time = time.time()
            with torch.no_grad():
                predictions = model(image)

            # Processing time
            processing_time = time.time() - start_time

            # Visualization
            scores = predictions[0]['scores']
            boxes = predictions[0]['boxes']
            labels = predictions[0]['labels']
            confidence_threshold = 0.5

            for score, box, label in zip(scores, boxes, labels):
                # and score > confidence_threshold
                if weights.meta["categories"][label.item()] in target_classes and score > confidence_threshold:
                    x1, y1, x2, y2 = map(int, box)
                    label_text = f'{weights.meta["categories"][label.item()].upper()} ({score*100:.2f}%) ({processing_time:.3f}s) {currentModel.reference}'
                    print(label_text)
                    VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, weights.meta["categories"][label.item()], float(score.item())*100, processing_time, device, source, frameIndex, box, label_text))
                    cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                    cv2.putText(processed_frame, label_text, (x1, y1+20), 0, 0.8, currentModel.color, 2)

        img = Image.fromarray(processed_frame, 'RGB')
        display(img)
        clear_output(wait=True)
    return frame

def handleFrame(frame, frameIndex=-1, source=None):
    processed_frame = process(frame,frameIndex,source)

def handleRelease():
    cam.release()
    print("Source released.")

    print(device)
    print(source)

    BestPrecision=None
    BestAvgPrecision=None
    WorstPrecision=None
    BestSpeed=None
    BestAvgSpeed=None
    WorstSpeed=None

    Models=[]
    BestPrecisions=[]
    AvgPrecisions=[]
    WorstPrecisions=[]
    BestSpeeds=[]
    AvgSpeeds=[]
    WorstSpeeds=[]
    BestInferenceSpeeds=[]
    AvgInferenceSpeeds=[]
    WorstInferenceSpeeds=[]
    TotalDetectionsArr=[]
    TotalInferencesArr=[]

    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        currentModel.printResults()

        Models.append(currentModel.reference)
        BestPrecisions.append(currentModel.getMaxPrecision())
        AvgPrecisions.append(currentModel.getAveragePrecision())
        WorstPrecisions.append(currentModel.getMinPrecision())
        BestSpeeds.append(currentModel.getMinSpeed())
        AvgSpeeds.append(currentModel.getAverageSpeed())
        WorstSpeeds.append(currentModel.getMaxSpeed())
        BestInferenceSpeeds.append(currentModel.getMinInferenceSpeed())
        AvgInferenceSpeeds.append(currentModel.getAverageInferenceSpeed())
        WorstInferenceSpeeds.append(currentModel.getMaxInferenceSpeed())
        TotalDetectionsArr.append(currentModel.detectionsCount())
        TotalInferencesArr.append(currentModel.inferencesCount())

        if BestPrecision is None or BestPrecision.getMaxPrecision()<=currentModel.getMaxPrecision():
            BestPrecision = currentModel
        if BestAvgPrecision is None or BestAvgPrecision.getAveragePrecision()<=currentModel.getAveragePrecision():
            BestAvgPrecision = currentModel
        if WorstPrecision is None or WorstPrecision.getMinPrecision()>=currentModel.getMinPrecision():
            WorstPrecision = currentModel
        if BestSpeed is None or BestSpeed.getMinSpeed()>=currentModel.getMinSpeed():
            BestSpeed = currentModel
        if BestAvgSpeed is None or BestAvgSpeed.getAverageSpeed()>=currentModel.getAverageSpeed():
            BestAvgSpeed = currentModel
        if WorstSpeed is None or WorstSpeed.getMaxSpeed()<=currentModel.getMaxSpeed():
            WorstSpeed = currentModel

    if BestPrecision is not None:
        print("Best Precision:")
        BestPrecision.printResults()
    if BestAvgPrecision is not None:
        print("Best Average Precision:")
        BestAvgPrecision.printResults()
    if WorstPrecision is not None:
        print("Worst Precision:")
        WorstPrecision.printResults()
    if BestSpeed is not None:
        print("Best Speed:")
        BestSpeed.printResults()
    if BestAvgSpeed is not None:
        print("Best Average Speed:")
        BestAvgSpeed.printResults()
    if WorstSpeed is not None:
        print("Worst Speed:")
        WorstSpeed.printResults()

    print("models=",Models)
    print("best_precisions=",BestPrecisions)
    print("avg_precisions=",AvgPrecisions)
    print("worst_precisions=",WorstPrecisions)
    print("best_speeds=",BestSpeeds)
    print("avg_speeds=",AvgSpeeds)
    print("worst_speeds=",WorstSpeeds)
    print("best_inference_speeds=",BestInferenceSpeeds)
    print("avg_inference_speeds=",AvgInferenceSpeeds)
    print("worst_inference_speeds=",WorstInferenceSpeeds)
    print("total_detections=",TotalDetectionsArr)
    print("total_inferences=",TotalInferencesArr)

inputType="video"
source = "data/video/vecteezy_car-and-truck-traffic-on-the-highway-in-europe-poland_7957364.MP4" if inputType=="video" else 0
cam = cv2.VideoCapture(source)
display_handle=display(None, display_id=True)
every = 30

start = 0 if source!=0 else -1
end = int(cam.get(cv2.CAP_PROP_FRAME_COUNT)) if source!=0 else -1
if source!=0: cam.set(1, start)

frameCount=start
while_safety=0
saved_count=0

if not cam.isOpened(): print("Error: Could not open source.")
else:
    try:
        while True if end<0 else frameCount<end:
            _, frame = cam.read()
            if frame is None:
                if source!=0:
                    if while_safety > 2000: break
                    while_safety += 1
                    continue
                else:
                    print("Error: Could not capture frame.")
                    cam.release()
                    break

            if every>0:
                if (frameCount+1)%math.floor(every) == 0:
                    while_safety = 0
                    handleFrame(frame, frameCount, source)
            else:
                while_safety=0
                handleFrame(frame)
            frameCount += 1
        handleRelease()
    except KeyboardInterrupt:
        handleRelease()
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Source could not be released.")
        except:
            pass
        traceback.print_exc()


models= ['fcos_resnet', 'retinanet_resnet', 'retinanet_resnet_v2', 'faster_rcnn_resnet', 'faster_rcnn_resnet_v2', 'faster_rcnn_mobilenet_v3', 'faster_rcnn_mobilenet_v3_320', 'mask_rcnn_resnet', 'mask_rcnn_resnet_v2', 'ssd_vgg16', 'ssd_mobilenet_v3', 'yolov5nu', 'yolov5su', 'yolov5mu', 'yolov5lu', 'yolov5xu', 'yolov5n6u', 'yolov5s6u', 'yolov5m6u', 'yolov5l6u', 'yolov5x6u', 'yolov8n', 'yolov8s', 'yolov8m', 'yolov8l', 'yolov8x', 'yolov9c', 'yolov9e']
best_precisions= [88.38133811950684, 96.5210497379303, 95.00588178634644, 99.93738532066345, 99.87070560455322, 99.32159781455994, 99.72949624061584, 99.91301894187927, 99.91182684898376, 98.0389416217804, 98.03178310394287, 91.97649955749512, 94.07822489738464, 96.37896418571472, 95.80461382865906, 97.3269522190094, 94.2396879196167, 95.54755091667175, 96.92941308021545, 97.27439284324646, 96.98181748390198, 95.45491933822632, 95.18426060676575, 96.5444028377533, 96.16449475288391, 96.99693322181702, 96.51097059249878, 97.04502820968628]
avg_precisions= [65.88658039928764, 70.20814386697916, 70.43154467093318, 79.90512614962698, 84.78623870760202, 80.90001817577141, 84.73448807542974, 81.36211392533689, 84.39679010556294, 74.3879671394825, 79.74823500428882, 68.64530959644833, 75.47572281049645, 75.31852468848228, 76.93267973569723, 78.24879854703121, 72.94682849634874, 77.37568999064787, 78.43043037379782, 79.05915665626526, 80.31515035953821, 72.15728767930645, 76.0428299807539, 76.25905139659478, 78.3721436315508, 79.44863706395246, 79.53483669470388, 79.5822869406806]
worst_precisions= [50.043344497680664, 50.141048431396484, 50.0144362449646, 50.187039375305176, 50.11507272720337, 50.00733137130737, 50.4946231842041, 50.41531324386597, 50.11341571807861, 51.834869384765625, 53.01353931427002, 50.18031597137451, 51.69903039932251, 50.16433000564575, 50.14612674713135, 50.73273777961731, 50.56397318840027, 51.35229825973511, 50.405651330947876, 50.280821323394775, 50.68036913871765, 50.467002391815186, 50.88526010513306, 50.05417466163635, 50.525641441345215, 50.45931339263916, 50.20453929901123, 50.40505528450012]
best_speeds= [1.3259952068328857, 1.564000129699707, 1.586963176727295, 1.5199997425079346, 2.2299721240997314, 0.5359997749328613, 0.22500085830688477, 2.335038185119629, 2.9229977130889893, 0.3679986000061035, 0.1589980125427246, 0.1020047664642334, 0.1549997329711914, 0.23100042343139648, 0.4310026168823242, 0.6449992656707764, 0.23600244522094727, 0.3709993362426758, 0.708000898361206, 1.1409988403320312, 1.8659992218017578, 0.09599995613098145, 0.1340346336364746, 0.23799943923950195, 0.42296361923217773, 0.6060004234313965, 0.40496087074279785, 0.7030034065246582]
avg_speeds= [1.4804099195746965, 1.6870004910689134, 1.67791663502392, 1.636231125542944, 2.3290149768193564, 0.588164737401915, 0.26038003834811124, 2.5125611929493097, 3.2404531680620634, 0.4652000069618225, 0.18286178793225968, 0.4873379307824212, 0.48656138129856275, 0.6188481072584788, 0.8468927255043617, 1.146145467277911, 0.5390798293792449, 0.7657689702419834, 1.2142297501365344, 1.7782643795013429, 2.668400798168482, 0.3707209939826025, 0.5037954985493361, 0.6442401796821656, 0.8580088081644542, 0.9582046892331995, 0.7477815827579363, 1.1395536599335847]
worst_speeds= [1.640033483505249, 1.917006492614746, 1.8769664764404297, 1.7729997634887695, 2.4289636611938477, 0.6590015888214111, 0.29900026321411133, 3.22900128364563, 3.4419989585876465, 0.5190005302429199, 0.21799993515014648, 3.807548999786377, 2.913516044616699, 3.203451156616211, 3.7676308155059814, 4.48351526260376, 3.3525142669677734, 3.643548011779785, 4.265509128570557, 5.5136635303497314, 7.199673652648926, 3.064513683319092, 3.102551221847534, 3.597548246383667, 3.900514841079712, 3.9635169506073, 3.3845136165618896, 4.1274800300598145]
best_inference_speeds= [1.3259952068328857, 1.564000129699707, 1.586963176727295, 1.5199997425079346, 2.2299721240997314, 0.5359997749328613, 0.22500085830688477, 2.335038185119629, 2.9229977130889893, 0.3679986000061035, 0.1589980125427246, 0.1020047664642334, 0.1549997329711914, 0.23100042343139648, 0.4310026168823242, 0.6449992656707764, 0.23600244522094727, 0.3709993362426758, 0.708000898361206, 1.1409988403320312, 1.8659992218017578, 0.09599995613098145, 0.1340346336364746, 0.23799943923950195, 0.42296361923217773, 0.6060004234313965, 0.40496087074279785, 0.7030034065246582]
avg_inference_speeds= [1.4791003704071044, 1.6934005260467528, 1.6829955339431764, 1.6389041900634767, 2.332097125053406, 0.5887988805770874, 0.2602996349334717, 2.520694613456726, 3.2421191930770874, 0.45089991092681886, 0.18328547477722168, 0.5067901611328125, 0.48995156288146974, 0.6142479181289673, 0.8655670881271362, 1.1386515378952027, 0.5965444564819335, 0.7422553300857544, 1.15734543800354, 1.7441579341888427, 2.642662525177002, 0.4216449737548828, 0.4964586734771729, 0.6507545948028565, 0.8660467147827149, 1.0017443895339966, 0.7483443021774292, 1.1267481803894044]
worst_inference_speeds= [1.640033483505249, 1.917006492614746, 1.8769664764404297, 1.7729997634887695, 2.4289636611938477, 0.6590015888214111, 0.29900026321411133, 3.22900128364563, 3.4419989585876465, 0.5190005302429199, 0.21799993515014648, 3.807548999786377, 2.913516044616699, 3.203451156616211, 3.7676308155059814, 4.48351526260376, 3.3525142669677734, 3.643548011779785, 4.265509128570557, 5.5136635303497314, 7.199673652648926, 3.064513683319092, 3.102551221847534, 3.597548246383667, 3.900514841079712, 3.9635169506073, 3.3845136165618896, 4.1274800300598145]
total_detections= [186, 130, 152, 261, 288, 121, 55, 262, 260, 40, 14, 74, 92, 120, 130, 139, 111, 131, 192, 175, 191, 73, 99, 123, 134, 138, 141, 135]
total_inferences= [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 7, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]


import numpy as np
import matplotlib.pyplot as plt
import matplotlib

# Generate a color map
num_models = len(models)
cmap = matplotlib.colormaps.get_cmap('nipy_spectral')
colors = [cmap(i / num_models) for i in range(num_models)]

# Create a figure and plot with unique colors
plt.figure(figsize=(14, 10))
for i, model in enumerate(models):
    plt.scatter(avg_speeds[i], avg_precisions[i], color=colors[i], label=model, edgecolor='black')

print("DEVICE: CPU (Intel Core i7-14700KF)")
print("SOURCE: data/video/vecteezy_car-and-truck-traffic-on-the-highway-in-europe-poland_7957364.MP4 (4k30fps Video)")
print("SKIP EVERY 60 FRAMES")
print("TOTALLY PROCESSED FRAMES: 84")
print("COCO Dataset, Classes truck, bus, car, train, bicycle")
print("ALL >50%")
# Axis labels and plot title
plt.xlabel('Average Speed (seconds)')
plt.ylabel('Average Precision (%)')
plt.title('Model Performance: Average Detection Speed x Average Accuracy')
plt.legend(loc='upper left', bbox_to_anchor=(1.05, 1), fontsize='small')
plt.grid(True)
plt.show()

# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_precisions, avg_precisions, worst_precisions])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Precision (%)", "Average Precision (%)", "Worst Precision (%)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

plt.title("Model Precision Metrics")
plt.show()

# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_speeds, avg_speeds, worst_speeds])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Speed (s)", "Average Speed (s)", "Worst Speed (s)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

plt.title("Model Speed Metrics")
plt.show()

# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_inference_speeds, avg_inference_speeds, worst_inference_speeds])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Inference Speed (s)", "Average Inference Speed (s)", "Worst Inference Speed (s)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

plt.title("Model Inference Speed Metrics")
plt.show()





import cv2
import torch
from torchvision.models.detection import *
from torchvision.io.image import read_image
from torchvision.utils import draw_bounding_boxes
from torchvision.transforms.functional import to_pil_image
import torchvision.transforms as transforms
from PIL import Image
import IPython
from IPython.display import display, clear_output
import time
import traceback
import math
from ultralytics import YOLO

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

transform = transforms.Compose([
    #transforms.Resize((300, 300)),
    transforms.ToTensor(),
])

class DetectionModel():
    def __init__(self, concept, backbone, model, weights, reference, color=None):
        self.concept = concept
        self.backbone = backbone
        self.model = model(weights) if concept=="YOLO" else model(weights=weights.DEFAULT)
        self.weights = weights
        self.reference = reference
        self.color = color
        self.detections = []

    def addDetection(self, detection):
        self.detections.append(detection)

    def getDetections(self):
        return self.detections

    def clearDetections(self):
        self.detections = []

    def detectionsCount(self):
        return len(self.detections)

    def inferencesCount(self):
        length = 0
        addedFrames = []
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    addedFrames.append(self.detections[i].frame)
        return len(addedFrames)

    def getAveragePrecision(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].precision
            avg = sum/len(self.detections)
        return  avg.item() if torch.is_tensor(avg) else avg

    def getMaxPrecision(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].precision:
                max=self.detections[i].precision
        return max.item() if torch.is_tensor(max) else max

    def getMinPrecision(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].precision:
                min=self.detections[i].precision
        return 0.0 if min==9999999.99 else min.item() if torch.is_tensor(min) else min

    def getAverageSpeed(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].speed
            avg = sum/len(self.detections)
        return avg

    def getMaxSpeed(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].speed:
                max=self.detections[i].speed
        return max

    def getMinSpeed(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].speed:
                min=self.detections[i].speed
        return 0.0 if min==9999999.99 else min

    def getAverageInferenceSpeed(self):
        sum=0.0
        avg=0.0
        addedFrames=[]
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    sum+=self.detections[i].speed
                    addedFrames.append(self.detections[i].frame)
            avg = sum/len(addedFrames) if len(addedFrames)>0 else 0.0
        return avg

    def getMaxInferenceSpeed(self):
        max=0.0
        addedFrames=[]
        for i in range(len(self.detections)):
            if max<self.detections[i].speed and self.detections[i].frame not in addedFrames:
                max=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return max

    def getMinInferenceSpeed(self):
        min=9999999.99
        addedFrames=[]
        for i in range(len(self.detections)):
            if min>self.detections[i].speed and self.detections[i].frame not in addedFrames:
                min=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return 0.0 if min==9999999.99 else min

    def filterPrecisionsBelow(self, minPrecision):
        newDetections = []
        for i in range(len(self.detections)):
            if self.detections[i].precision>=minPrecision:
                newDetections.append(self.detections[i])
        self.detections = newDetections

    def printResults(self):
        print("Model:", self.reference,
              "\nBest Precision:", self.getMaxPrecision(),
              "%\nAverage Precision:", self.getAveragePrecision(),
              "%\nWorst Precision:", self.getMinPrecision(),
              "%\nBest Speed:", self.getMinSpeed(),
              "seconds\nAverage Speed:", self.getAverageSpeed(),
              "seconds\nWorst Speed:", self.getMaxSpeed(),
              "seconds\nBest Inference Speed:", self.getMinInferenceSpeed(),
              "seconds\nAverage Inference Speed:", self.getAverageInferenceSpeed(),
              "seconds\nWorst Inference Speed:", self.getMaxInferenceSpeed(),
              "seconds\nTotal Detections:", self.detectionsCount(),
              "\nTotal Inferences:", self.inferencesCount(),
              "\n\n")

class DetectionResult():
    def __init__(self, model, label, precision, speed, device=None, source=None, frame=None, boundingBox=None, details=None):
        self.model = model
        self.label = label
        self.precision = precision.item() if torch.is_tensor(precision) else precision
        self.speed = speed.item() if torch.is_tensor(speed) else speed
        self.device = device
        self.source = source
        self.frame = frame
        self.boundingBox = boundingBox
        self.details = details

target_classes = ["truck", "bus", "car", "train", "bicycle"]


VEHICLE_DETECTION_MODELS = [
    DetectionModel("FCOS", "resnet50_fpn", fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights, "fcos_resnet", (255,0,0)),
    DetectionModel("RetinaNet", "resnet50_fpn", retinanet_resnet50_fpn, RetinaNet_ResNet50_FPN_Weights, "retinanet_resnet", (255,255,0)),
    DetectionModel("RetinaNet", "resnet50_fpn_v2", retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights, "retinanet_resnet_v2", (0,234,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn", fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights, "faster_rcnn_resnet", (170,0,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn_v2", fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights, "faster_rcnn_resnet_v2", (255,127,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_fpn", fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights, "faster_rcnn_mobilenet_v3", (191,255,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_320_fpn", fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights, "faster_rcnn_mobilenet_v3_320", (0,149,255)),
    DetectionModel("MaskRCNN", "resnet50_fpn", maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights, "mask_rcnn_resnet", (255,0,170)),
    DetectionModel("MaskRCNN", "resnet50_fpn_v2", maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights, "mask_rcnn_resnet_v2", (255,212,0)),
    DetectionModel("SSD300", "vgg16", ssd300_vgg16, SSD300_VGG16_Weights, "ssd_vgg16", (106,255,0)),
    DetectionModel("SSDLite320", "mobilenet_v3_large", ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights, "ssd_mobilenet_v3", (0,64,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5nu.pt', "yolov5nu", (237,185,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5su.pt', "yolov5su", (185,215,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5mu.pt', "yolov5mu", (231,233,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5lu.pt', "yolov5lu", (220,185,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5xu.pt', "yolov5xu", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5n6u.pt', "yolov5n6u", (143,35,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5s6u.pt', "yolov5s6u", (35,98,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5m6u.pt', "yolov5m6u", (143,106,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5l6u.pt', "yolov5l6u", (107,35,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5x6u.pt', "yolov5x6u", (79,143,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8n.pt', "yolov8n", (0,0,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8s.pt', "yolov8s", (115,115,115)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8m.pt', "yolov8m", (204,204,204)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8l.pt', "yolov8l", (255,0,0)), # cycle restarts
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8x.pt', "yolov8x", (255,255,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9c.pt', "yolov9c", (0,234,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9e.pt', "yolov9e", (170,0,255))
]

def process(frame, frameIndex=-1, source=None):
    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        processed_frame = frame.copy()

        if currentModel.concept=="YOLO":
            start_time = time.time()
            results = currentModel.model(processed_frame, agnostic_nms=True, verbose=False)[0]
            processing_time = time.time() - start_time
            for result in results:
                detection_count = result.boxes.shape[0]
                for j in range(detection_count):
                    cls = int(result.boxes.cls[j].item())
                    name = result.names[cls]
                    confidence = float(result.boxes.conf[j].item())
                    bounding_box = result.boxes.xyxy[j].cpu().numpy()
                    if name in target_classes and confidence>0.5:
                        x1, y1, x2, y2 = [int(x) for x in bounding_box]
                        # Ensure x1 < x2 and y1 < y2
                        x1, x2 = min(x1, x2), max(x1, x2)
                        y1, y2 = min(y1, y2), max(y1, y2)
                        label = '{} ({:.2f}%) ({:.3f}s) {}'.format(name.upper(), confidence*100, processing_time, currentModel.reference)
                        print(label)
                        VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, name, confidence*100, processing_time, device, source, frameIndex, bounding_box, label))
                        cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                        cv2.putText(processed_frame, label, (x1+10, y1+25), 0, 0.8, currentModel.color, 2)
        else:
            weights = currentModel.weights.DEFAULT
            model = currentModel.model
            model.to(device)
            model.eval()

            # Convert frame to RGB and PIL Image, then apply transformation
            frame_rgb = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            image = transform(pil_image).unsqueeze(0).to(device)

            # Object detection
            start_time = time.time()
            with torch.no_grad():
                predictions = model(image)

            # Processing time
            processing_time = time.time() - start_time

            # Visualization
            scores = predictions[0]['scores']
            boxes = predictions[0]['boxes']
            labels = predictions[0]['labels']
            confidence_threshold = 0.5

            for score, box, label in zip(scores, boxes, labels):
                # and score > confidence_threshold
                if weights.meta["categories"][label.item()] in target_classes and score > confidence_threshold:
                    x1, y1, x2, y2 = map(int, box)
                    label_text = f'{weights.meta["categories"][label.item()].upper()} ({score*100:.2f}%) ({processing_time:.3f}s) {currentModel.reference}'
                    print(label_text)
                    VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, weights.meta["categories"][label.item()], float(score.item())*100, processing_time, device, source, frameIndex, box, label_text))
                    cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                    cv2.putText(processed_frame, label_text, (x1, y1+20), 0, 0.8, currentModel.color, 2)

        img = Image.fromarray(processed_frame, 'RGB')
        display(img)
        clear_output(wait=True)
    return frame

def handleFrame(frame, frameIndex=-1, source=None):
    processed_frame = process(frame,frameIndex,source)

def handleRelease():
    cam.release()
    print("Source released.")

    print(device)
    print(source)

    BestPrecision=None
    BestAvgPrecision=None
    WorstPrecision=None
    BestSpeed=None
    BestAvgSpeed=None
    WorstSpeed=None

    Models=[]
    BestPrecisions=[]
    AvgPrecisions=[]
    WorstPrecisions=[]
    BestSpeeds=[]
    AvgSpeeds=[]
    WorstSpeeds=[]
    BestInferenceSpeeds=[]
    AvgInferenceSpeeds=[]
    WorstInferenceSpeeds=[]
    TotalDetectionsArr=[]
    TotalInferencesArr=[]

    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        # currentModel.printResults()

        Models.append(currentModel.reference)
        BestPrecisions.append(currentModel.getMaxPrecision())
        AvgPrecisions.append(currentModel.getAveragePrecision())
        WorstPrecisions.append(currentModel.getMinPrecision())
        BestSpeeds.append(currentModel.getMinSpeed())
        AvgSpeeds.append(currentModel.getAverageSpeed())
        WorstSpeeds.append(currentModel.getMaxSpeed())
        BestInferenceSpeeds.append(currentModel.getMinInferenceSpeed())
        AvgInferenceSpeeds.append(currentModel.getAverageInferenceSpeed())
        WorstInferenceSpeeds.append(currentModel.getMaxInferenceSpeed())
        TotalDetectionsArr.append(currentModel.detectionsCount())
        TotalInferencesArr.append(currentModel.inferencesCount())

        if BestPrecision is None or BestPrecision.getMaxPrecision()<=currentModel.getMaxPrecision():
            BestPrecision = currentModel
        if BestAvgPrecision is None or BestAvgPrecision.getAveragePrecision()<=currentModel.getAveragePrecision():
            BestAvgPrecision = currentModel
        if WorstPrecision is None or WorstPrecision.getMinPrecision()>=currentModel.getMinPrecision():
            WorstPrecision = currentModel
        if BestSpeed is None or BestSpeed.getMinSpeed()>=currentModel.getMinSpeed():
            BestSpeed = currentModel
        if BestAvgSpeed is None or BestAvgSpeed.getAverageSpeed()>=currentModel.getAverageSpeed():
            BestAvgSpeed = currentModel
        if WorstSpeed is None or WorstSpeed.getMaxSpeed()<=currentModel.getMaxSpeed():
            WorstSpeed = currentModel

    if BestPrecision is not None:
        print("Best Precision:")
        BestPrecision.printResults()
    if BestAvgPrecision is not None:
        print("Best Average Precision:")
        BestAvgPrecision.printResults()
    if WorstPrecision is not None:
        print("Worst Precision:")
        WorstPrecision.printResults()
    if BestSpeed is not None:
        print("Best Speed:")
        BestSpeed.printResults()
    if BestAvgSpeed is not None:
        print("Best Average Speed:")
        BestAvgSpeed.printResults()
    if WorstSpeed is not None:
        print("Worst Speed:")
        WorstSpeed.printResults()

    print("models=",Models)
    print("best_precisions=",BestPrecisions)
    print("avg_precisions=",AvgPrecisions)
    print("worst_precisions=",WorstPrecisions)
    print("best_speeds=",BestSpeeds)
    print("avg_speeds=",AvgSpeeds)
    print("worst_speeds=",WorstSpeeds)
    print("best_inference_speeds=",BestInferenceSpeeds)
    print("avg_inference_speeds=",AvgInferenceSpeeds)
    print("worst_inference_speeds=",WorstInferenceSpeeds)
    print("total_detections=",TotalDetectionsArr)
    print("total_inferences=",TotalInferencesArr)

inputType="video"
source = "data/video/2034115-hd_1920_1080_30fps.MP4" if inputType=="video" else 0
cam = cv2.VideoCapture(source)
display_handle=display(None, display_id=True)
every = 30

start = 0 if source!=0 else -1
end = int(cam.get(cv2.CAP_PROP_FRAME_COUNT)) if source!=0 else -1
if source!=0: cam.set(1, start)

frameCount=start
while_safety=0
saved_count=0

if not cam.isOpened(): print("Error: Could not open source.")
else:
    try:
        while True if end<0 else frameCount<end:
            _, frame = cam.read()
            if frame is None:
                if source!=0:
                    if while_safety > 2000: break
                    while_safety += 1
                    continue
                else:
                    print("Error: Could not capture frame.")
                    cam.release()
                    break

            if every>0:
                if (frameCount+1)%math.floor(every) == 0:
                    while_safety = 0
                    handleFrame(frame, frameCount, source)
            else:
                while_safety=0
                handleFrame(frame)
            frameCount += 1
        handleRelease()
    except KeyboardInterrupt:
        handleRelease()
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Source could not be released.")
        except:
            pass
        traceback.print_exc()


models= ['fcos_resnet', 'retinanet_resnet', 'retinanet_resnet_v2', 'faster_rcnn_resnet', 'faster_rcnn_resnet_v2', 'faster_rcnn_mobilenet_v3', 'faster_rcnn_mobilenet_v3_320', 'mask_rcnn_resnet', 'mask_rcnn_resnet_v2', 'ssd_vgg16', 'ssd_mobilenet_v3', 'yolov5nu', 'yolov5su', 'yolov5mu', 'yolov5lu', 'yolov5xu', 'yolov5n6u', 'yolov5s6u', 'yolov5m6u', 'yolov5l6u', 'yolov5x6u', 'yolov8n', 'yolov8s', 'yolov8m', 'yolov8l', 'yolov8x', 'yolov9c', 'yolov9e']
best_precisions= [84.83301997184753, 91.69307351112366, 95.4441785812378, 99.9228835105896, 99.9826729297638, 99.82061386108398, 99.8578667640686, 99.87086057662964, 99.93312358856201, 99.47913885116577, 99.56985712051392, 93.70522499084473, 94.43736672401428, 95.35672068595886, 95.55718898773193, 95.68715691566467, 96.31597399711609, 94.2226231098175, 95.38546800613403, 96.37797474861145, 96.30561470985413, 94.39494609832764, 95.28464078903198, 95.23571133613586, 95.44462561607361, 96.01415395736694, 96.25153541564941, 96.6651976108551]
avg_precisions= [65.8841269995485, 71.22555674401475, 73.83507871514813, 86.84455658679822, 88.96739099840386, 87.90522995424885, 85.2060926383769, 87.1236540860944, 89.62056282966856, 84.4696799311975, 84.35685192385027, 76.05521628554438, 77.53465430190165, 78.89073387700684, 81.06716857400052, 82.04506759622456, 79.49444311005729, 82.7447441394019, 83.24607770680349, 83.53853318534914, 84.0977500422729, 76.1033314355412, 81.14591127634048, 80.57522682865638, 81.92216429087493, 83.3882709881207, 83.31239580558741, 83.2613467378954]
worst_precisions= [50.04851818084717, 50.08883476257324, 50.05960464477539, 50.46467185020447, 50.51953196525574, 50.00492334365845, 50.13507008552551, 50.2067506313324, 51.425570249557495, 51.760393381118774, 50.511109828948975, 50.1340389251709, 50.268709659576416, 50.881582498550415, 50.4483699798584, 50.82172751426697, 50.26649832725525, 50.05990266799927, 52.43103504180908, 51.135653257369995, 50.0474750995636, 51.420825719833374, 51.97308659553528, 50.375568866729736, 50.06181001663208, 50.03392696380615, 52.08258628845215, 50.00579357147217]
best_speeds= [1.2819983959197998, 1.412001609802246, 1.3839972019195557, 1.450033187866211, 2.1810429096221924, 0.4680020809173584, 0.14999723434448242, 1.945995569229126, 2.6919984817504883, 0.2550020217895508, 0.09800028800964355, 0.08100056648254395, 0.11099958419799805, 0.21594023704528809, 0.3229997158050537, 0.5590000152587891, 0.17999982833862305, 0.29199814796447754, 0.6100068092346191, 1.0850000381469727, 1.7430000305175781, 0.07299971580505371, 0.11399650573730469, 0.22499990463256836, 0.3689992427825928, 0.5050008296966553, 0.32199907302856445, 0.5919618606567383]
avg_speeds= [1.4631610912936075, 1.5773446836185046, 1.6289157392854374, 1.5928707832476765, 2.284919011555495, 0.5415219002015601, 0.19399867537458973, 2.116630018531502, 2.840248506031339, 0.34148873223198783, 0.16334378591147802, 0.24704568654718534, 0.24522834892074266, 0.404758341458379, 0.5612616705340009, 0.8238894622937768, 0.33543206112725393, 0.4678006952653551, 0.8618050529843285, 1.3799152032273714, 2.1168192849498415, 0.2094741534002079, 0.2968625545501709, 0.39400601493938087, 0.5855554331530322, 0.7453565372003091, 0.5480135256244291, 0.9410694753174234]
worst_speeds= [2.349998950958252, 1.7779977321624756, 1.8489997386932373, 1.7581818103790283, 2.5146286487579346, 0.6599991321563721, 0.25500059127807617, 2.3431413173675537, 3.4179980754852295, 1.0569994449615479, 2.11299991607666, 3.2836833000183105, 2.950510025024414, 3.2354753017425537, 3.637511730194092, 4.305510520935059, 2.9895126819610596, 3.2525081634521484, 4.339514970779419, 5.3696448802948, 6.942508935928345, 3.3415088653564453, 3.2725086212158203, 3.7065110206604004, 4.291508913040161, 4.656514883041382, 3.996509552001953, 4.2609992027282715]
best_inference_speeds= [1.2819983959197998, 1.412001609802246, 1.3839972019195557, 1.450033187866211, 2.1810429096221924, 0.4680020809173584, 0.14999723434448242, 1.945995569229126, 2.6919984817504883, 0.2550020217895508, 0.09800028800964355, 0.08100056648254395, 0.11099958419799805, 0.21594023704528809, 0.3229997158050537, 0.5590000152587891, 0.17999982833862305, 0.29199814796447754, 0.6100068092346191, 1.0850000381469727, 1.7430000305175781, 0.07299971580505371, 0.11399650573730469, 0.22499990463256836, 0.3689992427825928, 0.5050008296966553, 0.32199907302856445, 0.5919618606567383]
avg_inference_speeds= [1.4655253802027022, 1.5779301864760262, 1.626159770148141, 1.5966418726103646, 2.2862672124590193, 0.5413636565208435, 0.1936060701097761, 2.1184321897370473, 2.8384465064321245, 0.3455299990517752, 0.19178588901247298, 0.24891796282359532, 0.2586242471422468, 0.38726661034992765, 0.5546619892120361, 0.8256608588354928, 0.3180897150720869, 0.46716292415346417, 0.8653393132346017, 1.3898093444960458, 2.124266973563603, 0.20419842004776, 0.27351787260600496, 0.39466223546436857, 0.5860149094036647, 0.7448397704533168, 0.5486297692571368, 0.9758589352880206]
worst_inference_speeds= [2.349998950958252, 1.7779977321624756, 1.8489997386932373, 1.7581818103790283, 2.5146286487579346, 0.6599991321563721, 0.25500059127807617, 2.3431413173675537, 3.4179980754852295, 1.0569994449615479, 2.11299991607666, 3.2836833000183105, 2.950510025024414, 3.2354753017425537, 3.637511730194092, 4.305510520935059, 2.9895126819610596, 3.2525081634521484, 4.339514970779419, 5.3696448802948, 6.942508935928345, 3.3415088653564453, 3.2725086212158203, 3.7065110206604004, 4.291508913040161, 4.656514883041382, 3.996509552001953, 4.2609992027282715]
total_detections= [280, 233, 211, 299, 319, 233, 169, 308, 315, 99, 93, 142, 192, 196, 215, 226, 168, 223, 231, 244, 239, 161, 200, 223, 222, 222, 217, 226]
total_inferences= [28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28]


import numpy as np
import matplotlib.pyplot as plt
import matplotlib

# Generate a color map
num_models = len(models)
cmap = matplotlib.colormaps.get_cmap('nipy_spectral')
colors = [cmap(i / num_models) for i in range(num_models)]

# Create a figure and plot with unique colors
plt.figure(figsize=(14, 10))
for i, model in enumerate(models):
    plt.scatter(avg_speeds[i], avg_precisions[i], color=colors[i], label=model, edgecolor='black')

print("DEVICE: CPU (Intel Core i7-14700KF)")
print("SOURCE: data/video/2034115-hd_1920_1080_30fps.MP4 (4k30fps Video)")
print("SKIP EVERY 60 FRAMES")
print("TOTALLY PROCESSED FRAMES: 28")
print("COCO Dataset, Classes truck, bus, car, train, bicycle")
print("ALL >50%")
# Axis labels and plot title
plt.xlabel('Average Speed (seconds)')
plt.ylabel('Average Precision (%)')
plt.title('Model Performance: Average Detection Speed x Average Accuracy')
plt.legend(loc='upper left', bbox_to_anchor=(1.05, 1), fontsize='small')
plt.grid(True)
plt.show()

# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_precisions, avg_precisions, worst_precisions])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Precision (%)", "Average Precision (%)", "Worst Precision (%)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

plt.title("Model Precision Metrics")
plt.show()

# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_speeds, avg_speeds, worst_speeds])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Speed (s)", "Average Speed (s)", "Worst Speed (s)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

plt.title("Model Speed Metrics")
plt.show()

# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_inference_speeds, avg_inference_speeds, worst_inference_speeds])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Inference Speed (s)", "Average Inference Speed (s)", "Worst Inference Speed (s)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

plt.title("Model Inference Speed Metrics")
plt.show()





import cv2
import torch
from torchvision.models.detection import *
from torchvision.io.image import read_image
from torchvision.utils import draw_bounding_boxes
from torchvision.transforms.functional import to_pil_image
import torchvision.transforms as transforms
from PIL import Image
import IPython
from IPython.display import display, clear_output
import time
import traceback
import math
from ultralytics import YOLO

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

transform = transforms.Compose([
    #transforms.Resize((300, 300)),
    transforms.ToTensor(),
])

class DetectionModel():
    def __init__(self, concept, backbone, model, weights, reference, color=None):
        self.concept = concept
        self.backbone = backbone
        self.model = model(weights) if concept=="YOLO" else model(weights=weights.DEFAULT)
        self.weights = weights
        self.reference = reference
        self.color = color
        self.detections = []

    def addDetection(self, detection):
        self.detections.append(detection)

    def getDetections(self):
        return self.detections

    def clearDetections(self):
        self.detections = []

    def detectionsCount(self):
        return len(self.detections)

    def inferencesCount(self):
        length = 0
        addedFrames = []
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    addedFrames.append(self.detections[i].frame)
        return len(addedFrames)

    def getAveragePrecision(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].precision
            avg = sum/len(self.detections)
        return  avg.item() if torch.is_tensor(avg) else avg

    def getMaxPrecision(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].precision:
                max=self.detections[i].precision
        return max.item() if torch.is_tensor(max) else max

    def getMinPrecision(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].precision:
                min=self.detections[i].precision
        return 0.0 if min==9999999.99 else min.item() if torch.is_tensor(min) else min

    def getAverageSpeed(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].speed
            avg = sum/len(self.detections)
        return avg

    def getMaxSpeed(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].speed:
                max=self.detections[i].speed
        return max

    def getMinSpeed(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].speed:
                min=self.detections[i].speed
        return 0.0 if min==9999999.99 else min

    def getAverageInferenceSpeed(self):
        sum=0.0
        avg=0.0
        addedFrames=[]
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    sum+=self.detections[i].speed
                    addedFrames.append(self.detections[i].frame)
            avg = sum/len(addedFrames) if len(addedFrames)>0 else 0.0
        return avg

    def getMaxInferenceSpeed(self):
        max=0.0
        addedFrames=[]
        for i in range(len(self.detections)):
            if max<self.detections[i].speed and self.detections[i].frame not in addedFrames:
                max=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return max

    def getMinInferenceSpeed(self):
        min=9999999.99
        addedFrames=[]
        for i in range(len(self.detections)):
            if min>self.detections[i].speed and self.detections[i].frame not in addedFrames:
                min=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return 0.0 if min==9999999.99 else min

    def filterPrecisionsBelow(self, minPrecision):
        newDetections = []
        for i in range(len(self.detections)):
            if self.detections[i].precision>=minPrecision:
                newDetections.append(self.detections[i])
        self.detections = newDetections

    def printResults(self):
        print("Model:", self.reference,
              "\nBest Precision:", self.getMaxPrecision(),
              "%\nAverage Precision:", self.getAveragePrecision(),
              "%\nWorst Precision:", self.getMinPrecision(),
              "%\nBest Speed:", self.getMinSpeed(),
              "seconds\nAverage Speed:", self.getAverageSpeed(),
              "seconds\nWorst Speed:", self.getMaxSpeed(),
              "seconds\nBest Inference Speed:", self.getMinInferenceSpeed(),
              "seconds\nAverage Inference Speed:", self.getAverageInferenceSpeed(),
              "seconds\nWorst Inference Speed:", self.getMaxInferenceSpeed(),
              "seconds\nTotal Detections:", self.detectionsCount(),
              "\nTotal Inferences:", self.inferencesCount(),
              "\n\n")

class DetectionResult():
    def __init__(self, model, label, precision, speed, device=None, source=None, frame=None, boundingBox=None, details=None):
        self.model = model
        self.label = label
        self.precision = precision.item() if torch.is_tensor(precision) else precision
        self.speed = speed.item() if torch.is_tensor(speed) else speed
        self.device = device
        self.source = source
        self.frame = frame
        self.boundingBox = boundingBox
        self.details = details

target_classes = ["truck", "bus", "car", "train", "bicycle"]


VEHICLE_DETECTION_MODELS = [
    DetectionModel("FCOS", "resnet50_fpn", fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights, "fcos_resnet", (255,0,0)),
    DetectionModel("RetinaNet", "resnet50_fpn", retinanet_resnet50_fpn, RetinaNet_ResNet50_FPN_Weights, "retinanet_resnet", (255,255,0)),
    DetectionModel("RetinaNet", "resnet50_fpn_v2", retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights, "retinanet_resnet_v2", (0,234,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn", fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights, "faster_rcnn_resnet", (170,0,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn_v2", fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights, "faster_rcnn_resnet_v2", (255,127,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_fpn", fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights, "faster_rcnn_mobilenet_v3", (191,255,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_320_fpn", fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights, "faster_rcnn_mobilenet_v3_320", (0,149,255)),
    DetectionModel("MaskRCNN", "resnet50_fpn", maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights, "mask_rcnn_resnet", (255,0,170)),
    DetectionModel("MaskRCNN", "resnet50_fpn_v2", maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights, "mask_rcnn_resnet_v2", (255,212,0)),
    DetectionModel("SSD300", "vgg16", ssd300_vgg16, SSD300_VGG16_Weights, "ssd_vgg16", (106,255,0)),
    DetectionModel("SSDLite320", "mobilenet_v3_large", ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights, "ssd_mobilenet_v3", (0,64,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5nu.pt', "yolov5nu", (237,185,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5su.pt', "yolov5su", (185,215,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5mu.pt', "yolov5mu", (231,233,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5lu.pt', "yolov5lu", (220,185,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5xu.pt', "yolov5xu", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5n6u.pt', "yolov5n6u", (143,35,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5s6u.pt', "yolov5s6u", (35,98,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5m6u.pt', "yolov5m6u", (143,106,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5l6u.pt', "yolov5l6u", (107,35,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5x6u.pt', "yolov5x6u", (79,143,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8n.pt', "yolov8n", (0,0,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8s.pt', "yolov8s", (115,115,115)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8m.pt', "yolov8m", (204,204,204)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8l.pt', "yolov8l", (255,0,0)), # cycle restarts
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8x.pt', "yolov8x", (255,255,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9c.pt', "yolov9c", (0,234,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9e.pt', "yolov9e", (170,0,255))
]

def process(frame, frameIndex=-1, source=None):
    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        processed_frame = frame.copy()

        if currentModel.concept=="YOLO":
            start_time = time.time()
            results = currentModel.model(processed_frame, agnostic_nms=True, verbose=False)[0]
            processing_time = time.time() - start_time
            for result in results:
                detection_count = result.boxes.shape[0]
                for j in range(detection_count):
                    cls = int(result.boxes.cls[j].item())
                    name = result.names[cls]
                    confidence = float(result.boxes.conf[j].item())
                    bounding_box = result.boxes.xyxy[j].cpu().numpy()
                    if name in target_classes and confidence>0.5:
                        x1, y1, x2, y2 = [int(x) for x in bounding_box]
                        # Ensure x1 < x2 and y1 < y2
                        x1, x2 = min(x1, x2), max(x1, x2)
                        y1, y2 = min(y1, y2), max(y1, y2)
                        label = '{} ({:.2f}%) ({:.3f}s) {}'.format(name.upper(), confidence*100, processing_time, currentModel.reference)
                        print(label)
                        VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, name, confidence*100, processing_time, device, source, frameIndex, bounding_box, label))
                        cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                        cv2.putText(processed_frame, label, (x1+10, y1+25), 0, 0.8, currentModel.color, 2)
        else:
            weights = currentModel.weights.DEFAULT
            model = currentModel.model
            model.to(device)
            model.eval()

            # Convert frame to RGB and PIL Image, then apply transformation
            frame_rgb = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            image = transform(pil_image).unsqueeze(0).to(device)

            # Object detection
            start_time = time.time()
            with torch.no_grad():
                predictions = model(image)

            # Processing time
            processing_time = time.time() - start_time

            # Visualization
            scores = predictions[0]['scores']
            boxes = predictions[0]['boxes']
            labels = predictions[0]['labels']
            confidence_threshold = 0.5

            for score, box, label in zip(scores, boxes, labels):
                # and score > confidence_threshold
                if weights.meta["categories"][label.item()] in target_classes and score > confidence_threshold:
                    x1, y1, x2, y2 = map(int, box)
                    label_text = f'{weights.meta["categories"][label.item()].upper()} ({score*100:.2f}%) ({processing_time:.3f}s) {currentModel.reference}'
                    print(label_text)
                    VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, weights.meta["categories"][label.item()], float(score.item())*100, processing_time, device, source, frameIndex, box, label_text))
                    cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                    cv2.putText(processed_frame, label_text, (x1, y1+20), 0, 0.8, currentModel.color, 2)

        img = Image.fromarray(processed_frame, 'RGB')
        display(img)
        clear_output(wait=True)
    return frame

def handleFrame(frame, frameIndex=-1, source=None):
    processed_frame = process(frame,frameIndex,source)

def handleRelease():
    cam.release()
    print("Source released.")

    print(device)
    print(source)

    BestPrecision=None
    BestAvgPrecision=None
    WorstPrecision=None
    BestSpeed=None
    BestAvgSpeed=None
    WorstSpeed=None

    Models=[]
    BestPrecisions=[]
    AvgPrecisions=[]
    WorstPrecisions=[]
    BestSpeeds=[]
    AvgSpeeds=[]
    WorstSpeeds=[]
    BestInferenceSpeeds=[]
    AvgInferenceSpeeds=[]
    WorstInferenceSpeeds=[]
    TotalDetectionsArr=[]
    TotalInferencesArr=[]

    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        # currentModel.printResults()

        Models.append(currentModel.reference)
        BestPrecisions.append(currentModel.getMaxPrecision())
        AvgPrecisions.append(currentModel.getAveragePrecision())
        WorstPrecisions.append(currentModel.getMinPrecision())
        BestSpeeds.append(currentModel.getMinSpeed())
        AvgSpeeds.append(currentModel.getAverageSpeed())
        WorstSpeeds.append(currentModel.getMaxSpeed())
        BestInferenceSpeeds.append(currentModel.getMinInferenceSpeed())
        AvgInferenceSpeeds.append(currentModel.getAverageInferenceSpeed())
        WorstInferenceSpeeds.append(currentModel.getMaxInferenceSpeed())
        TotalDetectionsArr.append(currentModel.detectionsCount())
        TotalInferencesArr.append(currentModel.inferencesCount())

        if BestPrecision is None or BestPrecision.getMaxPrecision()<=currentModel.getMaxPrecision():
            BestPrecision = currentModel
        if BestAvgPrecision is None or BestAvgPrecision.getAveragePrecision()<=currentModel.getAveragePrecision():
            BestAvgPrecision = currentModel
        if WorstPrecision is None or WorstPrecision.getMinPrecision()>=currentModel.getMinPrecision():
            WorstPrecision = currentModel
        if BestSpeed is None or BestSpeed.getMinSpeed()>=currentModel.getMinSpeed():
            BestSpeed = currentModel
        if BestAvgSpeed is None or BestAvgSpeed.getAverageSpeed()>=currentModel.getAverageSpeed():
            BestAvgSpeed = currentModel
        if WorstSpeed is None or WorstSpeed.getMaxSpeed()<=currentModel.getMaxSpeed():
            WorstSpeed = currentModel

    if BestPrecision is not None:
        print("Best Precision:")
        BestPrecision.printResults()
    if BestAvgPrecision is not None:
        print("Best Average Precision:")
        BestAvgPrecision.printResults()
    if WorstPrecision is not None:
        print("Worst Precision:")
        WorstPrecision.printResults()
    if BestSpeed is not None:
        print("Best Speed:")
        BestSpeed.printResults()
    if BestAvgSpeed is not None:
        print("Best Average Speed:")
        BestAvgSpeed.printResults()
    if WorstSpeed is not None:
        print("Worst Speed:")
        WorstSpeed.printResults()

    print("models=",Models)
    print("best_precisions=",BestPrecisions)
    print("avg_precisions=",AvgPrecisions)
    print("worst_precisions=",WorstPrecisions)
    print("best_speeds=",BestSpeeds)
    print("avg_speeds=",AvgSpeeds)
    print("worst_speeds=",WorstSpeeds)
    print("best_inference_speeds=",BestInferenceSpeeds)
    print("avg_inference_speeds=",AvgInferenceSpeeds)
    print("worst_inference_speeds=",WorstInferenceSpeeds)
    print("total_detections=",TotalDetectionsArr)
    print("total_inferences=",TotalInferencesArr)

inputType="video"
source = "data/video/sample.MP4" if inputType=="video" else 0
cam = cv2.VideoCapture(source)
display_handle=display(None, display_id=True)
every = 60

start = 0 if source!=0 else -1
end = int(cam.get(cv2.CAP_PROP_FRAME_COUNT)) if source!=0 else -1
if source!=0: cam.set(1, start)

frameCount=start
while_safety=0
saved_count=0

if not cam.isOpened(): print("Error: Could not open source.")
else:
    try:
        while True if end<0 else frameCount<end:
            _, frame = cam.read()
            if frame is None:
                if source!=0:
                    if while_safety > 2000: break
                    while_safety += 1
                    continue
                else:
                    print("Error: Could not capture frame.")
                    cam.release()
                    break

            if every>0:
                if (frameCount+1)%math.floor(every) == 0:
                    while_safety = 0
                    handleFrame(frame, frameCount, source)
            else:
                while_safety=0
                handleFrame(frame)
            frameCount += 1
        handleRelease()
    except KeyboardInterrupt:
        handleRelease()
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Source could not be released.")
        except:
            pass
        traceback.print_exc()


models= ['fcos_resnet', 'retinanet_resnet', 'retinanet_resnet_v2', 'faster_rcnn_resnet', 'faster_rcnn_resnet_v2', 'faster_rcnn_mobilenet_v3', 'faster_rcnn_mobilenet_v3_320', 'mask_rcnn_resnet', 'mask_rcnn_resnet_v2', 'ssd_vgg16', 'ssd_mobilenet_v3', 'yolov5nu', 'yolov5su', 'yolov5mu', 'yolov5lu', 'yolov5xu', 'yolov5n6u', 'yolov5s6u', 'yolov5m6u', 'yolov5l6u', 'yolov5x6u', 'yolov8n', 'yolov8s', 'yolov8m', 'yolov8l', 'yolov8x', 'yolov9c', 'yolov9e']
best_precisions= [84.01497006416321, 86.20967268943787, 96.10866904258728, 99.24747347831726, 99.60418939590454, 99.35287833213806, 98.90456199645996, 98.711097240448, 99.41349625587463, 95.25397419929504, 97.86267876625061, 89.55094814300537, 94.16013956069946, 95.83572149276733, 95.45990824699402, 96.57882452011108, 89.94278907775879, 93.81153583526611, 96.59476280212402, 96.98338508605957, 96.82453870773315, 89.87798690795898, 96.51697874069214, 96.09609842300415, 96.97030186653137, 96.95289731025696, 95.78874111175537, 96.19094133377075]
avg_precisions= [65.37111498588739, 69.96585425642348, 76.44101748562822, 79.92927298188847, 80.5721729032455, 78.78277043209357, 78.05265253409743, 78.70424458855076, 78.58292045186481, 78.78161410251296, 78.83799374103546, 70.05796279662695, 74.8091949991984, 75.29266642969708, 79.60579587767522, 82.98254124820232, 72.58705219752352, 81.56128210967846, 79.9654956091018, 81.12766295671463, 83.55558562491622, 74.6314052078459, 79.01419977987966, 82.52582819231095, 84.48680088571881, 87.62376296638263, 82.92970513042651, 87.14067172078254]
worst_precisions= [50.03466010093689, 51.02284550666809, 50.60317516326904, 50.00736713409424, 50.113362073898315, 50.14830827713013, 52.156418561935425, 50.028592348098755, 50.22304058074951, 51.88506245613098, 52.02542543411255, 51.16927623748779, 51.45619511604309, 50.144755840301514, 50.27608871459961, 51.19180083274841, 51.13379955291748, 50.647759437561035, 50.96123814582825, 54.25065755844116, 51.130348443984985, 50.188612937927246, 50.04092454910278, 51.17872357368469, 53.085654973983765, 56.22338652610779, 52.18837857246399, 51.280707120895386]
best_speeds= [1.2799952030181885, 1.4439949989318848, 1.4089994430541992, 1.4969995021820068, 2.1720008850097656, 0.32000064849853516, 0.12999606132507324, 1.5719971656799316, 2.3050005435943604, 0.24199914932250977, 0.09999966621398926, 0.07899951934814453, 0.11299920082092285, 0.20399999618530273, 0.3280000686645508, 0.534001350402832, 0.1640002727508545, 0.2759997844696045, 0.5689995288848877, 1.0089995861053467, 1.6840004920959473, 0.06799888610839844, 0.11299967765808105, 0.19099855422973633, 0.32599902153015137, 0.45099902153015137, 0.2839999198913574, 0.5679993629455566]
avg_speeds= [1.5971527062645254, 1.7004641159293579, 1.6796069602773647, 1.6877076523826722, 2.382198772122783, 0.4419286934768452, 0.1995463576167822, 1.831336057813544, 2.6133722422812222, 0.32221110183072377, 0.11852728347389066, 0.11169281372657189, 0.19663622607923534, 0.37289991489676544, 0.5611617341637611, 0.7633587891856829, 0.264922494619665, 0.3666183359167549, 0.7546916144234793, 1.3598056158849172, 2.107659171734537, 0.15743989083502027, 0.20622211117898265, 0.35799378477117066, 0.5734209521957065, 0.699716041583826, 0.4587964032825671, 0.8955036042963417]
worst_speeds= [4.820999383926392, 2.113999366760254, 1.884998083114624, 4.445999622344971, 3.7960007190704346, 2.112999200820923, 0.7949943542480469, 4.572998762130737, 6.00499963760376, 0.5579996109008789, 0.1809999942779541, 0.3449990749359131, 3.2705087661743164, 3.6574790477752686, 4.269509553909302, 4.718509912490845, 3.486508846282959, 0.5119976997375488, 0.9569990634918213, 4.6900012493133545, 5.503000259399414, 3.2505083084106445, 3.219511032104492, 3.727508544921875, 4.093508958816528, 4.594508171081543, 3.6935086250305176, 4.604509115219116]
best_inference_speeds= [1.2799952030181885, 1.4439949989318848, 1.4089994430541992, 1.4969995021820068, 2.1720008850097656, 0.32000064849853516, 0.12999606132507324, 1.5719971656799316, 2.3050005435943604, 0.24199914932250977, 0.09999966621398926, 0.07899951934814453, 0.11299920082092285, 0.20399999618530273, 0.3280000686645508, 0.534001350402832, 0.1640002727508545, 0.2759997844696045, 0.5689995288848877, 1.0089995861053467, 1.6840004920959473, 0.06799888610839844, 0.11299967765808105, 0.19099855422973633, 0.32599902153015137, 0.45099902153015137, 0.2839999198913574, 0.5679993629455566]
avg_inference_speeds= [1.5817686649690192, 1.6930524233060005, 1.6706142574548721, 1.6950069077043648, 2.385817207041241, 0.43732389381953646, 0.1981722425531458, 1.7976573875972204, 2.5796677470207214, 0.3215323166969495, 0.11794558777866593, 0.1115895787330523, 0.2012603425267917, 0.3858431899869764, 0.568374806926364, 0.7659667560032436, 0.26700722652932873, 0.36852608856401947, 0.7543860266480265, 1.334726220369339, 2.121320856942071, 0.16059811676249786, 0.21283722505336855, 0.36480024923761206, 0.5682376580065992, 0.7089686336971465, 0.4631744765100025, 0.8865699030104137]
worst_inference_speeds= [4.820999383926392, 2.113999366760254, 1.884998083114624, 4.445999622344971, 3.7960007190704346, 2.112999200820923, 0.7949943542480469, 4.572998762130737, 6.00499963760376, 0.5579996109008789, 0.1809999942779541, 0.3449990749359131, 3.2705087661743164, 3.6574790477752686, 4.269509553909302, 4.718509912490845, 3.486508846282959, 0.5119976997375488, 0.9569990634918213, 4.6900012493133545, 5.503000259399414, 3.2505083084106445, 3.219511032104492, 3.727508544921875, 4.093508958816528, 4.594508171081543, 3.6935086250305176, 4.604509115219116]
total_detections= [129, 97, 99, 187, 217, 136, 128, 190, 211, 83, 98, 78, 73, 86, 96, 96, 71, 89, 105, 112, 112, 72, 93, 93, 92, 101, 95, 103]
total_inferences= [83, 78, 80, 83, 84, 84, 81, 84, 84, 78, 83, 73, 67, 74, 84, 84, 69, 76, 79, 80, 81, 68, 82, 83, 83, 84, 84, 84]





import cv2
import torch
from torchvision.models.detection import *
from torchvision.io.image import read_image
from torchvision.utils import draw_bounding_boxes
from torchvision.transforms.functional import to_pil_image
import torchvision.transforms as transforms
from PIL import Image
import IPython
from IPython.display import display, clear_output
import time
import traceback
import math
from ultralytics import YOLO

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

transform = transforms.Compose([
    #transforms.Resize((300, 300)),
    transforms.ToTensor(),
])

class DetectionModel():
    def __init__(self, concept, backbone, model, weights, reference, color=None):
        self.concept = concept
        self.backbone = backbone
        self.model = model(weights) if concept=="YOLO" else model(weights=weights.DEFAULT)
        if concept!="YOLO":
            self.model.to(device)
            self.model.eval()
        self.weights = weights if concept=="YOLO" else weights.DEFAULT
        self.reference = reference
        self.color = color
        self.detections = []

    def addDetection(self, detection):
        self.detections.append(detection)

    def getDetections(self):
        return self.detections

    def clearDetections(self):
        self.detections = []

    def detectionsCount(self):
        return len(self.detections)

    def inferencesCount(self):
        length = 0
        addedFrames = []
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    addedFrames.append(self.detections[i].frame)
        return len(addedFrames)

    def getAveragePrecision(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].precision
            avg = sum/len(self.detections)
        return  avg.item() if torch.is_tensor(avg) else avg

    def getMaxPrecision(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].precision:
                max=self.detections[i].precision
        return max.item() if torch.is_tensor(max) else max

    def getMinPrecision(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].precision:
                min=self.detections[i].precision
        return 0.0 if min==9999999.99 else min.item() if torch.is_tensor(min) else min

    def getAverageSpeed(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].speed
            avg = sum/len(self.detections)
        return avg

    def getMaxSpeed(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].speed:
                max=self.detections[i].speed
        return max

    def getMinSpeed(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].speed:
                min=self.detections[i].speed
        return 0.0 if min==9999999.99 else min

    def getAverageInferenceSpeed(self):
        sum=0.0
        avg=0.0
        addedFrames=[]
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    sum+=self.detections[i].speed
                    addedFrames.append(self.detections[i].frame)
            avg = sum/len(addedFrames) if len(addedFrames)>0 else 0.0
        return avg

    def getMaxInferenceSpeed(self):
        max=0.0
        addedFrames=[]
        for i in range(len(self.detections)):
            if max<self.detections[i].speed and self.detections[i].frame not in addedFrames:
                max=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return max

    def getMinInferenceSpeed(self):
        min=9999999.99
        addedFrames=[]
        for i in range(len(self.detections)):
            if min>self.detections[i].speed and self.detections[i].frame not in addedFrames:
                min=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return 0.0 if min==9999999.99 else min

    def filterPrecisionsBelow(self, minPrecision):
        newDetections = []
        for i in range(len(self.detections)):
            if self.detections[i].precision>=minPrecision:
                newDetections.append(self.detections[i])
        self.detections = newDetections

    def printResults(self):
        print("Model:", self.reference,
              "\nBest Precision:", self.getMaxPrecision(),
              "%\nAverage Precision:", self.getAveragePrecision(),
              "%\nWorst Precision:", self.getMinPrecision(),
              "%\nBest Speed:", self.getMinSpeed(),
              "seconds\nAverage Speed:", self.getAverageSpeed(),
              "seconds\nWorst Speed:", self.getMaxSpeed(),
              "seconds\nBest Inference Speed:", self.getMinInferenceSpeed(),
              "seconds\nAverage Inference Speed:", self.getAverageInferenceSpeed(),
              "seconds\nWorst Inference Speed:", self.getMaxInferenceSpeed(),
              "seconds\nTotal Detections:", self.detectionsCount(),
              "\nTotal Inferences:", self.inferencesCount(),
              "\n\n")

class DetectionResult():
    def __init__(self, model, label, precision, speed, device=None, source=None, frame=None, boundingBox=None, details=None):
        self.model = model
        self.label = label
        self.precision = precision.item() if torch.is_tensor(precision) else precision
        self.speed = speed.item() if torch.is_tensor(speed) else speed
        self.device = device
        self.source = source
        self.frame = frame
        self.boundingBox = boundingBox
        self.details = details

target_classes = ["truck", "bus", "car", "train", "bicycle"]


VEHICLE_DETECTION_MODELS = [
    DetectionModel("FCOS", "resnet50_fpn", fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights, "fcos_resnet", (255,0,0)),
    DetectionModel("RetinaNet", "resnet50_fpn", retinanet_resnet50_fpn, RetinaNet_ResNet50_FPN_Weights, "retinanet_resnet", (255,255,0)),
    DetectionModel("RetinaNet", "resnet50_fpn_v2", retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights, "retinanet_resnet_v2", (0,234,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn", fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights, "faster_rcnn_resnet", (170,0,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn_v2", fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights, "faster_rcnn_resnet_v2", (255,127,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_fpn", fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights, "faster_rcnn_mobilenet_v3", (191,255,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_320_fpn", fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights, "faster_rcnn_mobilenet_v3_320", (0,149,255)),
    DetectionModel("MaskRCNN", "resnet50_fpn", maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights, "mask_rcnn_resnet", (255,0,170)),
    DetectionModel("MaskRCNN", "resnet50_fpn_v2", maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights, "mask_rcnn_resnet_v2", (255,212,0)),
    DetectionModel("SSD300", "vgg16", ssd300_vgg16, SSD300_VGG16_Weights, "ssd_vgg16", (106,255,0)),
    DetectionModel("SSDLite320", "mobilenet_v3_large", ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights, "ssd_mobilenet_v3", (0,64,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5nu.pt', "yolov5nu", (237,185,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5su.pt', "yolov5su", (185,215,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5mu.pt', "yolov5mu", (231,233,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5lu.pt', "yolov5lu", (220,185,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5xu.pt', "yolov5xu", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5n6u.pt', "yolov5n6u", (143,35,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5s6u.pt', "yolov5s6u", (35,98,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5m6u.pt', "yolov5m6u", (143,106,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5l6u.pt', "yolov5l6u", (107,35,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5x6u.pt', "yolov5x6u", (79,143,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8n.pt', "yolov8n", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8s.pt', "yolov8s", (115,115,115)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8m.pt', "yolov8m", (204,204,204)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8l.pt', "yolov8l", (255,0,0)), # cycle restarts
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8x.pt', "yolov8x", (255,255,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9c.pt', "yolov9c", (0,234,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9e.pt', "yolov9e", (170,0,255))#,
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10n.pt', "yolov10n", (185,237,224)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10s.pt', "yolov10s", (115,115,115)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10m.pt', "yolov10m", (204,204,204)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10b.pt', "yolov10b", (204,204,204)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10l.pt', "yolov10l", (255,0,0)), # cycle restarts
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10x.pt', "yolov10x", (255,255,0))
]

def process(frame, frameIndex=-1, source=None):
    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        processed_frame = frame.copy()

        if currentModel.concept=="YOLO":
            start_time = time.time()
            results = currentModel.model(processed_frame, agnostic_nms=True, verbose=False, device=device)[0]
            processing_time = time.time() - start_time
            for result in results:
                detection_count = result.boxes.shape[0]
                for j in range(detection_count):
                    cls = int(result.boxes.cls[j].item())
                    name = result.names[cls]
                    confidence = float(result.boxes.conf[j].item())
                    bounding_box = result.boxes.xyxy[j].cpu().numpy()
                    if name in target_classes and confidence>0.5:
                        x1, y1, x2, y2 = [int(x) for x in bounding_box]
                        # Ensure x1 < x2 and y1 < y2
                        x1, x2 = min(x1, x2), max(x1, x2)
                        y1, y2 = min(y1, y2), max(y1, y2)
                        label = '{} ({:.2f}%) ({:.3f}s) {}'.format(name.upper(), confidence*100, processing_time, currentModel.reference)
                        print(label)
                        VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, name, confidence*100, processing_time, device, source, frameIndex, bounding_box, label))
                        cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                        cv2.putText(processed_frame, label, (x1+10, y1+25), 0, 0.8, currentModel.color, 2)
        else:
            # Convert frame to RGB and PIL Image, then apply transformation
            frame_rgb = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            image = transform(pil_image).unsqueeze(0).to(device)

            # Object detection
            start_time = time.time()
            with torch.no_grad():
                predictions = currentModel.model(image)

            # Processing time
            processing_time = time.time() - start_time

            # Visualization
            scores = predictions[0]['scores']
            boxes = predictions[0]['boxes']
            labels = predictions[0]['labels']
            confidence_threshold = 0.5

            for score, box, label in zip(scores, boxes, labels):
                # and score > confidence_threshold
                if currentModel.weights.meta["categories"][label.item()] in target_classes and score > confidence_threshold:
                    x1, y1, x2, y2 = map(int, box)
                    label_text = f'{currentModel.weights.meta["categories"][label.item()].upper()} ({score*100:.2f}%) ({processing_time:.3f}s) {currentModel.reference}'
                    print(label_text)
                    VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, currentModel.weights.meta["categories"][label.item()], float(score.item())*100, processing_time, device, source, frameIndex, box, label_text))
                    cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                    cv2.putText(processed_frame, label_text, (x1, y1+20), 0, 0.8, currentModel.color, 2)

        img = Image.fromarray(processed_frame, 'RGB')
        display(img)
        clear_output(wait=True)
    return frame

def handleFrame(frame, frameIndex=-1, source=None):
    processed_frame = process(frame,frameIndex,source)

def handleRelease():
    cam.release()
    print("Source released.")

    print(device)
    print(source)

    BestPrecision=None
    BestAvgPrecision=None
    WorstPrecision=None
    BestSpeed=None
    BestAvgSpeed=None
    WorstSpeed=None

    Models=[]
    BestPrecisions=[]
    AvgPrecisions=[]
    WorstPrecisions=[]
    BestSpeeds=[]
    AvgSpeeds=[]
    WorstSpeeds=[]
    BestInferenceSpeeds=[]
    AvgInferenceSpeeds=[]
    WorstInferenceSpeeds=[]
    TotalDetectionsArr=[]
    TotalInferencesArr=[]

    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        # currentModel.printResults()

        Models.append(currentModel.reference)
        BestPrecisions.append(currentModel.getMaxPrecision())
        AvgPrecisions.append(currentModel.getAveragePrecision())
        WorstPrecisions.append(currentModel.getMinPrecision())
        BestSpeeds.append(currentModel.getMinSpeed())
        AvgSpeeds.append(currentModel.getAverageSpeed())
        WorstSpeeds.append(currentModel.getMaxSpeed())
        BestInferenceSpeeds.append(currentModel.getMinInferenceSpeed())
        AvgInferenceSpeeds.append(currentModel.getAverageInferenceSpeed())
        WorstInferenceSpeeds.append(currentModel.getMaxInferenceSpeed())
        TotalDetectionsArr.append(currentModel.detectionsCount())
        TotalInferencesArr.append(currentModel.inferencesCount())

        if BestPrecision is None or BestPrecision.getMaxPrecision()<=currentModel.getMaxPrecision():
            BestPrecision = currentModel
        if BestAvgPrecision is None or BestAvgPrecision.getAveragePrecision()<=currentModel.getAveragePrecision():
            BestAvgPrecision = currentModel
        if WorstPrecision is None or WorstPrecision.getMinPrecision()>=currentModel.getMinPrecision():
            WorstPrecision = currentModel
        if BestSpeed is None or BestSpeed.getMinSpeed()>=currentModel.getMinSpeed():
            BestSpeed = currentModel
        if BestAvgSpeed is None or BestAvgSpeed.getAverageSpeed()>=currentModel.getAverageSpeed():
            BestAvgSpeed = currentModel
        if WorstSpeed is None or WorstSpeed.getMaxSpeed()<=currentModel.getMaxSpeed():
            WorstSpeed = currentModel

    if BestPrecision is not None:
        print("Best Precision:")
        BestPrecision.printResults()
    if BestAvgPrecision is not None:
        print("Best Average Precision:")
        BestAvgPrecision.printResults()
    if WorstPrecision is not None:
        print("Worst Precision:")
        WorstPrecision.printResults()
    if BestSpeed is not None:
        print("Best Speed:")
        BestSpeed.printResults()
    if BestAvgSpeed is not None:
        print("Best Average Speed:")
        BestAvgSpeed.printResults()
    if WorstSpeed is not None:
        print("Worst Speed:")
        WorstSpeed.printResults()

    print("models=",Models)
    print("best_precisions=",BestPrecisions)
    print("avg_precisions=",AvgPrecisions)
    print("worst_precisions=",WorstPrecisions)
    print("best_speeds=",BestSpeeds)
    print("avg_speeds=",AvgSpeeds)
    print("worst_speeds=",WorstSpeeds)
    print("best_inference_speeds=",BestInferenceSpeeds)
    print("avg_inference_speeds=",AvgInferenceSpeeds)
    print("worst_inference_speeds=",WorstInferenceSpeeds)
    print("total_detections=",TotalDetectionsArr)
    print("total_inferences=",TotalInferencesArr)

inputType="video"
source = "data/video/sample.MP4" if inputType=="video" else 0
cam = cv2.VideoCapture(source)
display_handle=display(None, display_id=True)
every = 30

start = 0 if source!=0 else -1
end = int(cam.get(cv2.CAP_PROP_FRAME_COUNT)) if source!=0 else -1
if source!=0: cam.set(1, start)

frameCount=start
while_safety=0
saved_count=0

if not cam.isOpened(): print("Error: Could not open source.")
else:
    try:
        while True if end<0 else frameCount<end:
            _, frame = cam.read()
            if frame is None:
                if source!=0:
                    if while_safety > 2000: break
                    while_safety += 1
                    continue
                else:
                    print("Error: Could not capture frame.")
                    cam.release()
                    break

            if every>0:
                if (frameCount+1)%math.floor(every) == 0:
                    while_safety = 0
                    handleFrame(frame, frameCount, source)
            else:
                while_safety=0
                handleFrame(frame)
            frameCount += 1
        handleRelease()
    except KeyboardInterrupt:
        handleRelease()
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Source could not be released.")
        except:
            pass
        traceback.print_exc()


models= ['fcos_resnet', 'retinanet_resnet', 'retinanet_resnet_v2', 'faster_rcnn_resnet', 'faster_rcnn_resnet_v2', 'faster_rcnn_mobilenet_v3', 'faster_rcnn_mobilenet_v3_320', 'mask_rcnn_resnet', 'mask_rcnn_resnet_v2', 'ssd_vgg16', 'ssd_mobilenet_v3', 'yolov5nu', 'yolov5su', 'yolov5mu', 'yolov5lu', 'yolov5xu', 'yolov5n6u', 'yolov5s6u', 'yolov5m6u', 'yolov5l6u', 'yolov5x6u', 'yolov8n', 'yolov8s', 'yolov8m', 'yolov8l', 'yolov8x', 'yolov9c', 'yolov9e']
best_precisions= [84.01497006416321, 88.00891041755676, 96.10866904258728, 99.24747347831726, 99.67659711837769, 99.35287833213806, 98.90456199645996, 98.8481879234314, 99.59056377410889, 95.25397419929504, 97.86267876625061, 90.19646048545837, 94.16013956069946, 95.83572149276733, 97.12871313095093, 96.57882452011108, 89.94278907775879, 94.4776713848114, 96.59476280212402, 96.98338508605957, 96.82453870773315, 89.87798690795898, 96.51697874069214, 96.09609842300415, 96.97030186653137, 96.95289731025696, 95.9636390209198, 97.48988747596741]
avg_precisions= [65.43385904568892, 70.04024830336373, 75.86408981255123, 79.47075126682901, 79.79196012020111, 79.3591227771631, 77.86601981309455, 78.9016061876662, 78.61191116628193, 78.07162846633774, 79.79972055181861, 70.27605968136942, 74.80887823008202, 75.1855023247855, 80.17542245526793, 82.8638139180839, 71.77158995550506, 80.7107714119922, 80.0245333445464, 81.26633239013178, 83.07473066141787, 73.97319781069724, 79.52744325438698, 82.74208087334658, 84.03342531964103, 87.20352369196274, 83.06270292196324, 86.73328135324562]
worst_precisions= [50.01919865608215, 50.76150298118591, 50.47064423561096, 50.00074505805969, 50.113362073898315, 50.14830827713013, 50.014227628707886, 50.01877546310425, 50.208425521850586, 50.5246639251709, 51.88782811164856, 51.16927623748779, 51.45619511604309, 50.144755840301514, 50.27608871459961, 50.60702562332153, 50.43152570724487, 50.647759437561035, 50.05227327346802, 50.41007399559021, 50.94268321990967, 50.188612937927246, 50.04092454910278, 51.17872357368469, 52.242863178253174, 51.96268558502197, 50.43748617172241, 50.09580850601196]
best_speeds= [1.1979591846466064, 1.359999179840088, 1.3589975833892822, 1.3739955425262451, 2.0669991970062256, 0.2889993190765381, 0.13199830055236816, 1.4399967193603516, 2.1580004692077637, 0.21600079536437988, 0.09399938583374023, 0.07899975776672363, 0.1080007553100586, 0.1900010108947754, 0.3029978275299072, 0.4669625759124756, 0.1569986343383789, 0.26599931716918945, 0.5429987907409668, 0.9730007648468018, 1.6309993267059326, 0.06800079345703125, 0.10400104522705078, 0.1959993839263916, 0.3240010738372803, 0.4470040798187256, 0.28999996185302734, 0.549999475479126]
avg_speeds= [1.415347472520975, 1.5433508952458699, 1.5419237187930517, 1.5105405115956412, 2.2095253307188574, 0.37785166590961056, 0.20109875465002586, 1.631306105868193, 2.3612893501917522, 0.31118118120524696, 0.11568766583998998, 0.10310648948915543, 0.14731486906876434, 0.27268216950552804, 0.43233723993654605, 0.6687048859894276, 0.20792986259979457, 0.3579041087225582, 0.6984626485708174, 1.1902996544484739, 1.9016168609328334, 0.09316374292436815, 0.14706014145861615, 0.2718356535396474, 0.45428318263375184, 0.603670971066344, 0.38236655129326713, 0.7088354820214607]
worst_speeds= [2.9105520248413086, 1.9766957759857178, 2.112997055053711, 1.9227828979492188, 2.4856061935424805, 0.6687543392181396, 0.42299818992614746, 2.0046792030334473, 2.733490228652954, 0.5261998176574707, 0.15799951553344727, 0.3988163471221924, 0.4640007019042969, 0.6620004177093506, 1.0599994659423828, 1.938000202178955, 0.4230000972747803, 0.739999532699585, 1.5380003452301025, 2.559143304824829, 3.9170029163360596, 0.20900821685791016, 0.31799936294555664, 0.6339998245239258, 1.0089998245239258, 1.4163851737976074, 0.786001443862915, 1.6489999294281006]
best_inference_speeds= [1.1979591846466064, 1.359999179840088, 1.3589975833892822, 1.3739955425262451, 2.0669991970062256, 0.2889993190765381, 0.13199830055236816, 1.4399967193603516, 2.1580004692077637, 0.21600079536437988, 0.09399938583374023, 0.07899975776672363, 0.1080007553100586, 0.1900010108947754, 0.3029978275299072, 0.4669625759124756, 0.1569986343383789, 0.26599931716918945, 0.5429987907409668, 0.9730007648468018, 1.6309993267059326, 0.06800079345703125, 0.10400104522705078, 0.1959993839263916, 0.3240010738372803, 0.4470040798187256, 0.28999996185302734, 0.549999475479126]
avg_inference_speeds= [1.4184154171541512, 1.5416769828551855, 1.5425471610660795, 1.5202380668617295, 2.2147093287536075, 0.37930653480712523, 0.20342238645375885, 1.6340475834551311, 2.3631570012796494, 0.3124665275696785, 0.11556024376938982, 0.10348343043713956, 0.14698288705613877, 0.2750032927026812, 0.4328662559210536, 0.6741023991636174, 0.20932381222213525, 0.3586813051989124, 0.7111663207411766, 1.2096847348243185, 1.9288241966910984, 0.09359374283053351, 0.14748729747018696, 0.273051134840457, 0.4566625572112669, 0.6126725730441865, 0.38442328033677065, 0.7210712972141448]
worst_inference_speeds= [2.9105520248413086, 1.9766957759857178, 2.112997055053711, 1.9227828979492188, 2.4856061935424805, 0.6687543392181396, 0.42299818992614746, 2.0046792030334473, 2.733490228652954, 0.5261998176574707, 0.15799951553344727, 0.3988163471221924, 0.4640007019042969, 0.6620004177093506, 1.0599994659423828, 1.938000202178955, 0.4230000972747803, 0.739999532699585, 1.5380003452301025, 2.559143304824829, 3.9170029163360596, 0.20900821685791016, 0.31799936294555664, 0.6339998245239258, 1.0089998245239258, 1.4163851737976074, 0.786001443862915, 1.6489999294281006]
total_detections= [260, 192, 196, 382, 446, 268, 254, 371, 420, 167, 192, 155, 148, 175, 189, 192, 147, 178, 213, 216, 223, 151, 182, 187, 187, 204, 189, 207]
total_inferences= [166, 156, 158, 167, 168, 167, 161, 168, 168, 155, 164, 148, 135, 151, 166, 167, 138, 157, 160, 159, 161, 141, 162, 167, 166, 168, 166, 168]


import numpy as np
import matplotlib.pyplot as plt
import matplotlib

# Generate a color map
num_models = len(models)
cmap = matplotlib.colormaps.get_cmap('nipy_spectral')
colors = [cmap(i / num_models) for i in range(num_models)]

# Create a figure and plot with unique colors
plt.figure(figsize=(14, 10))
for i, model in enumerate(models):
    plt.scatter(avg_inference_speeds[i], avg_precisions[i], color=colors[i], label=model, edgecolor='black')

print("DEVICE: CPU (Intel Core i7-14700KF)")
print("SOURCE: data/video/sample.MP4 (1080p60fps Video)")
print("SKIP EVERY 30 FRAMES")
print("TOTALLY PROCESSED FRAMES: 168")
print("COCO Dataset, Classes truck, bus, car, train, bicycle")
print("ALL >50%")
# Axis labels and plot title
plt.xlabel('Average Speed (seconds)')
plt.ylabel('Average Precision (%)')
plt.title('Model Performance: Average Inference Speed x Average Accuracy')
plt.legend(loc='upper left', bbox_to_anchor=(1.05, 1), fontsize='small')
plt.grid(True)
plt.savefig("Analysis/Experiment1/i7_14700KF/Experiment1b_Diagram.png", bbox_inches='tight')
plt.show()

# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_precisions, avg_precisions, worst_precisions])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Precision (%)", "Average Precision (%)", "Worst Precision (%)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

plt.title("Model Precision Metrics")
plt.savefig("Analysis/Experiment1/i7_14700KF/Experiment1b_Precision_Table.png", bbox_inches='tight')
plt.show()

# # Set up the figure and axis for the table
# fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
# ax.axis('tight')
# ax.axis('off')

# # The table data: transpose the array to make each column a different metric
# table_data = np.transpose([models, best_speeds, avg_speeds, worst_speeds])

# # Create the table in the plot
# table = ax.table(cellText=table_data, colLabels=["Model", "Best Speed (s)", "Average Speed (s)", "Worst Speed (s)"],
#                  cellLoc='center', loc='center', colColours=["palegreen"] * 4)
# table.auto_set_font_size(False)
# table.set_fontsize(10)
# table.scale(1.2, 1.2)  # Scale table size

# plt.title("Model Speed Metrics")
# plt.show()

# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_inference_speeds, avg_inference_speeds, worst_inference_speeds])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Inference Speed (s)", "Average Inference Speed (s)", "Worst Inference Speed (s)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

plt.title("Model Inference Speed Metrics")
plt.savefig("Analysis/Experiment1/i7_14700KF/Experiment1b_Time_Table.png", bbox_inches='tight')
plt.show()





import cv2
import torch
from torchvision.models.detection import *
from torchvision.io.image import read_image
from torchvision.utils import draw_bounding_boxes
from torchvision.transforms.functional import to_pil_image
import torchvision.transforms as transforms
from PIL import Image
import IPython
from IPython.display import display, clear_output
import time
import traceback
import math
from ultralytics import YOLO

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

transform = transforms.Compose([
    #transforms.Resize((300, 300)),
    transforms.ToTensor(),
])

class DetectionModel():
    def __init__(self, concept, backbone, model, weights, reference, color=None):
        self.concept = concept
        self.backbone = backbone
        self.model = model(weights) if concept=="YOLO" else model(weights=weights.DEFAULT)
        if concept!="YOLO":
            self.model.to(device)
            self.model.eval()
        self.weights = weights if concept=="YOLO" else weights.DEFAULT
        self.reference = reference
        self.color = color
        self.detections = []

    def addDetection(self, detection):
        self.detections.append(detection)

    def getDetections(self):
        return self.detections

    def clearDetections(self):
        self.detections = []

    def detectionsCount(self):
        return len(self.detections)

    def inferencesCount(self):
        length = 0
        addedFrames = []
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    addedFrames.append(self.detections[i].frame)
        return len(addedFrames)

    def getAveragePrecision(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].precision
            avg = sum/len(self.detections)
        return  avg.item() if torch.is_tensor(avg) else avg

    def getMaxPrecision(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].precision:
                max=self.detections[i].precision
        return max.item() if torch.is_tensor(max) else max

    def getMinPrecision(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].precision:
                min=self.detections[i].precision
        return 0.0 if min==9999999.99 else min.item() if torch.is_tensor(min) else min

    def getAverageSpeed(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].speed
            avg = sum/len(self.detections)
        return avg

    def getMaxSpeed(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].speed:
                max=self.detections[i].speed
        return max

    def getMinSpeed(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].speed:
                min=self.detections[i].speed
        return 0.0 if min==9999999.99 else min

    def getAverageInferenceSpeed(self):
        sum=0.0
        avg=0.0
        addedFrames=[]
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    sum+=self.detections[i].speed
                    addedFrames.append(self.detections[i].frame)
            avg = sum/len(addedFrames) if len(addedFrames)>0 else 0.0
        return avg

    def getMaxInferenceSpeed(self):
        max=0.0
        addedFrames=[]
        for i in range(len(self.detections)):
            if max<self.detections[i].speed and self.detections[i].frame not in addedFrames:
                max=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return max

    def getMinInferenceSpeed(self):
        min=9999999.99
        addedFrames=[]
        for i in range(len(self.detections)):
            if min>self.detections[i].speed and self.detections[i].frame not in addedFrames:
                min=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return 0.0 if min==9999999.99 else min

    def filterPrecisionsBelow(self, minPrecision):
        newDetections = []
        for i in range(len(self.detections)):
            if self.detections[i].precision>=minPrecision:
                newDetections.append(self.detections[i])
        self.detections = newDetections

    def printResults(self):
        print("Model:", self.reference,
              "\nBest Precision:", self.getMaxPrecision(),
              "%\nAverage Precision:", self.getAveragePrecision(),
              "%\nWorst Precision:", self.getMinPrecision(),
              "%\nBest Speed:", self.getMinSpeed(),
              "seconds\nAverage Speed:", self.getAverageSpeed(),
              "seconds\nWorst Speed:", self.getMaxSpeed(),
              "seconds\nBest Inference Speed:", self.getMinInferenceSpeed(),
              "seconds\nAverage Inference Speed:", self.getAverageInferenceSpeed(),
              "seconds\nWorst Inference Speed:", self.getMaxInferenceSpeed(),
              "seconds\nTotal Detections:", self.detectionsCount(),
              "\nTotal Inferences:", self.inferencesCount(),
              "\n\n")

class DetectionResult():
    def __init__(self, model, label, precision, speed, device=None, source=None, frame=None, boundingBox=None, details=None):
        self.model = model
        self.label = label
        self.precision = precision.item() if torch.is_tensor(precision) else precision
        self.speed = speed.item() if torch.is_tensor(speed) else speed
        self.device = device
        self.source = source
        self.frame = frame
        self.boundingBox = boundingBox
        self.details = details

target_classes = ["truck", "bus", "car", "train", "bicycle"]


VEHICLE_DETECTION_MODELS = [
    DetectionModel("FCOS", "resnet50_fpn", fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights, "fcos_resnet", (255,0,0)),
    DetectionModel("RetinaNet", "resnet50_fpn", retinanet_resnet50_fpn, RetinaNet_ResNet50_FPN_Weights, "retinanet_resnet", (255,255,0)),
    DetectionModel("RetinaNet", "resnet50_fpn_v2", retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights, "retinanet_resnet_v2", (0,234,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn", fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights, "faster_rcnn_resnet", (170,0,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn_v2", fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights, "faster_rcnn_resnet_v2", (255,127,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_fpn", fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights, "faster_rcnn_mobilenet_v3", (191,255,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_320_fpn", fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights, "faster_rcnn_mobilenet_v3_320", (0,149,255)),
    DetectionModel("MaskRCNN", "resnet50_fpn", maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights, "mask_rcnn_resnet", (255,0,170)),
    DetectionModel("MaskRCNN", "resnet50_fpn_v2", maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights, "mask_rcnn_resnet_v2", (255,212,0)),
    DetectionModel("SSD300", "vgg16", ssd300_vgg16, SSD300_VGG16_Weights, "ssd_vgg16", (106,255,0)),
    DetectionModel("SSDLite320", "mobilenet_v3_large", ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights, "ssd_mobilenet_v3", (0,64,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5nu.pt', "yolov5nu", (237,185,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5su.pt', "yolov5su", (185,215,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5mu.pt', "yolov5mu", (231,233,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5lu.pt', "yolov5lu", (220,185,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5xu.pt', "yolov5xu", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5n6u.pt', "yolov5n6u", (143,35,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5s6u.pt', "yolov5s6u", (35,98,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5m6u.pt', "yolov5m6u", (143,106,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5l6u.pt', "yolov5l6u", (107,35,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5x6u.pt', "yolov5x6u", (79,143,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8n.pt', "yolov8n", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8s.pt', "yolov8s", (115,115,115)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8m.pt', "yolov8m", (204,204,204)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8l.pt', "yolov8l", (255,0,0)), # cycle restarts
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8x.pt', "yolov8x", (255,255,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9c.pt', "yolov9c", (0,234,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9e.pt', "yolov9e", (170,0,255))#,
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10n.pt', "yolov10n", (185,237,224)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10s.pt', "yolov10s", (115,115,115)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10m.pt', "yolov10m", (204,204,204)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10b.pt', "yolov10b", (204,204,204)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10l.pt', "yolov10l", (255,0,0)), # cycle restarts
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10x.pt', "yolov10x", (255,255,0))
]

def process(frame, frameIndex=-1, source=None):
    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        processed_frame = frame.copy()

        if currentModel.concept=="YOLO":
            start_time = time.time()
            results = currentModel.model(processed_frame, agnostic_nms=True, verbose=False, device=device)[0]
            processing_time = time.time() - start_time
            for result in results:
                detection_count = result.boxes.shape[0]
                for j in range(detection_count):
                    cls = int(result.boxes.cls[j].item())
                    name = result.names[cls]
                    confidence = float(result.boxes.conf[j].item())
                    bounding_box = result.boxes.xyxy[j].cpu().numpy()
                    if name in target_classes and confidence>0.5:
                        x1, y1, x2, y2 = [int(x) for x in bounding_box]
                        # Ensure x1 < x2 and y1 < y2
                        x1, x2 = min(x1, x2), max(x1, x2)
                        y1, y2 = min(y1, y2), max(y1, y2)
                        label = '{} ({:.2f}%) ({:.3f}s) {}'.format(name.upper(), confidence*100, processing_time, currentModel.reference)
                        print(label)
                        VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, name, confidence*100, processing_time, device, source, frameIndex, bounding_box, label))
                        cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                        cv2.putText(processed_frame, label, (x1+10, y1+25), 0, 0.8, currentModel.color, 2)
        else:
            # Convert frame to RGB and PIL Image, then apply transformation
            frame_rgb = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            image = transform(pil_image).unsqueeze(0).to(device)

            # Object detection
            start_time = time.time()
            with torch.no_grad():
                predictions = currentModel.model(image)

            # Processing time
            processing_time = time.time() - start_time

            # Visualization
            scores = predictions[0]['scores']
            boxes = predictions[0]['boxes']
            labels = predictions[0]['labels']
            confidence_threshold = 0.5

            for score, box, label in zip(scores, boxes, labels):
                # and score > confidence_threshold
                if currentModel.weights.meta["categories"][label.item()] in target_classes and score > confidence_threshold:
                    x1, y1, x2, y2 = map(int, box)
                    label_text = f'{currentModel.weights.meta["categories"][label.item()].upper()} ({score*100:.2f}%) ({processing_time:.3f}s) {currentModel.reference}'
                    print(label_text)
                    VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, currentModel.weights.meta["categories"][label.item()], float(score.item())*100, processing_time, device, source, frameIndex, box, label_text))
                    cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                    cv2.putText(processed_frame, label_text, (x1, y1+20), 0, 0.8, currentModel.color, 2)

        img = Image.fromarray(processed_frame, 'RGB')
        display(img)
        clear_output(wait=True)
    return frame

def handleFrame(frame, frameIndex=-1, source=None):
    processed_frame = process(frame,frameIndex,source)

def handleRelease():
    cam.release()
    print("Source released.")

    print(device)
    print(source)

    BestPrecision=None
    BestAvgPrecision=None
    WorstPrecision=None
    BestSpeed=None
    BestAvgSpeed=None
    WorstSpeed=None

    Models=[]
    BestPrecisions=[]
    AvgPrecisions=[]
    WorstPrecisions=[]
    BestSpeeds=[]
    AvgSpeeds=[]
    WorstSpeeds=[]
    BestInferenceSpeeds=[]
    AvgInferenceSpeeds=[]
    WorstInferenceSpeeds=[]
    TotalDetectionsArr=[]
    TotalInferencesArr=[]

    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        # currentModel.printResults()

        Models.append(currentModel.reference)
        BestPrecisions.append(currentModel.getMaxPrecision())
        AvgPrecisions.append(currentModel.getAveragePrecision())
        WorstPrecisions.append(currentModel.getMinPrecision())
        BestSpeeds.append(currentModel.getMinSpeed())
        AvgSpeeds.append(currentModel.getAverageSpeed())
        WorstSpeeds.append(currentModel.getMaxSpeed())
        BestInferenceSpeeds.append(currentModel.getMinInferenceSpeed())
        AvgInferenceSpeeds.append(currentModel.getAverageInferenceSpeed())
        WorstInferenceSpeeds.append(currentModel.getMaxInferenceSpeed())
        TotalDetectionsArr.append(currentModel.detectionsCount())
        TotalInferencesArr.append(currentModel.inferencesCount())

        if BestPrecision is None or BestPrecision.getMaxPrecision()<=currentModel.getMaxPrecision():
            BestPrecision = currentModel
        if BestAvgPrecision is None or BestAvgPrecision.getAveragePrecision()<=currentModel.getAveragePrecision():
            BestAvgPrecision = currentModel
        if WorstPrecision is None or WorstPrecision.getMinPrecision()>=currentModel.getMinPrecision():
            WorstPrecision = currentModel
        if BestSpeed is None or BestSpeed.getMinSpeed()>=currentModel.getMinSpeed():
            BestSpeed = currentModel
        if BestAvgSpeed is None or BestAvgSpeed.getAverageSpeed()>=currentModel.getAverageSpeed():
            BestAvgSpeed = currentModel
        if WorstSpeed is None or WorstSpeed.getMaxSpeed()<=currentModel.getMaxSpeed():
            WorstSpeed = currentModel

    if BestPrecision is not None:
        print("Best Precision:")
        BestPrecision.printResults()
    if BestAvgPrecision is not None:
        print("Best Average Precision:")
        BestAvgPrecision.printResults()
    if WorstPrecision is not None:
        print("Worst Precision:")
        WorstPrecision.printResults()
    if BestSpeed is not None:
        print("Best Speed:")
        BestSpeed.printResults()
    if BestAvgSpeed is not None:
        print("Best Average Speed:")
        BestAvgSpeed.printResults()
    if WorstSpeed is not None:
        print("Worst Speed:")
        WorstSpeed.printResults()

    print("models=",Models)
    print("best_precisions=",BestPrecisions)
    print("avg_precisions=",AvgPrecisions)
    print("worst_precisions=",WorstPrecisions)
    print("best_speeds=",BestSpeeds)
    print("avg_speeds=",AvgSpeeds)
    print("worst_speeds=",WorstSpeeds)
    print("best_inference_speeds=",BestInferenceSpeeds)
    print("avg_inference_speeds=",AvgInferenceSpeeds)
    print("worst_inference_speeds=",WorstInferenceSpeeds)
    print("total_detections=",TotalDetectionsArr)
    print("total_inferences=",TotalInferencesArr)

inputType="video"
source = "data/video/sample.MP4" if inputType=="video" else 0
cam = cv2.VideoCapture(source)
display_handle=display(None, display_id=True)
every = 60

start = 0 if source!=0 else -1
end = int(cam.get(cv2.CAP_PROP_FRAME_COUNT)) if source!=0 else -1
if source!=0: cam.set(1, start)

frameCount=start
while_safety=0
saved_count=0

if not cam.isOpened(): print("Error: Could not open source.")
else:
    try:
        while True if end<0 else frameCount<end:
            _, frame = cam.read()
            if frame is None:
                if source!=0:
                    if while_safety > 2000: break
                    while_safety += 1
                    continue
                else:
                    print("Error: Could not capture frame.")
                    cam.release()
                    break

            if every>0:
                if (frameCount+1)%math.floor(every) == 0:
                    while_safety = 0
                    handleFrame(frame, frameCount, source)
            else:
                while_safety=0
                handleFrame(frame)
            frameCount += 1
        handleRelease()
    except KeyboardInterrupt:
        handleRelease()
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Source could not be released.")
        except:
            pass
        traceback.print_exc()


models= ['fcos_resnet', 'retinanet_resnet', 'retinanet_resnet_v2', 'faster_rcnn_resnet', 'faster_rcnn_resnet_v2', 'faster_rcnn_mobilenet_v3', 'faster_rcnn_mobilenet_v3_320', 'mask_rcnn_resnet', 'mask_rcnn_resnet_v2', 'ssd_vgg16', 'ssd_mobilenet_v3', 'yolov5nu', 'yolov5su', 'yolov5mu', 'yolov5lu', 'yolov5xu', 'yolov5n6u', 'yolov5s6u', 'yolov5m6u', 'yolov5l6u', 'yolov5x6u', 'yolov8n', 'yolov8s', 'yolov8m', 'yolov8l', 'yolov8x', 'yolov9c', 'yolov9e']
best_precisions= [84.01497006416321, 86.20967268943787, 96.10866904258728, 99.24747347831726, 99.60418939590454, 99.35287833213806, 98.90456199645996, 98.711097240448, 99.41349625587463, 95.25397419929504, 97.86267876625061, 89.55094814300537, 94.16013956069946, 95.83572149276733, 95.45990824699402, 96.57882452011108, 89.94278907775879, 93.81153583526611, 96.59476280212402, 96.98338508605957, 96.82453870773315, 89.87798690795898, 96.51697874069214, 96.09609842300415, 96.97030186653137, 96.95289731025696, 95.78874111175537, 96.19094133377075]
avg_precisions= [65.37111498588739, 69.96585425642348, 76.44101748562822, 79.92927298188847, 80.5721729032455, 78.78277043209357, 78.05265253409743, 78.70424458855076, 78.58292045186481, 78.78161410251296, 78.83799374103546, 70.05796279662695, 74.8091949991984, 75.29266642969708, 79.60579587767522, 82.98254124820232, 72.58705219752352, 81.56128210967846, 79.9654956091018, 81.12766295671463, 83.55558562491622, 74.6314052078459, 79.01419977987966, 82.52582819231095, 84.48680088571881, 87.62376296638263, 82.92970513042651, 87.14067172078254]
worst_precisions= [50.03466010093689, 51.02284550666809, 50.60317516326904, 50.00736713409424, 50.113362073898315, 50.14830827713013, 52.156418561935425, 50.028592348098755, 50.22304058074951, 51.88506245613098, 52.02542543411255, 51.16927623748779, 51.45619511604309, 50.144755840301514, 50.27608871459961, 51.19180083274841, 51.13379955291748, 50.647759437561035, 50.96123814582825, 54.25065755844116, 51.130348443984985, 50.188612937927246, 50.04092454910278, 51.17872357368469, 53.085654973983765, 56.22338652610779, 52.18837857246399, 51.280707120895386]
best_speeds= [1.3186442852020264, 1.4654877185821533, 1.3700661659240723, 1.405256748199463, 2.0882694721221924, 0.29954051971435547, 0.14551043510437012, 1.5174963474273682, 2.158961534500122, 0.2494509220123291, 0.09437084197998047, 0.07900643348693848, 0.12017178535461426, 0.1930370330810547, 0.32104992866516113, 0.5035734176635742, 0.1704714298248291, 0.2926163673400879, 0.5926859378814697, 1.0141665935516357, 1.6597988605499268, 0.07199978828430176, 0.1070866584777832, 0.21702980995178223, 0.3591303825378418, 0.4792959690093994, 0.30332303047180176, 0.5961430072784424]
avg_speeds= [1.5801853967267414, 1.7135983373700958, 1.7049897656296238, 1.6238870034243333, 2.2767281686106036, 0.41239915525212006, 0.2124397698789835, 1.7313999878732782, 2.44724935264949, 0.35938864730926884, 0.1161490265203982, 0.11562915643056233, 0.15874161458995245, 0.28420019981472994, 0.4684831251700719, 0.7320004006226858, 0.22510115865250707, 0.3785420723175735, 0.7520663306826637, 1.2833541631698608, 2.047330434833254, 0.10347636209593879, 0.15466669554351478, 0.3006535242962581, 0.5098171078640482, 0.674809715535381, 0.4273643041911878, 0.8320349397011173]
worst_speeds= [1.9243056774139404, 2.055288791656494, 2.1968300342559814, 1.8782312870025635, 2.4975662231445312, 0.5578038692474365, 0.3666954040527344, 2.040642023086548, 2.780369281768799, 0.5123965740203857, 0.1739494800567627, 0.19903135299682617, 0.24152493476867676, 0.4822392463684082, 1.0441553592681885, 1.8402762413024902, 0.3315277099609375, 0.6791918277740479, 1.058586835861206, 1.644136905670166, 2.499473810195923, 0.18216156959533691, 0.3035452365875244, 0.5495758056640625, 0.9176466464996338, 1.3813481330871582, 0.7791266441345215, 1.470740795135498]
best_inference_speeds= [1.3186442852020264, 1.4654877185821533, 1.3700661659240723, 1.405256748199463, 2.0882694721221924, 0.29954051971435547, 0.14551043510437012, 1.5174963474273682, 2.158961534500122, 0.2494509220123291, 0.09437084197998047, 0.07900643348693848, 0.12017178535461426, 0.1930370330810547, 0.32104992866516113, 0.5035734176635742, 0.1704714298248291, 0.2926163673400879, 0.5926859378814697, 1.0141665935516357, 1.6597988605499268, 0.07199978828430176, 0.1070866584777832, 0.21702980995178223, 0.3591303825378418, 0.4792959690093994, 0.30332303047180176, 0.5961430072784424]
avg_inference_speeds= [1.5866728001330273, 1.7094634649081109, 1.7134452432394027, 1.6313878340893482, 2.2775334460394725, 0.4075895349184672, 0.2151956793702679, 1.7339828383354914, 2.4492330863362266, 0.36098582927997297, 0.11621259781251471, 0.11706211795545604, 0.1598539921774793, 0.29104919046969024, 0.4718808616910662, 0.7413130005200704, 0.22550442598868106, 0.3842933930848774, 0.7649800475639633, 1.3053585439920425, 2.0638596452312705, 0.10366734336404239, 0.1551857895967437, 0.3014895370207637, 0.5105531876345715, 0.6843890093621754, 0.42913957720711116, 0.8360498717853001]
worst_inference_speeds= [1.9243056774139404, 2.055288791656494, 2.1968300342559814, 1.8782312870025635, 2.4975662231445312, 0.5578038692474365, 0.3666954040527344, 2.040642023086548, 2.780369281768799, 0.5123965740203857, 0.1739494800567627, 0.19903135299682617, 0.24152493476867676, 0.4822392463684082, 1.0441553592681885, 1.8402762413024902, 0.3315277099609375, 0.6791918277740479, 1.058586835861206, 1.644136905670166, 2.499473810195923, 0.18216156959533691, 0.3035452365875244, 0.5495758056640625, 0.9176466464996338, 1.3813481330871582, 0.7791266441345215, 1.470740795135498]
total_detections= [129, 97, 99, 187, 217, 136, 128, 190, 211, 83, 98, 78, 73, 86, 96, 96, 71, 89, 105, 112, 112, 72, 93, 93, 92, 101, 95, 103]
total_inferences= [83, 78, 80, 83, 84, 84, 81, 84, 84, 78, 83, 73, 67, 74, 84, 84, 69, 76, 79, 80, 81, 68, 82, 83, 83, 84, 84, 84]


import cv2
import torch
from torchvision.models.detection import *
from torchvision.io.image import read_image
from torchvision.utils import draw_bounding_boxes
from torchvision.transforms.functional import to_pil_image
import torchvision.transforms as transforms
from PIL import Image
import IPython
from IPython.display import display, clear_output
import time
import traceback
import math
from ultralytics import YOLO

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

transform = transforms.Compose([
    #transforms.Resize((300, 300)),
    transforms.ToTensor(),
])

class DetectionModel():
    def __init__(self, concept, backbone, model, weights, reference, color=None):
        self.concept = concept
        self.backbone = backbone
        self.model = model(weights) if concept=="YOLO" else model(weights=weights.DEFAULT)
        if concept!="YOLO":
            self.model.to(device)
            self.model.eval()
        self.weights = weights if concept=="YOLO" else weights.DEFAULT
        self.reference = reference
        self.color = color
        self.detections = []

    def addDetection(self, detection):
        self.detections.append(detection)

    def getDetections(self):
        return self.detections

    def clearDetections(self):
        self.detections = []

    def detectionsCount(self):
        return len(self.detections)

    def inferencesCount(self):
        length = 0
        addedFrames = []
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    addedFrames.append(self.detections[i].frame)
        return len(addedFrames)

    def getAveragePrecision(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].precision
            avg = sum/len(self.detections)
        return  avg.item() if torch.is_tensor(avg) else avg

    def getMaxPrecision(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].precision:
                max=self.detections[i].precision
        return max.item() if torch.is_tensor(max) else max

    def getMinPrecision(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].precision:
                min=self.detections[i].precision
        return 0.0 if min==9999999.99 else min.item() if torch.is_tensor(min) else min

    def getAverageSpeed(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].speed
            avg = sum/len(self.detections)
        return avg

    def getMaxSpeed(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].speed:
                max=self.detections[i].speed
        return max

    def getMinSpeed(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].speed:
                min=self.detections[i].speed
        return 0.0 if min==9999999.99 else min

    def getAverageInferenceSpeed(self):
        sum=0.0
        avg=0.0
        addedFrames=[]
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    sum+=self.detections[i].speed
                    addedFrames.append(self.detections[i].frame)
            avg = sum/len(addedFrames) if len(addedFrames)>0 else 0.0
        return avg

    def getMaxInferenceSpeed(self):
        max=0.0
        addedFrames=[]
        for i in range(len(self.detections)):
            if max<self.detections[i].speed and self.detections[i].frame not in addedFrames:
                max=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return max

    def getMinInferenceSpeed(self):
        min=9999999.99
        addedFrames=[]
        for i in range(len(self.detections)):
            if min>self.detections[i].speed and self.detections[i].frame not in addedFrames:
                min=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return 0.0 if min==9999999.99 else min

    def filterPrecisionsBelow(self, minPrecision):
        newDetections = []
        for i in range(len(self.detections)):
            if self.detections[i].precision>=minPrecision:
                newDetections.append(self.detections[i])
        self.detections = newDetections

    def printResults(self):
        print("Model:", self.reference,
              "\nBest Precision:", self.getMaxPrecision(),
              "%\nAverage Precision:", self.getAveragePrecision(),
              "%\nWorst Precision:", self.getMinPrecision(),
              "%\nBest Speed:", self.getMinSpeed(),
              "seconds\nAverage Speed:", self.getAverageSpeed(),
              "seconds\nWorst Speed:", self.getMaxSpeed(),
              "seconds\nBest Inference Speed:", self.getMinInferenceSpeed(),
              "seconds\nAverage Inference Speed:", self.getAverageInferenceSpeed(),
              "seconds\nWorst Inference Speed:", self.getMaxInferenceSpeed(),
              "seconds\nTotal Detections:", self.detectionsCount(),
              "\nTotal Inferences:", self.inferencesCount(),
              "\n\n")

class DetectionResult():
    def __init__(self, model, label, precision, speed, device=None, source=None, frame=None, boundingBox=None, details=None):
        self.model = model
        self.label = label
        self.precision = precision.item() if torch.is_tensor(precision) else precision
        self.speed = speed.item() if torch.is_tensor(speed) else speed
        self.device = device
        self.source = source
        self.frame = frame
        self.boundingBox = boundingBox
        self.details = details

target_classes = ["truck", "bus", "car", "train", "bicycle"]


VEHICLE_DETECTION_MODELS = [
    DetectionModel("FCOS", "resnet50_fpn", fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights, "fcos_resnet", (255,0,0)),
    DetectionModel("RetinaNet", "resnet50_fpn", retinanet_resnet50_fpn, RetinaNet_ResNet50_FPN_Weights, "retinanet_resnet", (255,255,0)),
    DetectionModel("RetinaNet", "resnet50_fpn_v2", retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights, "retinanet_resnet_v2", (0,234,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn", fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights, "faster_rcnn_resnet", (170,0,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn_v2", fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights, "faster_rcnn_resnet_v2", (255,127,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_fpn", fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights, "faster_rcnn_mobilenet_v3", (191,255,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_320_fpn", fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights, "faster_rcnn_mobilenet_v3_320", (0,149,255)),
    DetectionModel("MaskRCNN", "resnet50_fpn", maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights, "mask_rcnn_resnet", (255,0,170)),
    DetectionModel("MaskRCNN", "resnet50_fpn_v2", maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights, "mask_rcnn_resnet_v2", (255,212,0)),
    DetectionModel("SSD300", "vgg16", ssd300_vgg16, SSD300_VGG16_Weights, "ssd_vgg16", (106,255,0)),
    DetectionModel("SSDLite320", "mobilenet_v3_large", ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights, "ssd_mobilenet_v3", (0,64,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5nu.pt', "yolov5nu", (237,185,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5su.pt', "yolov5su", (185,215,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5mu.pt', "yolov5mu", (231,233,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5lu.pt', "yolov5lu", (220,185,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5xu.pt', "yolov5xu", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5n6u.pt', "yolov5n6u", (143,35,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5s6u.pt', "yolov5s6u", (35,98,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5m6u.pt', "yolov5m6u", (143,106,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5l6u.pt', "yolov5l6u", (107,35,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5x6u.pt', "yolov5x6u", (79,143,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8n.pt', "yolov8n", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8s.pt', "yolov8s", (115,115,115)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8m.pt', "yolov8m", (204,204,204)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8l.pt', "yolov8l", (255,0,0)), # cycle restarts
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8x.pt', "yolov8x", (255,255,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9c.pt', "yolov9c", (0,234,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9e.pt', "yolov9e", (170,0,255))#,
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10n.pt', "yolov10n", (185,237,224)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10s.pt', "yolov10s", (115,115,115)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10m.pt', "yolov10m", (204,204,204)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10b.pt', "yolov10b", (204,204,204)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10l.pt', "yolov10l", (255,0,0)), # cycle restarts
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10x.pt', "yolov10x", (255,255,0))
]

def process(frame, frameIndex=-1, source=None):
    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        processed_frame = frame.copy()

        if currentModel.concept=="YOLO":
            start_time = time.time()
            results = currentModel.model(processed_frame, agnostic_nms=True, verbose=False, device=device)[0]
            processing_time = time.time() - start_time
            for result in results:
                detection_count = result.boxes.shape[0]
                for j in range(detection_count):
                    cls = int(result.boxes.cls[j].item())
                    name = result.names[cls]
                    confidence = float(result.boxes.conf[j].item())
                    bounding_box = result.boxes.xyxy[j].cpu().numpy()
                    if name in target_classes and confidence>0.5:
                        x1, y1, x2, y2 = [int(x) for x in bounding_box]
                        # Ensure x1 < x2 and y1 < y2
                        x1, x2 = min(x1, x2), max(x1, x2)
                        y1, y2 = min(y1, y2), max(y1, y2)
                        label = '{} ({:.2f}%) ({:.3f}s) {}'.format(name.upper(), confidence*100, processing_time, currentModel.reference)
                        print(label)
                        VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, name, confidence*100, processing_time, device, source, frameIndex, bounding_box, label))
                        cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                        cv2.putText(processed_frame, label, (x1+10, y1+25), 0, 0.8, currentModel.color, 2)
        else:
            # Convert frame to RGB and PIL Image, then apply transformation
            frame_rgb = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            image = transform(pil_image).unsqueeze(0).to(device)

            # Object detection
            start_time = time.time()
            with torch.no_grad():
                predictions = currentModel.model(image)

            # Processing time
            processing_time = time.time() - start_time

            # Visualization
            scores = predictions[0]['scores']
            boxes = predictions[0]['boxes']
            labels = predictions[0]['labels']
            confidence_threshold = 0.5

            for score, box, label in zip(scores, boxes, labels):
                # and score > confidence_threshold
                if currentModel.weights.meta["categories"][label.item()] in target_classes and score > confidence_threshold:
                    x1, y1, x2, y2 = map(int, box)
                    label_text = f'{currentModel.weights.meta["categories"][label.item()].upper()} ({score*100:.2f}%) ({processing_time:.3f}s) {currentModel.reference}'
                    print(label_text)
                    VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, currentModel.weights.meta["categories"][label.item()], float(score.item())*100, processing_time, device, source, frameIndex, box, label_text))
                    cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                    cv2.putText(processed_frame, label_text, (x1, y1+20), 0, 0.8, currentModel.color, 2)

        # img = Image.fromarray(processed_frame, 'RGB')
        # display(img)
        clear_output(wait=True)
    return frame

def handleFrame(frame, frameIndex=-1, source=None):
    processed_frame = process(frame,frameIndex,source)

def handleRelease():
    cam.release()
    print("Source released.")

    print(device)
    print(source)

    BestPrecision=None
    BestAvgPrecision=None
    WorstPrecision=None
    BestSpeed=None
    BestAvgSpeed=None
    WorstSpeed=None

    Models=[]
    BestPrecisions=[]
    AvgPrecisions=[]
    WorstPrecisions=[]
    BestSpeeds=[]
    AvgSpeeds=[]
    WorstSpeeds=[]
    BestInferenceSpeeds=[]
    AvgInferenceSpeeds=[]
    WorstInferenceSpeeds=[]
    TotalDetectionsArr=[]
    TotalInferencesArr=[]

    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        # currentModel.printResults()

        Models.append(currentModel.reference)
        BestPrecisions.append(currentModel.getMaxPrecision())
        AvgPrecisions.append(currentModel.getAveragePrecision())
        WorstPrecisions.append(currentModel.getMinPrecision())
        BestSpeeds.append(currentModel.getMinSpeed())
        AvgSpeeds.append(currentModel.getAverageSpeed())
        WorstSpeeds.append(currentModel.getMaxSpeed())
        BestInferenceSpeeds.append(currentModel.getMinInferenceSpeed())
        AvgInferenceSpeeds.append(currentModel.getAverageInferenceSpeed())
        WorstInferenceSpeeds.append(currentModel.getMaxInferenceSpeed())
        TotalDetectionsArr.append(currentModel.detectionsCount())
        TotalInferencesArr.append(currentModel.inferencesCount())

        if BestPrecision is None or BestPrecision.getMaxPrecision()<=currentModel.getMaxPrecision():
            BestPrecision = currentModel
        if BestAvgPrecision is None or BestAvgPrecision.getAveragePrecision()<=currentModel.getAveragePrecision():
            BestAvgPrecision = currentModel
        if WorstPrecision is None or WorstPrecision.getMinPrecision()>=currentModel.getMinPrecision():
            WorstPrecision = currentModel
        if BestSpeed is None or BestSpeed.getMinSpeed()>=currentModel.getMinSpeed():
            BestSpeed = currentModel
        if BestAvgSpeed is None or BestAvgSpeed.getAverageSpeed()>=currentModel.getAverageSpeed():
            BestAvgSpeed = currentModel
        if WorstSpeed is None or WorstSpeed.getMaxSpeed()<=currentModel.getMaxSpeed():
            WorstSpeed = currentModel

    if BestPrecision is not None:
        print("Best Precision:")
        BestPrecision.printResults()
    if BestAvgPrecision is not None:
        print("Best Average Precision:")
        BestAvgPrecision.printResults()
    if WorstPrecision is not None:
        print("Worst Precision:")
        WorstPrecision.printResults()
    if BestSpeed is not None:
        print("Best Speed:")
        BestSpeed.printResults()
    if BestAvgSpeed is not None:
        print("Best Average Speed:")
        BestAvgSpeed.printResults()
    if WorstSpeed is not None:
        print("Worst Speed:")
        WorstSpeed.printResults()

    print("models=",Models)
    print("best_precisions=",BestPrecisions)
    print("avg_precisions=",AvgPrecisions)
    print("worst_precisions=",WorstPrecisions)
    print("best_speeds=",BestSpeeds)
    print("avg_speeds=",AvgSpeeds)
    print("worst_speeds=",WorstSpeeds)
    print("best_inference_speeds=",BestInferenceSpeeds)
    print("avg_inference_speeds=",AvgInferenceSpeeds)
    print("worst_inference_speeds=",WorstInferenceSpeeds)
    print("total_detections=",TotalDetectionsArr)
    print("total_inferences=",TotalInferencesArr)

inputType="video"
source = "data/video/vecteezy_car-and-truck-traffic-on-the-highway-in-europe-poland_7957364.MP4" if inputType=="video" else 0
cam = cv2.VideoCapture(source)
display_handle=display(None, display_id=True)
every = 60

start = 0 if source!=0 else -1
end = int(cam.get(cv2.CAP_PROP_FRAME_COUNT)) if source!=0 else -1
if source!=0: cam.set(1, start)

frameCount=start
while_safety=0
saved_count=0

if not cam.isOpened(): print("Error: Could not open source.")
else:
    try:
        while True if end<0 else frameCount<end:
            _, frame = cam.read()
            if frame is None:
                if source!=0:
                    if while_safety > 2000: break
                    while_safety += 1
                    continue
                else:
                    print("Error: Could not capture frame.")
                    cam.release()
                    break

            if every>0:
                if (frameCount+1)%math.floor(every) == 0:
                    while_safety = 0
                    handleFrame(frame, frameCount, source)
            else:
                while_safety=0
                handleFrame(frame)
            frameCount += 1
        handleRelease()
    except KeyboardInterrupt:
        handleRelease()
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Source could not be released.")
        except:
            pass
        traceback.print_exc()


models= ['fcos_resnet', 'retinanet_resnet', 'retinanet_resnet_v2', 'faster_rcnn_resnet', 'faster_rcnn_resnet_v2', 'faster_rcnn_mobilenet_v3', 'faster_rcnn_mobilenet_v3_320', 'mask_rcnn_resnet', 'mask_rcnn_resnet_v2', 'ssd_vgg16', 'ssd_mobilenet_v3', 'yolov5nu', 'yolov5su', 'yolov5mu', 'yolov5lu', 'yolov5xu', 'yolov5n6u', 'yolov5s6u', 'yolov5m6u', 'yolov5l6u', 'yolov5x6u', 'yolov8n', 'yolov8s', 'yolov8m', 'yolov8l', 'yolov8x', 'yolov9c', 'yolov9e']
best_precisions= [88.38133811950684, 94.31947469711304, 94.65104341506958, 99.8128354549408, 99.86345171928406, 99.15496110916138, 99.13085103034973, 99.87894892692566, 99.84723925590515, 94.76912021636963, 98.03178310394287, 91.97649955749512, 94.07822489738464, 96.37896418571472, 95.80461382865906, 97.3269522190094, 94.2396879196167, 95.54755091667175, 96.92941308021545, 96.77505493164062, 96.98181748390198, 95.45491933822632, 95.18426060676575, 96.5444028377533, 96.16449475288391, 96.99693322181702, 96.3855504989624, 97.04502820968628]
avg_precisions= [66.38103199529124, 70.3169395999303, 70.4655567536483, 79.88080206767533, 85.44702819415501, 78.62774114454946, 80.35412695672777, 81.8152489606291, 84.55543837547302, 75.39921685269005, 71.74324222973415, 66.3287295235528, 75.24836903268641, 75.4142104569128, 76.96108240929861, 77.87653480257306, 72.0814951828548, 76.98340313509107, 78.38425508109472, 79.01228727264838, 80.49517690494497, 70.95426238245435, 76.98351041130398, 76.53432935476303, 78.49710430159713, 79.66656900834346, 79.37960922718048, 79.35655668600282]
worst_precisions= [50.54351091384888, 50.141048431396484, 50.10627508163452, 50.60608983039856, 50.11507272720337, 50.00733137130737, 50.4946231842041, 50.688743591308594, 50.22878646850586, 54.57262396812439, 53.01353931427002, 50.18031597137451, 51.69903039932251, 50.16433000564575, 50.79203248023987, 51.136183738708496, 50.56397318840027, 52.83355712890625, 50.405651330947876, 50.280821323394775, 50.93953609466553, 51.906341314315796, 50.88526010513306, 52.15650200843811, 51.11284852027893, 53.600770235061646, 50.20453929901123, 50.40505528450012]
best_speeds= [1.299997091293335, 1.5140001773834229, 1.485999345779419, 1.462993860244751, 2.192002058029175, 0.5210022926330566, 0.17803263664245605, 2.2490005493164062, 3.2149953842163086, 0.5359988212585449, 0.1399993896484375, 0.08900046348571777, 0.10800027847290039, 0.21200156211853027, 0.35700106620788574, 0.6059985160827637, 0.19199872016906738, 0.3240010738372803, 0.692997932434082, 1.1499993801116943, 1.7750039100646973, 0.0729987621307373, 0.12000012397766113, 0.22300076484680176, 0.37400102615356445, 0.5530011653900146, 0.33100056648254395, 0.6849989891052246]
avg_speeds= [1.5234267659239717, 1.5866820282406278, 1.5734234790544253, 1.58338570409967, 2.241606026036399, 0.5955235035188736, 0.24316815976743344, 3.592290658503771, 3.9164626541137695, 0.6119565838261655, 0.14599949972970144, 0.16447197728686863, 0.15861440246755426, 0.31266805681131654, 0.5092772604927184, 0.851857328414917, 0.21815848350524902, 0.39068909734487534, 0.7956346568240914, 1.3580221262845127, 2.154871307393556, 0.09216646353403728, 0.15645505552706512, 0.28819008270899454, 0.5068418618404504, 0.7337302677873252, 0.42248673571480644, 0.8627905454208602]
worst_speeds= [2.169999361038208, 1.6749987602233887, 1.7139310836791992, 1.7379984855651855, 2.313999652862549, 0.7430028915405273, 0.371002197265625, 5.202039957046509, 4.391000270843506, 0.6829984188079834, 0.1569991111755371, 0.3240015506744385, 0.3469994068145752, 0.6659984588623047, 0.9969990253448486, 1.6079983711242676, 0.28899645805358887, 0.5569989681243896, 1.0540006160736084, 2.050997018814087, 3.2959985733032227, 0.13899970054626465, 0.24999666213989258, 0.4759986400604248, 0.7979998588562012, 1.1129977703094482, 0.6710002422332764, 1.2559983730316162]
best_inference_speeds= [1.299997091293335, 1.5140001773834229, 1.485999345779419, 1.462993860244751, 2.192002058029175, 0.5210022926330566, 0.17803263664245605, 2.2490005493164062, 3.2149953842163086, 0.5359988212585449, 0.1399993896484375, 0.08900046348571777, 0.10800027847290039, 0.21200156211853027, 0.35700106620788574, 0.6059985160827637, 0.19199872016906738, 0.3240010738372803, 0.692997932434082, 1.1499993801116943, 1.7750039100646973, 0.0729987621307373, 0.12000012397766113, 0.22300076484680176, 0.37400102615356445, 0.5530011653900146, 0.33100056648254395, 0.6849989891052246]
avg_inference_speeds= [1.5179980754852296, 1.5901995182037354, 1.5763912200927734, 1.5865981578826904, 2.2412064552307127, 0.5936071395874023, 0.2280205726623535, 3.6154096126556396, 3.9105986595153808, 0.6148121356964111, 0.1463327407836914, 0.1531996726989746, 0.16260075569152832, 0.31900744438171386, 0.514991044998169, 0.8532001495361328, 0.2203977108001709, 0.3934015274047852, 0.7880002498626709, 1.3631993293762208, 2.151800298690796, 0.09079985618591309, 0.15819840431213378, 0.2958066940307617, 0.5173938274383545, 0.7190060615539551, 0.42440056800842285, 0.8499995708465576]
worst_inference_speeds= [2.169999361038208, 1.6749987602233887, 1.7139310836791992, 1.7379984855651855, 2.313999652862549, 0.7430028915405273, 0.371002197265625, 5.202039957046509, 4.391000270843506, 0.6829984188079834, 0.1569991111755371, 0.3240015506744385, 0.3469994068145752, 0.6659984588623047, 0.9969990253448486, 1.6079983711242676, 0.28899645805358887, 0.5569989681243896, 1.0540006160736084, 2.050997018814087, 3.2959985733032227, 0.13899970054626465, 0.24999666213989258, 0.4759986400604248, 0.7979998588562012, 1.1129977703094482, 0.6710002422332764, 1.2559983730316162]
total_detections= [91, 63, 74, 129, 140, 62, 27, 128, 125, 19, 7, 36, 44, 59, 63, 70, 56, 64, 93, 88, 93, 36, 46, 60, 66, 69, 72, 67]
total_inferences= [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]


import numpy as np
import matplotlib.pyplot as plt
import matplotlib

# Generate a color map
num_models = len(models)
cmap = matplotlib.colormaps.get_cmap('nipy_spectral')
colors = [cmap(i / num_models) for i in range(num_models)]

# Create a figure and plot with unique colors
plt.figure(figsize=(14, 10))
for i, model in enumerate(models):
    plt.scatter(avg_inference_speeds[i], avg_precisions[i], color=colors[i], label=model, edgecolor='black')

print("DEVICE: CPU (Intel Core i7-14700KF)")
print("SOURCE: data/video/sample.MP4 (1080p60fps Video)")
print("SKIP EVERY 60 FRAMES")
print("TOTALLY PROCESSED FRAMES: 84")
print("COCO Dataset, Classes truck, bus, car, train, bicycle")
print("ALL >50%")
# Axis labels and plot title
plt.xlabel('Average Speed (seconds)')
plt.ylabel('Average Precision (%)')
plt.title('Model Performance: Average Inference Speed x Average Accuracy')
plt.legend(loc='upper left', bbox_to_anchor=(1.05, 1), fontsize='small')
plt.grid(True)
plt.savefig("Analysis/Experiment1/i7_14700KF/Experiment1_Diagram.png", bbox_inches='tight')
plt.show()

# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_precisions, avg_precisions, worst_precisions])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Precision (%)", "Average Precision (%)", "Worst Precision (%)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

plt.title("Model Precision Metrics")
plt.savefig("Analysis/Experiment1/i7_14700KF/Experiment1_Precision_Table.png", bbox_inches='tight')
plt.show()

# # Set up the figure and axis for the table
# fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
# ax.axis('tight')
# ax.axis('off')

# # The table data: transpose the array to make each column a different metric
# table_data = np.transpose([models, best_speeds, avg_speeds, worst_speeds])

# # Create the table in the plot
# table = ax.table(cellText=table_data, colLabels=["Model", "Best Speed (s)", "Average Speed (s)", "Worst Speed (s)"],
#                  cellLoc='center', loc='center', colColours=["palegreen"] * 4)
# table.auto_set_font_size(False)
# table.set_fontsize(10)
# table.scale(1.2, 1.2)  # Scale table size

# plt.title("Model Speed Metrics")
# plt.show()

# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_inference_speeds, avg_inference_speeds, worst_inference_speeds])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Inference Speed (s)", "Average Inference Speed (s)", "Worst Inference Speed (s)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

plt.title("Model Inference Speed Metrics")
plt.savefig("Analysis/Experiment1/i7_14700KF/Experiment1_Time_Table.png", bbox_inches='tight')
plt.show()





import cv2
import torch
from torchvision.models.detection import *
from torchvision.io.image import read_image
from torchvision.utils import draw_bounding_boxes
from torchvision.transforms.functional import to_pil_image
import torchvision.transforms as transforms
from PIL import Image
import IPython
from IPython.display import display, clear_output
import time
import traceback
import math
from ultralytics import YOLO

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

transform = transforms.Compose([
    #transforms.Resize((300, 300)),
    transforms.ToTensor(),
])

class DetectionModel():
    def __init__(self, concept, backbone, model, weights, reference, color=None):
        self.concept = concept
        self.backbone = backbone
        self.model = model(weights) if concept=="YOLO" else model(weights=weights.DEFAULT)
        if concept!="YOLO":
            self.model.to(device)
            self.model.eval()
        self.weights = weights if concept=="YOLO" else weights.DEFAULT
        self.reference = reference
        self.color = color
        self.detections = []

    def addDetection(self, detection):
        self.detections.append(detection)

    def getDetections(self):
        return self.detections

    def clearDetections(self):
        self.detections = []

    def detectionsCount(self):
        return len(self.detections)

    def inferencesCount(self):
        length = 0
        addedFrames = []
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    addedFrames.append(self.detections[i].frame)
        return len(addedFrames)

    def getAveragePrecision(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].precision
            avg = sum/len(self.detections)
        return  avg.item() if torch.is_tensor(avg) else avg

    def getMaxPrecision(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].precision:
                max=self.detections[i].precision
        return max.item() if torch.is_tensor(max) else max

    def getMinPrecision(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].precision:
                min=self.detections[i].precision
        return 0.0 if min==9999999.99 else min.item() if torch.is_tensor(min) else min

    def getAverageSpeed(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].speed
            avg = sum/len(self.detections)
        return avg

    def getMaxSpeed(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].speed:
                max=self.detections[i].speed
        return max

    def getMinSpeed(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].speed:
                min=self.detections[i].speed
        return 0.0 if min==9999999.99 else min

    def getAverageInferenceSpeed(self):
        sum=0.0
        avg=0.0
        addedFrames=[]
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    sum+=self.detections[i].speed
                    addedFrames.append(self.detections[i].frame)
            avg = sum/len(addedFrames) if len(addedFrames)>0 else 0.0
        return avg

    def getMaxInferenceSpeed(self):
        max=0.0
        addedFrames=[]
        for i in range(len(self.detections)):
            if max<self.detections[i].speed and self.detections[i].frame not in addedFrames:
                max=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return max

    def getMinInferenceSpeed(self):
        min=9999999.99
        addedFrames=[]
        for i in range(len(self.detections)):
            if min>self.detections[i].speed and self.detections[i].frame not in addedFrames:
                min=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return 0.0 if min==9999999.99 else min

    def filterPrecisionsBelow(self, minPrecision):
        newDetections = []
        for i in range(len(self.detections)):
            if self.detections[i].precision>=minPrecision:
                newDetections.append(self.detections[i])
        self.detections = newDetections

    def printResults(self):
        print("Model:", self.reference,
              "\nBest Precision:", self.getMaxPrecision(),
              "%\nAverage Precision:", self.getAveragePrecision(),
              "%\nWorst Precision:", self.getMinPrecision(),
              "%\nBest Speed:", self.getMinSpeed(),
              "seconds\nAverage Speed:", self.getAverageSpeed(),
              "seconds\nWorst Speed:", self.getMaxSpeed(),
              "seconds\nBest Inference Speed:", self.getMinInferenceSpeed(),
              "seconds\nAverage Inference Speed:", self.getAverageInferenceSpeed(),
              "seconds\nWorst Inference Speed:", self.getMaxInferenceSpeed(),
              "seconds\nTotal Detections:", self.detectionsCount(),
              "\nTotal Inferences:", self.inferencesCount(),
              "\n\n")

class DetectionResult():
    def __init__(self, model, label, precision, speed, device=None, source=None, frame=None, boundingBox=None, details=None):
        self.model = model
        self.label = label
        self.precision = precision.item() if torch.is_tensor(precision) else precision
        self.speed = speed.item() if torch.is_tensor(speed) else speed
        self.device = device
        self.source = source
        self.frame = frame
        self.boundingBox = boundingBox
        self.details = details

target_classes = ["truck", "bus", "car", "train", "bicycle"]


VEHICLE_DETECTION_MODELS = [
    DetectionModel("FCOS", "resnet50_fpn", fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights, "fcos_resnet", (255,0,0)),
    DetectionModel("RetinaNet", "resnet50_fpn", retinanet_resnet50_fpn, RetinaNet_ResNet50_FPN_Weights, "retinanet_resnet", (255,255,0)),
    DetectionModel("RetinaNet", "resnet50_fpn_v2", retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights, "retinanet_resnet_v2", (0,234,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn", fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights, "faster_rcnn_resnet", (170,0,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn_v2", fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights, "faster_rcnn_resnet_v2", (255,127,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_fpn", fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights, "faster_rcnn_mobilenet_v3", (191,255,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_320_fpn", fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights, "faster_rcnn_mobilenet_v3_320", (0,149,255)),
    DetectionModel("MaskRCNN", "resnet50_fpn", maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights, "mask_rcnn_resnet", (255,0,170)),
    DetectionModel("MaskRCNN", "resnet50_fpn_v2", maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights, "mask_rcnn_resnet_v2", (255,212,0)),
    DetectionModel("SSD300", "vgg16", ssd300_vgg16, SSD300_VGG16_Weights, "ssd_vgg16", (106,255,0)),
    DetectionModel("SSDLite320", "mobilenet_v3_large", ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights, "ssd_mobilenet_v3", (0,64,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5nu.pt', "yolov5nu", (237,185,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5su.pt', "yolov5su", (185,215,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5mu.pt', "yolov5mu", (231,233,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5lu.pt', "yolov5lu", (220,185,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5xu.pt', "yolov5xu", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5n6u.pt', "yolov5n6u", (143,35,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5s6u.pt', "yolov5s6u", (35,98,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5m6u.pt', "yolov5m6u", (143,106,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5l6u.pt', "yolov5l6u", (107,35,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5x6u.pt', "yolov5x6u", (79,143,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8n.pt', "yolov8n", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8s.pt', "yolov8s", (115,115,115)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8m.pt', "yolov8m", (204,204,204)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8l.pt', "yolov8l", (255,0,0)), # cycle restarts
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8x.pt', "yolov8x", (255,255,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9c.pt', "yolov9c", (0,234,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9e.pt', "yolov9e", (170,0,255))#,
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10n.pt', "yolov10n", (185,237,224)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10s.pt', "yolov10s", (115,115,115)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10m.pt', "yolov10m", (204,204,204)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10b.pt', "yolov10b", (204,204,204)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10l.pt', "yolov10l", (255,0,0)), # cycle restarts
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10x.pt', "yolov10x", (255,255,0))
]

def process(frame, frameIndex=-1, source=None):
    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        processed_frame = frame.copy()

        if currentModel.concept=="YOLO":
            start_time = time.time()
            results = currentModel.model(processed_frame, agnostic_nms=True, verbose=False, device=device)[0]
            processing_time = time.time() - start_time
            for result in results:
                detection_count = result.boxes.shape[0]
                for j in range(detection_count):
                    cls = int(result.boxes.cls[j].item())
                    name = result.names[cls]
                    confidence = float(result.boxes.conf[j].item())
                    bounding_box = result.boxes.xyxy[j].cpu().numpy()
                    if name in target_classes and confidence>0.5:
                        x1, y1, x2, y2 = [int(x) for x in bounding_box]
                        # Ensure x1 < x2 and y1 < y2
                        x1, x2 = min(x1, x2), max(x1, x2)
                        y1, y2 = min(y1, y2), max(y1, y2)
                        label = '{} ({:.2f}%) ({:.3f}s) {}'.format(name.upper(), confidence*100, processing_time, currentModel.reference)
                        print(label)
                        VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, name, confidence*100, processing_time, device, source, frameIndex, bounding_box, label))
                        cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                        cv2.putText(processed_frame, label, (x1+10, y1+25), 0, 0.8, currentModel.color, 2)
        else:
            # Convert frame to RGB and PIL Image, then apply transformation
            frame_rgb = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            image = transform(pil_image).unsqueeze(0).to(device)

            # Object detection
            start_time = time.time()
            with torch.no_grad():
                predictions = currentModel.model(image)

            # Processing time
            processing_time = time.time() - start_time

            # Visualization
            scores = predictions[0]['scores']
            boxes = predictions[0]['boxes']
            labels = predictions[0]['labels']
            confidence_threshold = 0.5

            for score, box, label in zip(scores, boxes, labels):
                # and score > confidence_threshold
                if currentModel.weights.meta["categories"][label.item()] in target_classes and score > confidence_threshold:
                    x1, y1, x2, y2 = map(int, box)
                    label_text = f'{currentModel.weights.meta["categories"][label.item()].upper()} ({score*100:.2f}%) ({processing_time:.3f}s) {currentModel.reference}'
                    print(label_text)
                    VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, currentModel.weights.meta["categories"][label.item()], float(score.item())*100, processing_time, device, source, frameIndex, box, label_text))
                    cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                    cv2.putText(processed_frame, label_text, (x1, y1+20), 0, 0.8, currentModel.color, 2)

        img = Image.fromarray(processed_frame, 'RGB')
        display(img)
        clear_output(wait=True)
    return frame

def handleFrame(frame, frameIndex=-1, source=None):
    processed_frame = process(frame,frameIndex,source)

def handleRelease():
    cam.release()
    print("Source released.")

    print(device)
    print(source)

    BestPrecision=None
    BestAvgPrecision=None
    WorstPrecision=None
    BestSpeed=None
    BestAvgSpeed=None
    WorstSpeed=None

    Models=[]
    BestPrecisions=[]
    AvgPrecisions=[]
    WorstPrecisions=[]
    BestSpeeds=[]
    AvgSpeeds=[]
    WorstSpeeds=[]
    BestInferenceSpeeds=[]
    AvgInferenceSpeeds=[]
    WorstInferenceSpeeds=[]
    TotalDetectionsArr=[]
    TotalInferencesArr=[]

    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        # currentModel.printResults()

        Models.append(currentModel.reference)
        BestPrecisions.append(currentModel.getMaxPrecision())
        AvgPrecisions.append(currentModel.getAveragePrecision())
        WorstPrecisions.append(currentModel.getMinPrecision())
        BestSpeeds.append(currentModel.getMinSpeed())
        AvgSpeeds.append(currentModel.getAverageSpeed())
        WorstSpeeds.append(currentModel.getMaxSpeed())
        BestInferenceSpeeds.append(currentModel.getMinInferenceSpeed())
        AvgInferenceSpeeds.append(currentModel.getAverageInferenceSpeed())
        WorstInferenceSpeeds.append(currentModel.getMaxInferenceSpeed())
        TotalDetectionsArr.append(currentModel.detectionsCount())
        TotalInferencesArr.append(currentModel.inferencesCount())

        if BestPrecision is None or BestPrecision.getMaxPrecision()<=currentModel.getMaxPrecision():
            BestPrecision = currentModel
        if BestAvgPrecision is None or BestAvgPrecision.getAveragePrecision()<=currentModel.getAveragePrecision():
            BestAvgPrecision = currentModel
        if WorstPrecision is None or WorstPrecision.getMinPrecision()>=currentModel.getMinPrecision():
            WorstPrecision = currentModel
        if BestSpeed is None or BestSpeed.getMinSpeed()>=currentModel.getMinSpeed():
            BestSpeed = currentModel
        if BestAvgSpeed is None or BestAvgSpeed.getAverageSpeed()>=currentModel.getAverageSpeed():
            BestAvgSpeed = currentModel
        if WorstSpeed is None or WorstSpeed.getMaxSpeed()<=currentModel.getMaxSpeed():
            WorstSpeed = currentModel

    if BestPrecision is not None:
        print("Best Precision:")
        BestPrecision.printResults()
    if BestAvgPrecision is not None:
        print("Best Average Precision:")
        BestAvgPrecision.printResults()
    if WorstPrecision is not None:
        print("Worst Precision:")
        WorstPrecision.printResults()
    if BestSpeed is not None:
        print("Best Speed:")
        BestSpeed.printResults()
    if BestAvgSpeed is not None:
        print("Best Average Speed:")
        BestAvgSpeed.printResults()
    if WorstSpeed is not None:
        print("Worst Speed:")
        WorstSpeed.printResults()

    print("models=",Models)
    print("best_precisions=",BestPrecisions)
    print("avg_precisions=",AvgPrecisions)
    print("worst_precisions=",WorstPrecisions)
    print("best_speeds=",BestSpeeds)
    print("avg_speeds=",AvgSpeeds)
    print("worst_speeds=",WorstSpeeds)
    print("best_inference_speeds=",BestInferenceSpeeds)
    print("avg_inference_speeds=",AvgInferenceSpeeds)
    print("worst_inference_speeds=",WorstInferenceSpeeds)
    print("total_detections=",TotalDetectionsArr)
    print("total_inferences=",TotalInferencesArr)

inputType="video"
source = "data/video/vecteezy_car-and-truck-traffic-on-the-highway-in-europe-poland_7957364.MP4" if inputType=="video" else 0
cam = cv2.VideoCapture(source)
display_handle=display(None, display_id=True)
every = 30

start = 0 if source!=0 else -1
end = int(cam.get(cv2.CAP_PROP_FRAME_COUNT)) if source!=0 else -1
if source!=0: cam.set(1, start)

frameCount=start
while_safety=0
saved_count=0

if not cam.isOpened(): print("Error: Could not open source.")
else:
    try:
        while True if end<0 else frameCount<end:
            _, frame = cam.read()
            if frame is None:
                if source!=0:
                    if while_safety > 2000: break
                    while_safety += 1
                    continue
                else:
                    print("Error: Could not capture frame.")
                    cam.release()
                    break

            if every>0:
                if (frameCount+1)%math.floor(every) == 0:
                    while_safety = 0
                    handleFrame(frame, frameCount, source)
            else:
                while_safety=0
                handleFrame(frame)
            frameCount += 1
        handleRelease()
    except KeyboardInterrupt:
        handleRelease()
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Source could not be released.")
        except:
            pass
        traceback.print_exc()


models= ['fcos_resnet', 'retinanet_resnet', 'retinanet_resnet_v2', 'faster_rcnn_resnet', 'faster_rcnn_resnet_v2', 'faster_rcnn_mobilenet_v3', 'faster_rcnn_mobilenet_v3_320', 'mask_rcnn_resnet', 'mask_rcnn_resnet_v2', 'ssd_vgg16', 'ssd_mobilenet_v3', 'yolov5nu', 'yolov5su', 'yolov5mu', 'yolov5lu', 'yolov5xu', 'yolov5n6u', 'yolov5s6u', 'yolov5m6u', 'yolov5l6u', 'yolov5x6u', 'yolov8n', 'yolov8s', 'yolov8m', 'yolov8l', 'yolov8x', 'yolov9c', 'yolov9e']
best_precisions= [88.38133811950684, 96.5210497379303, 95.00588178634644, 99.93738532066345, 99.87070560455322, 99.32159781455994, 99.72949624061584, 99.91301894187927, 99.91182684898376, 98.0389416217804, 98.03178310394287, 91.97649955749512, 94.07822489738464, 96.37896418571472, 95.80461382865906, 97.3269522190094, 94.2396879196167, 95.54755091667175, 96.92941308021545, 97.27439284324646, 96.98181748390198, 95.45491933822632, 95.18426060676575, 96.5444028377533, 96.16449475288391, 96.99693322181702, 96.51097059249878, 97.04502820968628]
avg_precisions= [65.88658039928764, 70.20814386697916, 70.43154467093318, 79.90512614962698, 84.78623870760202, 80.90001817577141, 84.73448807542974, 81.36211392533689, 84.39679010556294, 74.3879671394825, 79.74823500428882, 68.64530959644833, 75.47572281049645, 75.31852468848228, 76.93267973569723, 78.24879854703121, 72.94682849634874, 77.37568999064787, 78.43043037379782, 79.05915665626526, 80.31515035953821, 72.15728767930645, 76.0428299807539, 76.25905139659478, 78.3721436315508, 79.44863706395246, 79.53483669470388, 79.5822869406806]
worst_precisions= [50.043344497680664, 50.141048431396484, 50.0144362449646, 50.187039375305176, 50.11507272720337, 50.00733137130737, 50.4946231842041, 50.41531324386597, 50.11341571807861, 51.834869384765625, 53.01353931427002, 50.18031597137451, 51.69903039932251, 50.16433000564575, 50.14612674713135, 50.73273777961731, 50.56397318840027, 51.35229825973511, 50.405651330947876, 50.280821323394775, 50.68036913871765, 50.467002391815186, 50.88526010513306, 50.05417466163635, 50.525641441345215, 50.45931339263916, 50.20453929901123, 50.40505528450012]
best_speeds= [1.4419939517974854, 1.6030006408691406, 1.6049995422363281, 1.6340000629425049, 2.2949628829956055, 0.6049995422363281, 0.21399903297424316, 2.4310033321380615, 3.003432035446167, 0.4179999828338623, 0.16900110244750977, 0.09299898147583008, 0.15099811553955078, 0.30900096893310547, 0.48999905586242676, 0.5900020599365234, 0.2409987449645996, 0.42099881172180176, 0.6579997539520264, 1.296961784362793, 2.0330018997192383, 0.09999871253967285, 0.16299772262573242, 0.2819983959197998, 0.5230000019073486, 0.6169986724853516, 0.4529988765716553, 0.733999490737915]
avg_speeds= [1.5701232597392092, 1.7835977462621835, 1.7622825001415454, 1.7149992025674987, 2.393650323152542, 0.6497629713421026, 0.26257901191711425, 3.2317879127182123, 3.424077913394341, 0.5685714960098267, 0.2062422377722604, 0.2119327396959872, 0.2273447124854378, 0.4024129827817281, 0.6614861543361957, 0.9679857792614175, 0.33664346815229534, 0.503007837834249, 0.897259154667457, 1.5135456371307372, 2.3972167956267354, 0.14557499428317972, 0.21899981209726044, 0.38500520853492304, 0.6263469653343087, 0.8093072210533031, 0.551460567095601, 0.9900218857659234]
worst_speeds= [1.8153979778289795, 2.00600004196167, 2.0221517086029053, 1.8604893684387207, 2.5209238529205322, 0.7217705249786377, 0.3600001335144043, 9.641268730163574, 4.771999835968018, 0.7350001335144043, 0.25100040435791016, 0.8219983577728271, 0.4139983654022217, 0.6999993324279785, 1.1500000953674316, 1.835000991821289, 0.40000224113464355, 0.6510004997253418, 1.3540000915527344, 2.5025761127471924, 3.8429982662200928, 0.29400062561035156, 0.41300106048583984, 0.7379984855651855, 1.0570011138916016, 1.4489991664886475, 0.9169983863830566, 1.682999610900879]
best_inference_speeds= [1.4419939517974854, 1.6030006408691406, 1.6049995422363281, 1.6340000629425049, 2.2949628829956055, 0.6049995422363281, 0.21399903297424316, 2.4310033321380615, 3.003432035446167, 0.4179999828338623, 0.16900110244750977, 0.09299898147583008, 0.15099811553955078, 0.30900096893310547, 0.48999905586242676, 0.5900020599365234, 0.2409987449645996, 0.42099881172180176, 0.6579997539520264, 1.296961784362793, 2.0330018997192383, 0.09999871253967285, 0.16299772262573242, 0.2819983959197998, 0.5230000019073486, 0.6169986724853516, 0.4529988765716553, 0.733999490737915]
avg_inference_speeds= [1.5722375154495238, 1.7815402269363403, 1.756295347213745, 1.7152619123458863, 2.392438006401062, 0.6513879537582398, 0.2705963373184204, 3.3156680345535277, 3.4347437381744386, 0.5586000204086303, 0.20181339127676828, 0.2191002607345581, 0.23090670108795167, 0.40499653816223147, 0.6679117202758789, 0.9654001474380494, 0.33820385932922364, 0.4996068000793457, 0.8890566110610962, 1.5030311584472655, 2.3905657291412354, 0.147499680519104, 0.22209980487823486, 0.3863356590270996, 0.6245957851409912, 0.8186956882476807, 0.550099539756775, 0.988499665260315]
worst_inference_speeds= [1.8153979778289795, 2.00600004196167, 2.0221517086029053, 1.8604893684387207, 2.5209238529205322, 0.7217705249786377, 0.3600001335144043, 9.641268730163574, 4.771999835968018, 0.7350001335144043, 0.25100040435791016, 0.8219983577728271, 0.4139983654022217, 0.6999993324279785, 1.1500000953674316, 1.835000991821289, 0.40000224113464355, 0.6510004997253418, 1.3540000915527344, 2.5025761127471924, 3.8429982662200928, 0.29400062561035156, 0.41300106048583984, 0.7379984855651855, 1.0570011138916016, 1.4489991664886475, 0.9169983863830566, 1.682999610900879]
total_detections= [186, 130, 152, 261, 288, 121, 55, 262, 260, 40, 14, 74, 92, 120, 130, 139, 111, 131, 192, 175, 191, 73, 99, 123, 134, 138, 141, 135]
total_inferences= [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 7, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]


import numpy as np
import matplotlib.pyplot as plt
import matplotlib

# Generate a color map
num_models = len(models)
cmap = matplotlib.colormaps.get_cmap('nipy_spectral')
colors = [cmap(i / num_models) for i in range(num_models)]

# Create a figure and plot with unique colors
plt.figure(figsize=(14, 10))
for i, model in enumerate(models):
    plt.scatter(avg_inference_speeds[i], avg_precisions[i], color=colors[i], label=model, edgecolor='black')

print("DEVICE: CPU (Intel Core i7-14700KF)")
print("SOURCE: data/video/vecteezy_car-and-truck-traffic-on-the-highway-in-europe-poland_7957364.MP4 (4k30fps Video)")
print("SKIP EVERY 30 FRAMES")
print("TOTALLY PROCESSED FRAMES: 10")
print("COCO Dataset, Classes truck, bus, car, train, bicycle")
print("ALL >50%")
# Axis labels and plot title
plt.xlabel('Average Speed (seconds)')
plt.ylabel('Average Precision (%)')
plt.title('Model Performance: Average Inference Speed x Average Accuracy')
plt.legend(loc='upper left', bbox_to_anchor=(1.05, 1), fontsize='small')
plt.grid(True)
plt.savefig("Analysis/Experiment2/i7_14700KF/Experiment2_Diagram.png", bbox_inches='tight')
plt.show()

# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_precisions, avg_precisions, worst_precisions])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Precision (%)", "Average Precision (%)", "Worst Precision (%)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

plt.title("Model Precision Metrics")
plt.savefig("Analysis/Experiment2/i7_14700KF/Experiment2_Precision_Table.png", bbox_inches='tight')
plt.show()

# # Set up the figure and axis for the table
# fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
# ax.axis('tight')
# ax.axis('off')

# # The table data: transpose the array to make each column a different metric
# table_data = np.transpose([models, best_speeds, avg_speeds, worst_speeds])

# # Create the table in the plot
# table = ax.table(cellText=table_data, colLabels=["Model", "Best Speed (s)", "Average Speed (s)", "Worst Speed (s)"],
#                  cellLoc='center', loc='center', colColours=["palegreen"] * 4)
# table.auto_set_font_size(False)
# table.set_fontsize(10)
# table.scale(1.2, 1.2)  # Scale table size

# plt.title("Model Speed Metrics")
# plt.show()

# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_inference_speeds, avg_inference_speeds, worst_inference_speeds])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Inference Speed (s)", "Average Inference Speed (s)", "Worst Inference Speed (s)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

plt.title("Model Inference Speed Metrics")
plt.savefig("Analysis/Experiment2/i7_14700KF/Experiment2_Time_Table.png", bbox_inches='tight')
plt.show()


import cv2
import torch
from torchvision.models.detection import *
from torchvision.io.image import read_image
from torchvision.utils import draw_bounding_boxes
from torchvision.transforms.functional import to_pil_image
import torchvision.transforms as transforms
from PIL import Image
import IPython
from IPython.display import display, clear_output
import time
import traceback
import math
from ultralytics import YOLO

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

transform = transforms.Compose([
    #transforms.Resize((300, 300)),
    transforms.ToTensor(),
])

class DetectionModel():
    def __init__(self, concept, backbone, model, weights, reference, color=None):
        self.concept = concept
        self.backbone = backbone
        self.model = model(weights) if concept=="YOLO" else model(weights=weights.DEFAULT)
        if concept!="YOLO":
            self.model.to(device)
            self.model.eval()
        self.weights = weights if concept=="YOLO" else weights.DEFAULT
        self.reference = reference
        self.color = color
        self.detections = []

    def addDetection(self, detection):
        self.detections.append(detection)

    def getDetections(self):
        return self.detections

    def clearDetections(self):
        self.detections = []

    def detectionsCount(self):
        return len(self.detections)

    def inferencesCount(self):
        length = 0
        addedFrames = []
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    addedFrames.append(self.detections[i].frame)
        return len(addedFrames)

    def getAveragePrecision(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].precision
            avg = sum/len(self.detections)
        return  avg.item() if torch.is_tensor(avg) else avg

    def getMaxPrecision(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].precision:
                max=self.detections[i].precision
        return max.item() if torch.is_tensor(max) else max

    def getMinPrecision(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].precision:
                min=self.detections[i].precision
        return 0.0 if min==9999999.99 else min.item() if torch.is_tensor(min) else min

    def getAverageSpeed(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].speed
            avg = sum/len(self.detections)
        return avg

    def getMaxSpeed(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].speed:
                max=self.detections[i].speed
        return max

    def getMinSpeed(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].speed:
                min=self.detections[i].speed
        return 0.0 if min==9999999.99 else min

    def getAverageInferenceSpeed(self):
        sum=0.0
        avg=0.0
        addedFrames=[]
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    sum+=self.detections[i].speed
                    addedFrames.append(self.detections[i].frame)
            avg = sum/len(addedFrames) if len(addedFrames)>0 else 0.0
        return avg

    def getMaxInferenceSpeed(self):
        max=0.0
        addedFrames=[]
        for i in range(len(self.detections)):
            if max<self.detections[i].speed and self.detections[i].frame not in addedFrames:
                max=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return max

    def getMinInferenceSpeed(self):
        min=9999999.99
        addedFrames=[]
        for i in range(len(self.detections)):
            if min>self.detections[i].speed and self.detections[i].frame not in addedFrames:
                min=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return 0.0 if min==9999999.99 else min

    def filterPrecisionsBelow(self, minPrecision):
        newDetections = []
        for i in range(len(self.detections)):
            if self.detections[i].precision>=minPrecision:
                newDetections.append(self.detections[i])
        self.detections = newDetections

    def printResults(self):
        print("Model:", self.reference,
              "\nBest Precision:", self.getMaxPrecision(),
              "%\nAverage Precision:", self.getAveragePrecision(),
              "%\nWorst Precision:", self.getMinPrecision(),
              "%\nBest Speed:", self.getMinSpeed(),
              "seconds\nAverage Speed:", self.getAverageSpeed(),
              "seconds\nWorst Speed:", self.getMaxSpeed(),
              "seconds\nBest Inference Speed:", self.getMinInferenceSpeed(),
              "seconds\nAverage Inference Speed:", self.getAverageInferenceSpeed(),
              "seconds\nWorst Inference Speed:", self.getMaxInferenceSpeed(),
              "seconds\nTotal Detections:", self.detectionsCount(),
              "\nTotal Inferences:", self.inferencesCount(),
              "\n\n")

class DetectionResult():
    def __init__(self, model, label, precision, speed, device=None, source=None, frame=None, boundingBox=None, details=None):
        self.model = model
        self.label = label
        self.precision = precision.item() if torch.is_tensor(precision) else precision
        self.speed = speed.item() if torch.is_tensor(speed) else speed
        self.device = device
        self.source = source
        self.frame = frame
        self.boundingBox = boundingBox
        self.details = details

target_classes = ["truck", "bus", "car", "train", "bicycle"]


VEHICLE_DETECTION_MODELS = [
    DetectionModel("FCOS", "resnet50_fpn", fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights, "fcos_resnet", (255,0,0)),
    DetectionModel("RetinaNet", "resnet50_fpn", retinanet_resnet50_fpn, RetinaNet_ResNet50_FPN_Weights, "retinanet_resnet", (255,255,0)),
    DetectionModel("RetinaNet", "resnet50_fpn_v2", retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights, "retinanet_resnet_v2", (0,234,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn", fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights, "faster_rcnn_resnet", (170,0,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn_v2", fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights, "faster_rcnn_resnet_v2", (255,127,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_fpn", fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights, "faster_rcnn_mobilenet_v3", (191,255,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_320_fpn", fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights, "faster_rcnn_mobilenet_v3_320", (0,149,255)),
    DetectionModel("MaskRCNN", "resnet50_fpn", maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights, "mask_rcnn_resnet", (255,0,170)),
    DetectionModel("MaskRCNN", "resnet50_fpn_v2", maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights, "mask_rcnn_resnet_v2", (255,212,0)),
    DetectionModel("SSD300", "vgg16", ssd300_vgg16, SSD300_VGG16_Weights, "ssd_vgg16", (106,255,0)),
    DetectionModel("SSDLite320", "mobilenet_v3_large", ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights, "ssd_mobilenet_v3", (0,64,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5nu.pt', "yolov5nu", (237,185,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5su.pt', "yolov5su", (185,215,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5mu.pt', "yolov5mu", (231,233,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5lu.pt', "yolov5lu", (220,185,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5xu.pt', "yolov5xu", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5n6u.pt', "yolov5n6u", (143,35,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5s6u.pt', "yolov5s6u", (35,98,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5m6u.pt', "yolov5m6u", (143,106,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5l6u.pt', "yolov5l6u", (107,35,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5x6u.pt', "yolov5x6u", (79,143,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8n.pt', "yolov8n", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8s.pt', "yolov8s", (115,115,115)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8m.pt', "yolov8m", (204,204,204)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8l.pt', "yolov8l", (255,0,0)), # cycle restarts
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8x.pt', "yolov8x", (255,255,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9c.pt', "yolov9c", (0,234,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9e.pt', "yolov9e", (170,0,255))#,
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10n.pt', "yolov10n", (185,237,224)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10s.pt', "yolov10s", (115,115,115)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10m.pt', "yolov10m", (204,204,204)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10b.pt', "yolov10b", (204,204,204)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10l.pt', "yolov10l", (255,0,0)), # cycle restarts
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10x.pt', "yolov10x", (255,255,0))
]

def process(frame, frameIndex=-1, source=None):
    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        processed_frame = frame.copy()

        if currentModel.concept=="YOLO":
            start_time = time.time()
            results = currentModel.model(processed_frame, agnostic_nms=True, verbose=False, device=device)[0]
            processing_time = time.time() - start_time
            for result in results:
                detection_count = result.boxes.shape[0]
                for j in range(detection_count):
                    cls = int(result.boxes.cls[j].item())
                    name = result.names[cls]
                    confidence = float(result.boxes.conf[j].item())
                    bounding_box = result.boxes.xyxy[j].cpu().numpy()
                    if name in target_classes and confidence>0.5:
                        x1, y1, x2, y2 = [int(x) for x in bounding_box]
                        # Ensure x1 < x2 and y1 < y2
                        x1, x2 = min(x1, x2), max(x1, x2)
                        y1, y2 = min(y1, y2), max(y1, y2)
                        label = '{} ({:.2f}%) ({:.3f}s) {}'.format(name.upper(), confidence*100, processing_time, currentModel.reference)
                        print(label)
                        VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, name, confidence*100, processing_time, device, source, frameIndex, bounding_box, label))
                        cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                        cv2.putText(processed_frame, label, (x1+10, y1+25), 0, 0.8, currentModel.color, 2)
        else:
            # Convert frame to RGB and PIL Image, then apply transformation
            frame_rgb = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            image = transform(pil_image).unsqueeze(0).to(device)

            # Object detection
            start_time = time.time()
            with torch.no_grad():
                predictions = currentModel.model(image)

            # Processing time
            processing_time = time.time() - start_time

            # Visualization
            scores = predictions[0]['scores']
            boxes = predictions[0]['boxes']
            labels = predictions[0]['labels']
            confidence_threshold = 0.5

            for score, box, label in zip(scores, boxes, labels):
                # and score > confidence_threshold
                if currentModel.weights.meta["categories"][label.item()] in target_classes and score > confidence_threshold:
                    x1, y1, x2, y2 = map(int, box)
                    label_text = f'{currentModel.weights.meta["categories"][label.item()].upper()} ({score*100:.2f}%) ({processing_time:.3f}s) {currentModel.reference}'
                    print(label_text)
                    VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, currentModel.weights.meta["categories"][label.item()], float(score.item())*100, processing_time, device, source, frameIndex, box, label_text))
                    cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                    cv2.putText(processed_frame, label_text, (x1, y1+20), 0, 0.8, currentModel.color, 2)

        img = Image.fromarray(processed_frame, 'RGB')
        display(img)
        clear_output(wait=True)
    return frame

def handleFrame(frame, frameIndex=-1, source=None):
    processed_frame = process(frame,frameIndex,source)

def handleRelease():
    cam.release()
    print("Source released.")

    print(device)
    print(source)

    BestPrecision=None
    BestAvgPrecision=None
    WorstPrecision=None
    BestSpeed=None
    BestAvgSpeed=None
    WorstSpeed=None

    Models=[]
    BestPrecisions=[]
    AvgPrecisions=[]
    WorstPrecisions=[]
    BestSpeeds=[]
    AvgSpeeds=[]
    WorstSpeeds=[]
    BestInferenceSpeeds=[]
    AvgInferenceSpeeds=[]
    WorstInferenceSpeeds=[]
    TotalDetectionsArr=[]
    TotalInferencesArr=[]

    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        # currentModel.printResults()

        Models.append(currentModel.reference)
        BestPrecisions.append(currentModel.getMaxPrecision())
        AvgPrecisions.append(currentModel.getAveragePrecision())
        WorstPrecisions.append(currentModel.getMinPrecision())
        BestSpeeds.append(currentModel.getMinSpeed())
        AvgSpeeds.append(currentModel.getAverageSpeed())
        WorstSpeeds.append(currentModel.getMaxSpeed())
        BestInferenceSpeeds.append(currentModel.getMinInferenceSpeed())
        AvgInferenceSpeeds.append(currentModel.getAverageInferenceSpeed())
        WorstInferenceSpeeds.append(currentModel.getMaxInferenceSpeed())
        TotalDetectionsArr.append(currentModel.detectionsCount())
        TotalInferencesArr.append(currentModel.inferencesCount())

        if BestPrecision is None or BestPrecision.getMaxPrecision()<=currentModel.getMaxPrecision():
            BestPrecision = currentModel
        if BestAvgPrecision is None or BestAvgPrecision.getAveragePrecision()<=currentModel.getAveragePrecision():
            BestAvgPrecision = currentModel
        if WorstPrecision is None or WorstPrecision.getMinPrecision()>=currentModel.getMinPrecision():
            WorstPrecision = currentModel
        if BestSpeed is None or BestSpeed.getMinSpeed()>=currentModel.getMinSpeed():
            BestSpeed = currentModel
        if BestAvgSpeed is None or BestAvgSpeed.getAverageSpeed()>=currentModel.getAverageSpeed():
            BestAvgSpeed = currentModel
        if WorstSpeed is None or WorstSpeed.getMaxSpeed()<=currentModel.getMaxSpeed():
            WorstSpeed = currentModel

    if BestPrecision is not None:
        print("Best Precision:")
        BestPrecision.printResults()
    if BestAvgPrecision is not None:
        print("Best Average Precision:")
        BestAvgPrecision.printResults()
    if WorstPrecision is not None:
        print("Worst Precision:")
        WorstPrecision.printResults()
    if BestSpeed is not None:
        print("Best Speed:")
        BestSpeed.printResults()
    if BestAvgSpeed is not None:
        print("Best Average Speed:")
        BestAvgSpeed.printResults()
    if WorstSpeed is not None:
        print("Worst Speed:")
        WorstSpeed.printResults()

    print("models=",Models)
    print("best_precisions=",BestPrecisions)
    print("avg_precisions=",AvgPrecisions)
    print("worst_precisions=",WorstPrecisions)
    print("best_speeds=",BestSpeeds)
    print("avg_speeds=",AvgSpeeds)
    print("worst_speeds=",WorstSpeeds)
    print("best_inference_speeds=",BestInferenceSpeeds)
    print("avg_inference_speeds=",AvgInferenceSpeeds)
    print("worst_inference_speeds=",WorstInferenceSpeeds)
    print("total_detections=",TotalDetectionsArr)
    print("total_inferences=",TotalInferencesArr)

inputType="video"
source = "data/video/2034115-hd_1920_1080_30fps.MP4" if inputType=="video" else 0
cam = cv2.VideoCapture(source)
display_handle=display(None, display_id=True)
every = 30

start = 0 if source!=0 else -1
end = int(cam.get(cv2.CAP_PROP_FRAME_COUNT)) if source!=0 else -1
if source!=0: cam.set(1, start)

frameCount=start
while_safety=0
saved_count=0

if not cam.isOpened(): print("Error: Could not open source.")
else:
    try:
        while True if end<0 else frameCount<end:
            _, frame = cam.read()
            if frame is None:
                if source!=0:
                    if while_safety > 2000: break
                    while_safety += 1
                    continue
                else:
                    print("Error: Could not capture frame.")
                    cam.release()
                    break

            if every>0:
                if (frameCount+1)%math.floor(every) == 0:
                    while_safety = 0
                    handleFrame(frame, frameCount, source)
            else:
                while_safety=0
                handleFrame(frame)
            frameCount += 1
        handleRelease()
    except KeyboardInterrupt:
        handleRelease()
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Source could not be released.")
        except:
            pass
        traceback.print_exc()


models= ['fcos_resnet', 'retinanet_resnet', 'retinanet_resnet_v2', 'faster_rcnn_resnet', 'faster_rcnn_resnet_v2', 'faster_rcnn_mobilenet_v3', 'faster_rcnn_mobilenet_v3_320', 'mask_rcnn_resnet', 'mask_rcnn_resnet_v2', 'ssd_vgg16', 'ssd_mobilenet_v3', 'yolov5nu', 'yolov5su', 'yolov5mu', 'yolov5lu', 'yolov5xu', 'yolov5n6u', 'yolov5s6u', 'yolov5m6u', 'yolov5l6u', 'yolov5x6u', 'yolov8n', 'yolov8s', 'yolov8m', 'yolov8l', 'yolov8x', 'yolov9c', 'yolov9e']
best_precisions= [84.83301997184753, 91.69307351112366, 95.4441785812378, 99.9228835105896, 99.9826729297638, 99.82061386108398, 99.8578667640686, 99.87086057662964, 99.93312358856201, 99.47913885116577, 99.56985712051392, 93.70522499084473, 94.43736672401428, 95.35672068595886, 95.55718898773193, 95.68715691566467, 96.31597399711609, 94.2226231098175, 95.38546800613403, 96.37797474861145, 96.30561470985413, 94.39494609832764, 95.28464078903198, 95.23571133613586, 95.44462561607361, 96.01415395736694, 96.25153541564941, 96.6651976108551]
avg_precisions= [65.8841269995485, 71.22555674401475, 73.83507871514813, 86.84455658679822, 88.96739099840386, 87.90522995424885, 85.2060926383769, 87.1236540860944, 89.62056282966856, 84.4696799311975, 84.35685192385027, 76.05521628554438, 77.53465430190165, 78.89073387700684, 81.06716857400052, 82.04506759622456, 79.49444311005729, 82.7447441394019, 83.24607770680349, 83.53853318534914, 84.0977500422729, 76.1033314355412, 81.14591127634048, 80.57522682865638, 81.92216429087493, 83.3882709881207, 83.31239580558741, 83.2613467378954]
worst_precisions= [50.04851818084717, 50.08883476257324, 50.05960464477539, 50.46467185020447, 50.51953196525574, 50.00492334365845, 50.13507008552551, 50.2067506313324, 51.425570249557495, 51.760393381118774, 50.511109828948975, 50.1340389251709, 50.268709659576416, 50.881582498550415, 50.4483699798584, 50.82172751426697, 50.26649832725525, 50.05990266799927, 52.43103504180908, 51.135653257369995, 50.0474750995636, 51.420825719833374, 51.97308659553528, 50.375568866729736, 50.06181001663208, 50.03392696380615, 52.08258628845215, 50.00579357147217]
best_speeds= [1.2939982414245605, 1.4590015411376953, 1.4879951477050781, 1.4339981079101562, 2.1850008964538574, 0.4590003490447998, 0.15099620819091797, 2.0179989337921143, 2.623997211456299, 0.24299955368041992, 0.09999275207519531, 0.08100271224975586, 0.10800290107727051, 0.2200016975402832, 0.3410007953643799, 0.5160000324249268, 0.16899943351745605, 0.2919614315032959, 0.5589990615844727, 1.080000877380371, 1.7790000438690186, 0.07499933242797852, 0.11400008201599121, 0.21600079536437988, 0.367999792098999, 0.48400115966796875, 0.3370022773742676, 0.628000020980835]
avg_speeds= [1.4563422339303154, 1.6032773179557702, 1.6313001691447615, 1.5841807976215578, 2.2665348740580686, 0.5296143482683042, 0.19784420481800327, 2.1258253710610524, 2.826122958319528, 0.3438773107047033, 0.1136811420481692, 0.10987122461829386, 0.15782002235452333, 0.2789648759121798, 0.475917911529541, 0.6995562462680107, 0.2270243309793018, 0.3782830035205379, 0.7312872874272334, 1.2445544326891664, 2.027648583615674, 0.09538565363202776, 0.15655957221984862, 0.2932759357674774, 0.4915978629309852, 0.6589661471478574, 0.4048375501061365, 0.7906569073685502]
worst_speeds= [1.8428466320037842, 1.752997636795044, 1.8386259078979492, 1.731032133102417, 2.3620004653930664, 0.6270010471343994, 0.27900171279907227, 2.285999059677124, 2.9818227291107178, 0.45195484161376953, 0.17599940299987793, 0.20799827575683594, 0.27300119400024414, 0.6269998550415039, 0.9719977378845215, 1.694998025894165, 0.3450000286102295, 0.5580008029937744, 1.2999992370605469, 2.248999834060669, 3.8299999237060547, 0.16693544387817383, 0.286998987197876, 0.5940005779266357, 0.8849990367889404, 1.257000207901001, 0.6979990005493164, 1.5379984378814697]
best_inference_speeds= [1.2939982414245605, 1.4590015411376953, 1.4879951477050781, 1.4339981079101562, 2.1850008964538574, 0.4590003490447998, 0.15099620819091797, 2.0179989337921143, 2.623997211456299, 0.24299955368041992, 0.09999275207519531, 0.08100271224975586, 0.10800290107727051, 0.2200016975402832, 0.3410007953643799, 0.5160000324249268, 0.16899943351745605, 0.2919614315032959, 0.5589990615844727, 1.080000877380371, 1.7790000438690186, 0.07499933242797852, 0.11400008201599121, 0.21600079536437988, 0.367999792098999, 0.48400115966796875, 0.3370022773742676, 0.628000020980835]
avg_inference_speeds= [1.4565706423350744, 1.6020343473979406, 1.6291637846401759, 1.5848999108586992, 2.2667130742754256, 0.5294294868196759, 0.19733131783349173, 2.1245794551713124, 2.826475126402719, 0.3423555663653782, 0.11384967395237514, 0.11024813141141619, 0.15853804349899292, 0.2774289846420288, 0.4759256754602705, 0.6991418514932904, 0.22667841400418962, 0.38053063835416523, 0.7339971576418195, 1.2496399538857597, 2.036110622542245, 0.0946019036429269, 0.1554992369243077, 0.2930383937699454, 0.4924230234963553, 0.660430771963937, 0.40581999506269184, 0.7940691879817418]
worst_inference_speeds= [1.8428466320037842, 1.752997636795044, 1.8386259078979492, 1.731032133102417, 2.3620004653930664, 0.6270010471343994, 0.27900171279907227, 2.285999059677124, 2.9818227291107178, 0.45195484161376953, 0.17599940299987793, 0.20799827575683594, 0.27300119400024414, 0.6269998550415039, 0.9719977378845215, 1.694998025894165, 0.3450000286102295, 0.5580008029937744, 1.2999992370605469, 2.248999834060669, 3.8299999237060547, 0.16693544387817383, 0.286998987197876, 0.5940005779266357, 0.8849990367889404, 1.257000207901001, 0.6979990005493164, 1.5379984378814697]
total_detections= [280, 233, 211, 299, 319, 233, 169, 308, 315, 99, 93, 142, 192, 196, 215, 226, 168, 223, 231, 244, 239, 161, 200, 223, 222, 222, 217, 226]
total_inferences= [28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28]


import numpy as np
import matplotlib.pyplot as plt
import matplotlib

# Generate a color map
num_models = len(models)
cmap = matplotlib.colormaps.get_cmap('nipy_spectral')
colors = [cmap(i / num_models) for i in range(num_models)]

# Create a figure and plot with unique colors
plt.figure(figsize=(14, 10))
for i, model in enumerate(models):
    plt.scatter(avg_inference_speeds[i], avg_precisions[i], color=colors[i], label=model, edgecolor='black')

print("DEVICE: CPU (Intel Core i7-14700KF)")
print("SOURCE: data/video/2034115-hd_1920_1080_30fps.MP4 (1080p30fps Video)")
print("SKIP EVERY 30 FRAMES")
print("TOTALLY PROCESSED FRAMES: 28")
print("COCO Dataset, Classes truck, bus, car, train, bicycle")
print("ALL >50%")
# Axis labels and plot title
plt.xlabel('Average Speed (seconds)')
plt.ylabel('Average Precision (%)')
plt.title('Model Performance: Average Inference Speed x Average Accuracy')
plt.legend(loc='upper left', bbox_to_anchor=(1.05, 1), fontsize='small')
plt.grid(True)
plt.savefig("Analysis/Experiment3/i7_14700KF/Experiment3_Diagram.png", bbox_inches='tight')
plt.show()

# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_precisions, avg_precisions, worst_precisions])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Precision (%)", "Average Precision (%)", "Worst Precision (%)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

plt.title("Model Precision Metrics")
plt.savefig("Analysis/Experiment3/i7_14700KF/Experiment3_Precision_Table.png", bbox_inches='tight')
plt.show()


# # Set up the figure and axis for the table
# fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
# ax.axis('tight')
# ax.axis('off')

# # The table data: transpose the array to make each column a different metric
# table_data = np.transpose([models, best_speeds, avg_speeds, worst_speeds])

# # Create the table in the plot
# table = ax.table(cellText=table_data, colLabels=["Model", "Best Speed (s)", "Average Speed (s)", "Worst Speed (s)"],
#                  cellLoc='center', loc='center', colColours=["palegreen"] * 4)
# table.auto_set_font_size(False)
# table.set_fontsize(10)
# table.scale(1.2, 1.2)  # Scale table size

# plt.title("Model Speed Metrics")
# plt.show()

# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_inference_speeds, avg_inference_speeds, worst_inference_speeds])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Inference Speed (s)", "Average Inference Speed (s)", "Worst Inference Speed (s)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

plt.title("Model Inference Speed Metrics")
plt.savefig("Analysis/Experiment3/i7_14700KF/Experiment3_Time_Table.png", bbox_inches='tight')
plt.show()





# Neuronal Network Layered Stream with the better fps
import cv2
import IPython
import numpy as np
from typing import Tuple, Union
import math
from PIL import Image
# import pytesseract
import traceback

import torch
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)

from ultralytics import YOLO
model = YOLO('yolov8x.pt')
plate_model = YOLO('vehicle_license_best.pt')
characters_model = YOLO('plate_char/yolov8x/weights/best.pt')

detected_plates = []

# pytesseract.pytesseract.tesseract_cmd = 'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'

target_classes = ["truck", "bus", "car"]

# NN Layer License Plate Recognition "OCR"
def process_license(processed_frame, bounding_box, name, confidence, parentName, parentConfidence):
    x1, y1, x2, y2 = [int(x) for x in bounding_box]
    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(255,0,255),2)
    cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1-15),0,0.7,(255,0,255))

    license_frame = processed_frame[y1:y2,x1:x2]

    plate_parts=[]
    plate_text=""

    results = characters_model(license_frame, agnostic_nms=True, verbose=False, device=device)[0]
    for result in results:
        detection_count = result.boxes.shape[0]
        for i in range(detection_count):
            cls = int(result.boxes.cls[i].item())
            name2 = result.names[cls]
            confidence2 = float(result.boxes.conf[i].item())
            bounding_box2 = result.boxes.xyxy[i].cpu().numpy()
            a1, b1, a2, b2 = [int(x) for x in bounding_box2]
            # Ensure a1 < a2 and b1 < b2
            a1, a2 = min(a1, a2), max(a1, a2)
            b1, b2 = min(b1, b2), max(b1, b2)
            if confidence2>0.3:
                cv2.rectangle(processed_frame,(x1+a1,y1+b1),(x1+a2,y1+b2),(0,0,255),1)
                cv2.putText(processed_frame, '{}'.format(name2),(x1+a1,y1+b1+75),0,0.9,(0,0,255),2)
                cv2.putText(processed_frame, '{:.2f}%'.format(confidence2*100),(x1+a1,y1+b1+100+25*i),0,0.7,(0,0,255),2)
                plate_parts.append([a1,b1,name2])

    sorted_list = sorted(plate_parts,key=lambda l:l[0])
    for part in range(len(sorted_list)):
        if sorted_list[part][2]!="undefined":
            plate_text+=sorted_list[part][2]
    detected_plates.append(plate_text)

# NN Layer Vehicle Found -> License Detection
def process_vehicle(processed_frame, bounding_box, name, confidence):
    x1, y1, x2, y2 = [int(x) for x in bounding_box]
    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(0,255,255),2)
    cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1-15),0,0.7,(0,255,255))
    vehicle_frame = processed_frame[y1:y2,x1:x2]
    results = plate_model(vehicle_frame, agnostic_nms=True, verbose=False, device=device)[0]
    for result in results:
        detection_count = result.boxes.shape[0]
        for i in range(detection_count):
            cls = int(result.boxes.cls[i].item())
            name2 = result.names[cls]
            confidence2 = float(result.boxes.conf[i].item())

            if confidence2 > 0.5:
                bounding_box2 = result.boxes.xyxy[i].cpu().numpy()
                a1, b1, a2, b2 = [int(x) for x in bounding_box2]
                # Ensure a1 < a2 and b1 < b2
                a1, a2 = min(a1, a2), max(a1, a2)
                b1, b2 = min(b1, b2), max(b1, b2)
                # if name2=="License_Plate":
                process_license(processed_frame, [x1+a1, y1+b1, x1+a2, y1+b2], name2, confidence2, name, confidence)

# NN Layer Other Objects Detected
def process_detection(processed_frame, bounding_box, name, confidence):
    x1, y1, x2, y2 = [int(x) for x in bounding_box]
    cv2.rectangle(processed_frame,(x1,y1),(x2,y2),(255,255,0),2)
    cv2.putText(processed_frame, '{} {:.2f}%'.format(name.upper(), confidence*100),(x1+10,y1-15),0,0.7,(255,255,0))

# NN Layer Yolov8 Detection
def process_frame(frame):
    processed_frame = frame
    # processed_frame = cv2.flip(processed_frame, 1)
    results = model(processed_frame, agnostic_nms=True, verbose=False, device=device)[0]
    for result in results:
        detection_count = result.boxes.shape[0]
        for i in range(detection_count):
            cls = int(result.boxes.cls[i].item())
            name = result.names[cls]
            confidence = float(result.boxes.conf[i].item())
            bounding_box = result.boxes.xyxy[i].cpu().numpy()
            x1, y1, x2, y2 = [int(x) for x in bounding_box]
            # Ensure x1 < x2 and y1 < y2
            x1, x2 = min(x1, x2), max(x1, x2)
            y1, y2 = min(y1, y2), max(y1, y2)
            if name in target_classes: process_vehicle(processed_frame, bounding_box, name, confidence)
            else: process_detection(processed_frame, bounding_box, name, confidence)
    return processed_frame

def handleFrame(frame):
    processed_frame = process_frame(frame)
    _, processed_frame = cv2.imencode('.jpeg', processed_frame)
    display_handle.update(IPython.display.Image(data=processed_frame.tobytes()))
    IPython.display.clear_output(wait=True)

def handleRelease():
    cam.release()
    print("Source released.")
    print()
    print(detected_plates)
    # correct_plates = []
    # better_plates = []
    # best_plate = []
    # most_repeated_plates = []
    # best_repeated_plates = []
    # highest_score = 0.0
    # if len(detected_plates)>0:
    #     repetitions = {}
    #     count, item = 0, ''
    #     for plate in detected_plates:
    #         if plate[0]>50.0 and plate[2]>50.0 and plate[4]>50.0: correct_plates.append(plate)
    #         if plate[0]>70.0 and plate[2]>70.0 and plate[4]>70.0: better_plates.append(plate)
    #         if plate[0] + plate[2] + plate[4] > highest_score:
    #             highest_score = plate[0] + plate[2] + plate[4]
    #             best_plate = plate
    #         repetitions[plate[5]] = repetitions.get(plate[5], 0) + 1
    #         if repetitions[plate[5]]>count: count, item = repetitions[plate[5]], plate[5]
    #     if len(repetitions.keys())>0:
    #         for itm in repetitions.keys():
    #             if repetitions[itm]==count:
    #                 repeated_detected = [plate for plate in detected_plates if plate[5]==itm]
    #                 best_percentages = []
    #                 for plate in repeated_detected:
    #                     if len(best_percentages)==0: best_percentages=plate
    #                     elif (plate[0] + plate[2] + plate[4])/3.0>(best_percentages[0] + best_percentages[2] + best_percentages[4])/3.0: best_percentages=plate
    #                 most_repeated_plates.append(best_percentages)
    #     if len(most_repeated_plates)>0:
    #         for plate in most_repeated_plates:
    #             best_repeated = []
    #             if len(best_repeated)==0: best_repeated=plate
    #             elif (plate[0] + plate[2] + plate[4])/3.0>(best_repeated[0] + best_repeated[2] + best_repeated[4])/3.0: best_repeated=plate
    #         best_repeated_plates.append(best_repeated)
    # print("\nDetected Plates: \n", np.matrix(detected_plates))
    # print("\nCorrectly Detected Plates: \n", np.matrix(correct_plates))
    # print("\nBetter Detected Plates: \n", np.matrix(better_plates))
    # print("\nBest Detected Plate: \n", np.matrix(best_plate))
    # print("\nMost Repeated Plates: \n", np.matrix(most_repeated_plates))
    # print("\nBest Repeated Plate: \n", np.matrix(best_repeated_plates))

inputType="video"
# cam = cv2.VideoCapture(0)
source = "data/video/sample.MP4" if inputType=="video" else 0
cam = cv2.VideoCapture(source)
display_handle=display(None, display_id=True)
every = 60

start = 0 if source!=0 else -1
end = int(cam.get(cv2.CAP_PROP_FRAME_COUNT)) if source!=0 else -1
if source!=0: cam.set(1, start)

frameCount=start
while_safety=0
saved_count=0

if not cam.isOpened(): print("Error: Could not open source.")
else:
    try:
        while True if end<0 else frameCount<end:
            _, frame = cam.read()
            if frame is None:
                if source!=0:
                    if while_safety > 2000: break
                    while_safety += 1
                    continue
                else:
                    print("Error: Could not capture frame.")
                    cam.release()
                    break

            if every>0:
                if (frameCount+1)%math.floor(every) == 0:
                    while_safety = 0
                    handleFrame(frame)
            else:
                while_safety=0
                handleFrame(frame)
            frameCount += 1
        handleRelease()
    except KeyboardInterrupt:
        handleRelease()
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Source could not be released.")
        except:
            pass
        traceback.print_exc()


output = ['WAFHW379', 'HAFHW379', 'HAFHW379', 'WAFHW379', 'HAFHW379', 'WAFHW379', 'WAFHW379', 'WAFHW379', 'HAFHW379', 'HAFHW379', 'HAFHW379', 'WAFHW379', 'WAFHW379', 'WAFHW379', 'WAFHW379', 'WAFHW379', 'WAFHW379', 'WAFHW379', 'AFHW379', 'WAFHW379', 'HAFHW379', 'WAFHW379', 'WAFHW379', 'WAFHW379', 'WAFHW379', 'HAFHW379', 'WAFHW379', 'WAFHW379', 'HAFHW379', '', '', '', '', '', '', '', '5', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'K2', '3NC35EA', 'NG43353A', 'WGM3353A', 'WGM3353A', 'WGM3353A', '3353A', '', '', '505', 'S0CT114E', 'SOCT114', 'SOCT114', 'SOCT114', 'FSOCT114', 'SOCT114', 'SOCT114', 'SOCT114', 'SOCT114', 'SOCT114', 'SOCT114', '', 'SOCT114', '', 'SOCT114', 'SOCT114']


import cv2
import torch
from torchvision.models.detection import *
from torchvision.io.image import read_image
from torchvision.utils import draw_bounding_boxes
from torchvision.transforms.functional import to_pil_image
import torchvision.transforms as transforms
from PIL import Image
import IPython
from IPython.display import display, clear_output
import time
import traceback
import math
from ultralytics import YOLO

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

transform = transforms.Compose([
    #transforms.Resize((300, 300)),
    transforms.ToTensor(),
])

class DetectionModel():
    def __init__(self, concept, backbone, model, weights, reference, color=None):
        self.concept = concept
        self.backbone = backbone
        self.model = model(weights) if concept=="YOLO" else model(weights=weights.DEFAULT)
        if concept!="YOLO":
            self.model.to(device)
            self.model.eval()
        self.weights = weights if concept=="YOLO" else weights.DEFAULT
        self.reference = reference
        self.color = color
        self.detections = []

    def addDetection(self, detection):
        self.detections.append(detection)

    def getDetections(self):
        return self.detections

    def clearDetections(self):
        self.detections = []

    def detectionsCount(self):
        return len(self.detections)

    def inferencesCount(self):
        length = 0
        addedFrames = []
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    addedFrames.append(self.detections[i].frame)
        return len(addedFrames)

    def getAveragePrecision(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].precision
            avg = sum/len(self.detections)
        return  avg.item() if torch.is_tensor(avg) else avg

    def getMaxPrecision(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].precision:
                max=self.detections[i].precision
        return max.item() if torch.is_tensor(max) else max

    def getMinPrecision(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].precision:
                min=self.detections[i].precision
        return 0.0 if min==9999999.99 else min.item() if torch.is_tensor(min) else min

    def getAverageSpeed(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].speed
            avg = sum/len(self.detections)
        return avg

    def getMaxSpeed(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].speed:
                max=self.detections[i].speed
        return max

    def getMinSpeed(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].speed:
                min=self.detections[i].speed
        return 0.0 if min==9999999.99 else min

    def getAverageInferenceSpeed(self):
        sum=0.0
        avg=0.0
        addedFrames=[]
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    sum+=self.detections[i].speed
                    addedFrames.append(self.detections[i].frame)
            avg = sum/len(addedFrames) if len(addedFrames)>0 else 0.0
        return avg

    def getMaxInferenceSpeed(self):
        max=0.0
        addedFrames=[]
        for i in range(len(self.detections)):
            if max<self.detections[i].speed and self.detections[i].frame not in addedFrames:
                max=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return max

    def getMinInferenceSpeed(self):
        min=9999999.99
        addedFrames=[]
        for i in range(len(self.detections)):
            if min>self.detections[i].speed and self.detections[i].frame not in addedFrames:
                min=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return 0.0 if min==9999999.99 else min

    def filterPrecisionsBelow(self, minPrecision):
        newDetections = []
        for i in range(len(self.detections)):
            if self.detections[i].precision>=minPrecision:
                newDetections.append(self.detections[i])
        self.detections = newDetections

    def printResults(self):
        print("Model:", self.reference,
              "\nBest Precision:", self.getMaxPrecision(),
              "%\nAverage Precision:", self.getAveragePrecision(),
              "%\nWorst Precision:", self.getMinPrecision(),
              "%\nBest Speed:", self.getMinSpeed(),
              "seconds\nAverage Speed:", self.getAverageSpeed(),
              "seconds\nWorst Speed:", self.getMaxSpeed(),
              "seconds\nBest Inference Speed:", self.getMinInferenceSpeed(),
              "seconds\nAverage Inference Speed:", self.getAverageInferenceSpeed(),
              "seconds\nWorst Inference Speed:", self.getMaxInferenceSpeed(),
              "seconds\nTotal Detections:", self.detectionsCount(),
              "\nTotal Inferences:", self.inferencesCount(),
              "\n\n")

class DetectionResult():
    def __init__(self, model, label, precision, speed, device=None, source=None, frame=None, boundingBox=None, details=None):
        self.model = model
        self.label = label
        self.precision = precision.item() if torch.is_tensor(precision) else precision
        self.speed = speed.item() if torch.is_tensor(speed) else speed
        self.device = device
        self.source = source
        self.frame = frame
        self.boundingBox = boundingBox
        self.details = details

target_classes = ["truck", "bus", "car", "train", "bicycle"]


VEHICLE_DETECTION_MODELS = [
    DetectionModel("FCOS", "resnet50_fpn", fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights, "fcos_resnet", (255,0,0)),
    DetectionModel("RetinaNet", "resnet50_fpn", retinanet_resnet50_fpn, RetinaNet_ResNet50_FPN_Weights, "retinanet_resnet", (255,255,0)),
    DetectionModel("RetinaNet", "resnet50_fpn_v2", retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights, "retinanet_resnet_v2", (0,234,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn", fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights, "faster_rcnn_resnet", (170,0,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn_v2", fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights, "faster_rcnn_resnet_v2", (255,127,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_fpn", fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights, "faster_rcnn_mobilenet_v3", (191,255,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_320_fpn", fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights, "faster_rcnn_mobilenet_v3_320", (0,149,255)),
    DetectionModel("MaskRCNN", "resnet50_fpn", maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights, "mask_rcnn_resnet", (255,0,170)),
    DetectionModel("MaskRCNN", "resnet50_fpn_v2", maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights, "mask_rcnn_resnet_v2", (255,212,0)),
    DetectionModel("SSD300", "vgg16", ssd300_vgg16, SSD300_VGG16_Weights, "ssd_vgg16", (106,255,0)),
    DetectionModel("SSDLite320", "mobilenet_v3_large", ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights, "ssd_mobilenet_v3", (0,64,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5nu.pt', "yolov5nu", (237,185,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5su.pt', "yolov5su", (185,215,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5mu.pt', "yolov5mu", (231,233,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5lu.pt', "yolov5lu", (220,185,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5xu.pt', "yolov5xu", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5n6u.pt', "yolov5n6u", (143,35,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5s6u.pt', "yolov5s6u", (35,98,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5m6u.pt', "yolov5m6u", (143,106,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5l6u.pt', "yolov5l6u", (107,35,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5x6u.pt', "yolov5x6u", (79,143,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8n.pt', "yolov8n", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8s.pt', "yolov8s", (115,115,115)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8m.pt', "yolov8m", (204,204,204)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8l.pt', "yolov8l", (255,0,0)), # cycle restarts
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8x.pt', "yolov8x", (255,255,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9c.pt', "yolov9c", (0,234,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9e.pt', "yolov9e", (170,0,255))#,
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10n.pt', "yolov10n", (185,237,224)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10s.pt', "yolov10s", (115,115,115)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10m.pt', "yolov10m", (204,204,204)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10b.pt', "yolov10b", (204,204,204)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10l.pt', "yolov10l", (255,0,0)), # cycle restarts
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10x.pt', "yolov10x", (255,255,0))
]

def process(frame, frameIndex=-1, source=None):
    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        processed_frame = frame.copy()

        if currentModel.concept=="YOLO":
            start_time = time.time()
            results = currentModel.model(processed_frame, agnostic_nms=True, verbose=False, device=device)[0]
            processing_time = time.time() - start_time
            for result in results:
                detection_count = result.boxes.shape[0]
                for j in range(detection_count):
                    cls = int(result.boxes.cls[j].item())
                    name = result.names[cls]
                    confidence = float(result.boxes.conf[j].item())
                    bounding_box = result.boxes.xyxy[j].cpu().numpy()
                    if name in target_classes and confidence>0.5:
                        x1, y1, x2, y2 = [int(x) for x in bounding_box]
                        # Ensure x1 < x2 and y1 < y2
                        x1, x2 = min(x1, x2), max(x1, x2)
                        y1, y2 = min(y1, y2), max(y1, y2)
                        label = '{} ({:.2f}%) ({:.3f}s) {}'.format(name.upper(), confidence*100, processing_time, currentModel.reference)
                        print(label)
                        VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, name, confidence*100, processing_time, device, source, frameIndex, bounding_box, label))
                        cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                        cv2.putText(processed_frame, label, (x1+10, y1+25), 0, 0.8, currentModel.color, 2)
        else:
            # Convert frame to RGB and PIL Image, then apply transformation
            frame_rgb = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            image = transform(pil_image).unsqueeze(0).to(device)

            # Object detection
            start_time = time.time()
            with torch.no_grad():
                predictions = currentModel.model(image)

            # Processing time
            processing_time = time.time() - start_time

            # Visualization
            scores = predictions[0]['scores']
            boxes = predictions[0]['boxes']
            labels = predictions[0]['labels']
            confidence_threshold = 0.5

            for score, box, label in zip(scores, boxes, labels):
                # and score > confidence_threshold
                if currentModel.weights.meta["categories"][label.item()] in target_classes and score > confidence_threshold:
                    x1, y1, x2, y2 = map(int, box)
                    label_text = f'{currentModel.weights.meta["categories"][label.item()].upper()} ({score*100:.2f}%) ({processing_time:.3f}s) {currentModel.reference}'
                    print(label_text)
                    VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, currentModel.weights.meta["categories"][label.item()], float(score.item())*100, processing_time, device, source, frameIndex, box, label_text))
                    cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                    cv2.putText(processed_frame, label_text, (x1, y1+20), 0, 0.8, currentModel.color, 2)

        img = Image.fromarray(processed_frame, 'RGB')
        display(img)
        clear_output(wait=True)
    return frame

def handleFrame(frame, frameIndex=-1, source=None):
    processed_frame = process(frame,frameIndex,source)

def handleRelease():
    cam.release()
    print("Source released.")

    print(device)
    print(source)

    BestPrecision=None
    BestAvgPrecision=None
    WorstPrecision=None
    BestSpeed=None
    BestAvgSpeed=None
    WorstSpeed=None

    Models=[]
    BestPrecisions=[]
    AvgPrecisions=[]
    WorstPrecisions=[]
    BestSpeeds=[]
    AvgSpeeds=[]
    WorstSpeeds=[]
    BestInferenceSpeeds=[]
    AvgInferenceSpeeds=[]
    WorstInferenceSpeeds=[]
    TotalDetectionsArr=[]
    TotalInferencesArr=[]

    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        # currentModel.printResults()

        Models.append(currentModel.reference)
        BestPrecisions.append(currentModel.getMaxPrecision())
        AvgPrecisions.append(currentModel.getAveragePrecision())
        WorstPrecisions.append(currentModel.getMinPrecision())
        BestSpeeds.append(currentModel.getMinSpeed())
        AvgSpeeds.append(currentModel.getAverageSpeed())
        WorstSpeeds.append(currentModel.getMaxSpeed())
        BestInferenceSpeeds.append(currentModel.getMinInferenceSpeed())
        AvgInferenceSpeeds.append(currentModel.getAverageInferenceSpeed())
        WorstInferenceSpeeds.append(currentModel.getMaxInferenceSpeed())
        TotalDetectionsArr.append(currentModel.detectionsCount())
        TotalInferencesArr.append(currentModel.inferencesCount())

        if BestPrecision is None or BestPrecision.getMaxPrecision()<=currentModel.getMaxPrecision():
            BestPrecision = currentModel
        if BestAvgPrecision is None or BestAvgPrecision.getAveragePrecision()<=currentModel.getAveragePrecision():
            BestAvgPrecision = currentModel
        if WorstPrecision is None or WorstPrecision.getMinPrecision()>=currentModel.getMinPrecision():
            WorstPrecision = currentModel
        if BestSpeed is None or BestSpeed.getMinSpeed()>=currentModel.getMinSpeed():
            BestSpeed = currentModel
        if BestAvgSpeed is None or BestAvgSpeed.getAverageSpeed()>=currentModel.getAverageSpeed():
            BestAvgSpeed = currentModel
        if WorstSpeed is None or WorstSpeed.getMaxSpeed()<=currentModel.getMaxSpeed():
            WorstSpeed = currentModel

    if BestPrecision is not None:
        print("Best Precision:")
        BestPrecision.printResults()
    if BestAvgPrecision is not None:
        print("Best Average Precision:")
        BestAvgPrecision.printResults()
    if WorstPrecision is not None:
        print("Worst Precision:")
        WorstPrecision.printResults()
    if BestSpeed is not None:
        print("Best Speed:")
        BestSpeed.printResults()
    if BestAvgSpeed is not None:
        print("Best Average Speed:")
        BestAvgSpeed.printResults()
    if WorstSpeed is not None:
        print("Worst Speed:")
        WorstSpeed.printResults()

    print("models=",Models)
    print("best_precisions=",BestPrecisions)
    print("avg_precisions=",AvgPrecisions)
    print("worst_precisions=",WorstPrecisions)
    print("best_speeds=",BestSpeeds)
    print("avg_speeds=",AvgSpeeds)
    print("worst_speeds=",WorstSpeeds)
    print("best_inference_speeds=",BestInferenceSpeeds)
    print("avg_inference_speeds=",AvgInferenceSpeeds)
    print("worst_inference_speeds=",WorstInferenceSpeeds)
    print("total_detections=",TotalDetectionsArr)
    print("total_inferences=",TotalInferencesArr)

inputType="video"
source = "data/video/sample.MP4" if inputType=="video" else 0
cam = cv2.VideoCapture(source)
display_handle=display(None, display_id=True)
every = 30

start = 0 if source!=0 else -1
end = int(cam.get(cv2.CAP_PROP_FRAME_COUNT)) if source!=0 else -1
if source!=0: cam.set(1, start)

frameCount=start
while_safety=0
saved_count=0

if not cam.isOpened(): print("Error: Could not open source.")
else:
    try:
        while True if end<0 else frameCount<end:
            _, frame = cam.read()
            if frame is None:
                if source!=0:
                    if while_safety > 2000: break
                    while_safety += 1
                    continue
                else:
                    print("Error: Could not capture frame.")
                    cam.release()
                    break

            if every>0:
                if (frameCount+1)%math.floor(every) == 0:
                    while_safety = 0
                    handleFrame(frame, frameCount, source)
            else:
                while_safety=0
                handleFrame(frame)
            frameCount += 1
        handleRelease()
    except KeyboardInterrupt:
        handleRelease()
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Source could not be released.")
        except:
            pass
        traceback.print_exc()


models= ['fcos_resnet', 'retinanet_resnet', 'retinanet_resnet_v2', 'faster_rcnn_resnet', 'faster_rcnn_resnet_v2', 'faster_rcnn_mobilenet_v3', 'faster_rcnn_mobilenet_v3_320', 'mask_rcnn_resnet', 'mask_rcnn_resnet_v2', 'ssd_vgg16', 'ssd_mobilenet_v3', 'yolov5nu', 'yolov5su', 'yolov5mu', 'yolov5lu', 'yolov5xu', 'yolov5n6u', 'yolov5s6u', 'yolov5m6u', 'yolov5l6u', 'yolov5x6u', 'yolov8n', 'yolov8s', 'yolov8m', 'yolov8l', 'yolov8x', 'yolov9c', 'yolov9e']
best_precisions= [84.01496410369873, 88.00891041755676, 96.10868096351624, 99.2474615573883, 99.67659711837769, 99.35287833213806, 98.90453815460205, 98.84817600250244, 99.59055185317993, 95.25400996208191, 97.86270260810852, 90.19644856452942, 94.16013956069946, 95.83573341369629, 97.12871313095093, 96.57882452011108, 89.94280099868774, 94.4776713848114, 96.59475088119507, 96.98339700698853, 96.82453870773315, 89.8779571056366, 96.51696681976318, 96.09609842300415, 96.97031378746033, 96.952885389328, 95.96365094184875, 97.48988747596741]
avg_precisions= [65.43386094845258, 70.04024246707559, 75.86409708066863, 79.47075281155671, 79.79196265941243, 79.35912337765765, 77.86602516343274, 78.90159849207998, 78.61191980895542, 78.07162496858014, 79.79971235617995, 70.27605364399571, 74.80887480684228, 75.18549493380955, 80.17541258423417, 82.86381429061294, 71.77158703609389, 80.71076551849923, 80.024529203003, 81.26633186583166, 83.0747248345961, 73.97318577134845, 79.52743735942212, 82.74207940713607, 84.03342363031153, 87.20352448084775, 83.06269957905724, 86.73328682420335]
worst_precisions= [50.01915693283081, 50.76141953468323, 50.47063231468201, 50.00084042549133, 50.112247467041016, 50.148385763168335, 50.01423358917236, 50.0189483165741, 50.20836591720581, 50.52481293678284, 51.88778638839722, 51.16922855377197, 51.456159353256226, 50.1446008682251, 50.27618408203125, 50.6072461605072, 50.431621074676514, 50.64784288406372, 50.05226135253906, 50.41007399559021, 50.942665338516235, 50.188636779785156, 50.04086494445801, 51.17880702018738, 52.24266052246094, 51.962679624557495, 50.43753385543823, 50.09565353393555]
best_speeds= [0.08676719665527344, 0.09474658966064453, 0.0887613296508789, 0.08477282524108887, 0.11070370674133301, 0.020943641662597656, 0.01795196533203125, 0.09973359107971191, 0.12666106224060059, 0.045877933502197266, 0.04886817932128906, 0.010955810546875, 0.00997018814086914, 0.013960123062133789, 0.01892399787902832, 0.029922008514404297, 0.018950223922729492, 0.029918193817138672, 0.060834646224975586, 0.10072851181030273, 0.14860033988952637, 0.009968757629394531, 0.008974790573120117, 0.014957189559936523, 0.0199434757232666, 0.02891993522644043, 0.01795029640197754, 0.030914306640625]
avg_speeds= [0.2001816887121934, 0.11632449552416801, 0.10106161175941934, 0.09269411713665068, 0.11476701417845996, 0.031121264642743923, 0.023747624374750094, 0.11805012026887056, 0.14820145936239335, 0.05496083476586256, 0.0844971959789594, 0.014021179752965128, 0.01366634143365396, 0.02043137959071568, 0.03115912089272151, 0.05408866827686628, 0.024246689413680512, 0.03697841756799248, 0.07675189479415966, 0.12715804908010694, 0.17970170996114276, 0.012265248014437442, 0.013020316323081215, 0.020644101229580967, 0.028963901142385555, 0.041514950640061325, 0.02557828943565409, 0.04672065679577814]
worst_speeds= [1.2553186416625977, 0.16655349731445312, 0.14959979057312012, 0.18191862106323242, 0.15059447288513184, 0.11235880851745605, 0.056847572326660156, 0.15857553482055664, 0.17953062057495117, 0.08477401733398438, 0.16146373748779297, 0.22637557983398438, 0.20844054222106934, 0.37998127937316895, 0.5784504413604736, 1.023259162902832, 0.21741676330566406, 0.3151569366455078, 0.5425503253936768, 0.8906149864196777, 1.5827860832214355, 0.11967873573303223, 0.18350768089294434, 0.30219101905822754, 0.45179128646850586, 0.7001254558563232, 0.3775138854980469, 0.7214341163635254]
best_inference_speeds= [0.08676719665527344, 0.09474658966064453, 0.0887613296508789, 0.08477282524108887, 0.11070370674133301, 0.020943641662597656, 0.01795196533203125, 0.09973359107971191, 0.12666106224060059, 0.045877933502197266, 0.04886817932128906, 0.010955810546875, 0.00997018814086914, 0.013960123062133789, 0.01892399787902832, 0.029922008514404297, 0.018950223922729492, 0.029918193817138672, 0.060834646224975586, 0.10072851181030273, 0.14860033988952637, 0.009968757629394531, 0.008974790573120117, 0.014957189559936523, 0.0199434757232666, 0.02891993522644043, 0.01795029640197754, 0.030914306640625]
avg_inference_speeds= [0.19512719275003457, 0.11609358665270683, 0.10128006904940062, 0.09190036151223555, 0.1148357405549004, 0.031369604750307735, 0.023917558030312105, 0.11662774000849042, 0.14811004343486966, 0.05504635995434177, 0.08398789167404175, 0.01408451795578003, 0.013837132630524811, 0.020812347235269105, 0.031673181487853266, 0.055228447485826684, 0.024346482926520748, 0.03726206008036425, 0.07754206657409668, 0.12861353496335587, 0.18183531376145642, 0.012293284666453692, 0.01320547969252975, 0.020817265539112204, 0.02929369943687715, 0.0421951881476811, 0.025854061885052418, 0.047563934610003515]
worst_inference_speeds= [1.2553186416625977, 0.16655349731445312, 0.14959979057312012, 0.18191862106323242, 0.15059447288513184, 0.11235880851745605, 0.056847572326660156, 0.15857553482055664, 0.17953062057495117, 0.08477401733398438, 0.16146373748779297, 0.22637557983398438, 0.20844054222106934, 0.37998127937316895, 0.5784504413604736, 1.023259162902832, 0.21741676330566406, 0.3151569366455078, 0.5425503253936768, 0.8906149864196777, 1.5827860832214355, 0.11967873573303223, 0.18350768089294434, 0.30219101905822754, 0.45179128646850586, 0.7001254558563232, 0.3775138854980469, 0.7214341163635254]
total_detections= [260, 192, 196, 382, 446, 268, 254, 371, 420, 167, 192, 155, 148, 175, 189, 192, 147, 178, 213, 216, 223, 151, 182, 187, 187, 204, 189, 207]
total_inferences= [166, 156, 158, 167, 168, 167, 161, 168, 168, 155, 164, 148, 135, 151, 166, 167, 138, 157, 160, 159, 161, 141, 162, 167, 166, 168, 166, 168]



import numpy as np
import matplotlib.pyplot as plt
import matplotlib

# Generate a color map
num_models = len(models)
cmap = matplotlib.colormaps.get_cmap('nipy_spectral')
colors = [cmap(i / num_models) for i in range(num_models)]

# Create a figure and plot with unique colors
plt.figure(figsize=(14, 10))
for i, model in enumerate(models):
    plt.scatter(avg_inference_speeds[i], avg_precisions[i], color=colors[i], label=model, edgecolor='black')

print("DEVICE: GPU (NVIDIA RTX2070S Mobile)")
print("SOURCE: data/video/sample.MP4 (1080p60fps Video)")
print("SKIP EVERY 30 FRAMES")
print("TOTALLY PROCESSED FRAMES: 168")
print("COCO Dataset, Classes truck, bus, car, train, bicycle")
print("ALL >50%")
# Axis labels and plot title
plt.xlabel('Average Speed (seconds)')
plt.ylabel('Average Precision (%)')
plt.title('Model Performance: Average Inference Speed x Average Accuracy')
plt.legend(loc='upper left', bbox_to_anchor=(1.05, 1), fontsize='small')
plt.grid(True)
plt.savefig("Analysis/Experiment1/RTX2070S_M/Experiment1b_Diagram.png", bbox_inches='tight')
plt.show()

# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_precisions, avg_precisions, worst_precisions])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Precision (%)", "Average Precision (%)", "Worst Precision (%)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

plt.title("Model Precision Metrics")
plt.savefig("Analysis/Experiment1/RTX2070S_M/Experiment1b_Precision_Table.png", bbox_inches='tight')
plt.show()


# # Set up the figure and axis for the table
# fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
# ax.axis('tight')
# ax.axis('off')

# # The table data: transpose the array to make each column a different metric
# table_data = np.transpose([models, best_speeds, avg_speeds, worst_speeds])

# # Create the table in the plot
# table = ax.table(cellText=table_data, colLabels=["Model", "Best Speed (s)", "Average Speed (s)", "Worst Speed (s)"],
#                  cellLoc='center', loc='center', colColours=["palegreen"] * 4)
# table.auto_set_font_size(False)
# table.set_fontsize(10)
# table.scale(1.2, 1.2)  # Scale table size

# plt.title("Model Speed Metrics")
# plt.show()

# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_inference_speeds, avg_inference_speeds, worst_inference_speeds])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Inference Speed (s)", "Average Inference Speed (s)", "Worst Inference Speed (s)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

plt.title("Model Inference Speed Metrics")
plt.savefig("Analysis/Experiment1/RTX2070S_M/Experiment1b_Time_Table.png", bbox_inches='tight')
plt.show()





import cv2
import torch
from torchvision.models.detection import *
from torchvision.io.image import read_image
from torchvision.utils import draw_bounding_boxes
from torchvision.transforms.functional import to_pil_image
import torchvision.transforms as transforms
from PIL import Image
import IPython
from IPython.display import display, clear_output
import time
import traceback
import math
from ultralytics import YOLO

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

transform = transforms.Compose([
    #transforms.Resize((300, 300)),
    transforms.ToTensor(),
])

class DetectionModel():
    def __init__(self, concept, backbone, model, weights, reference, color=None):
        self.concept = concept
        self.backbone = backbone
        self.model = model(weights) if concept=="YOLO" else model(weights=weights.DEFAULT)
        if concept!="YOLO":
            self.model.to(device)
            self.model.eval()
        self.weights = weights if concept=="YOLO" else weights.DEFAULT
        self.reference = reference
        self.color = color
        self.detections = []

    def addDetection(self, detection):
        self.detections.append(detection)

    def getDetections(self):
        return self.detections

    def clearDetections(self):
        self.detections = []

    def detectionsCount(self):
        return len(self.detections)

    def inferencesCount(self):
        length = 0
        addedFrames = []
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    addedFrames.append(self.detections[i].frame)
        return len(addedFrames)

    def getAveragePrecision(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].precision
            avg = sum/len(self.detections)
        return  avg.item() if torch.is_tensor(avg) else avg

    def getMaxPrecision(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].precision:
                max=self.detections[i].precision
        return max.item() if torch.is_tensor(max) else max

    def getMinPrecision(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].precision:
                min=self.detections[i].precision
        return 0.0 if min==9999999.99 else min.item() if torch.is_tensor(min) else min

    def getAverageSpeed(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].speed
            avg = sum/len(self.detections)
        return avg

    def getMaxSpeed(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].speed:
                max=self.detections[i].speed
        return max

    def getMinSpeed(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].speed:
                min=self.detections[i].speed
        return 0.0 if min==9999999.99 else min

    def getAverageInferenceSpeed(self):
        sum=0.0
        avg=0.0
        addedFrames=[]
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    sum+=self.detections[i].speed
                    addedFrames.append(self.detections[i].frame)
            avg = sum/len(addedFrames) if len(addedFrames)>0 else 0.0
        return avg

    def getMaxInferenceSpeed(self):
        max=0.0
        addedFrames=[]
        for i in range(len(self.detections)):
            if max<self.detections[i].speed and self.detections[i].frame not in addedFrames:
                max=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return max

    def getMinInferenceSpeed(self):
        min=9999999.99
        addedFrames=[]
        for i in range(len(self.detections)):
            if min>self.detections[i].speed and self.detections[i].frame not in addedFrames:
                min=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return 0.0 if min==9999999.99 else min

    def filterPrecisionsBelow(self, minPrecision):
        newDetections = []
        for i in range(len(self.detections)):
            if self.detections[i].precision>=minPrecision:
                newDetections.append(self.detections[i])
        self.detections = newDetections

    def printResults(self):
        print("Model:", self.reference,
              "\nBest Precision:", self.getMaxPrecision(),
              "%\nAverage Precision:", self.getAveragePrecision(),
              "%\nWorst Precision:", self.getMinPrecision(),
              "%\nBest Speed:", self.getMinSpeed(),
              "seconds\nAverage Speed:", self.getAverageSpeed(),
              "seconds\nWorst Speed:", self.getMaxSpeed(),
              "seconds\nBest Inference Speed:", self.getMinInferenceSpeed(),
              "seconds\nAverage Inference Speed:", self.getAverageInferenceSpeed(),
              "seconds\nWorst Inference Speed:", self.getMaxInferenceSpeed(),
              "seconds\nTotal Detections:", self.detectionsCount(),
              "\nTotal Inferences:", self.inferencesCount(),
              "\n\n")

class DetectionResult():
    def __init__(self, model, label, precision, speed, device=None, source=None, frame=None, boundingBox=None, details=None):
        self.model = model
        self.label = label
        self.precision = precision.item() if torch.is_tensor(precision) else precision
        self.speed = speed.item() if torch.is_tensor(speed) else speed
        self.device = device
        self.source = source
        self.frame = frame
        self.boundingBox = boundingBox
        self.details = details

target_classes = ["truck", "bus", "car", "train", "bicycle"]


VEHICLE_DETECTION_MODELS = [
    DetectionModel("FCOS", "resnet50_fpn", fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights, "fcos_resnet", (255,0,0)),
    DetectionModel("RetinaNet", "resnet50_fpn", retinanet_resnet50_fpn, RetinaNet_ResNet50_FPN_Weights, "retinanet_resnet", (255,255,0)),
    DetectionModel("RetinaNet", "resnet50_fpn_v2", retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights, "retinanet_resnet_v2", (0,234,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn", fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights, "faster_rcnn_resnet", (170,0,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn_v2", fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights, "faster_rcnn_resnet_v2", (255,127,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_fpn", fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights, "faster_rcnn_mobilenet_v3", (191,255,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_320_fpn", fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights, "faster_rcnn_mobilenet_v3_320", (0,149,255)),
    DetectionModel("MaskRCNN", "resnet50_fpn", maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights, "mask_rcnn_resnet", (255,0,170)),
    DetectionModel("MaskRCNN", "resnet50_fpn_v2", maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights, "mask_rcnn_resnet_v2", (255,212,0)),
    DetectionModel("SSD300", "vgg16", ssd300_vgg16, SSD300_VGG16_Weights, "ssd_vgg16", (106,255,0)),
    DetectionModel("SSDLite320", "mobilenet_v3_large", ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights, "ssd_mobilenet_v3", (0,64,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5nu.pt', "yolov5nu", (237,185,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5su.pt', "yolov5su", (185,215,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5mu.pt', "yolov5mu", (231,233,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5lu.pt', "yolov5lu", (220,185,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5xu.pt', "yolov5xu", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5n6u.pt', "yolov5n6u", (143,35,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5s6u.pt', "yolov5s6u", (35,98,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5m6u.pt', "yolov5m6u", (143,106,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5l6u.pt', "yolov5l6u", (107,35,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5x6u.pt', "yolov5x6u", (79,143,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8n.pt', "yolov8n", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8s.pt', "yolov8s", (115,115,115)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8m.pt', "yolov8m", (204,204,204)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8l.pt', "yolov8l", (255,0,0)), # cycle restarts
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8x.pt', "yolov8x", (255,255,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9c.pt', "yolov9c", (0,234,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9e.pt', "yolov9e", (170,0,255))#,
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10n.pt', "yolov10n", (185,237,224)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10s.pt', "yolov10s", (115,115,115)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10m.pt', "yolov10m", (204,204,204)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10b.pt', "yolov10b", (204,204,204)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10l.pt', "yolov10l", (255,0,0)), # cycle restarts
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10x.pt', "yolov10x", (255,255,0))
]

def process(frame, frameIndex=-1, source=None):
    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        processed_frame = frame.copy()

        if currentModel.concept=="YOLO":
            start_time = time.time()
            results = currentModel.model(processed_frame, agnostic_nms=True, verbose=False, device=device)[0]
            processing_time = time.time() - start_time
            for result in results:
                detection_count = result.boxes.shape[0]
                for j in range(detection_count):
                    cls = int(result.boxes.cls[j].item())
                    name = result.names[cls]
                    confidence = float(result.boxes.conf[j].item())
                    bounding_box = result.boxes.xyxy[j].cpu().numpy()
                    if name in target_classes and confidence>0.5:
                        x1, y1, x2, y2 = [int(x) for x in bounding_box]
                        # Ensure x1 < x2 and y1 < y2
                        x1, x2 = min(x1, x2), max(x1, x2)
                        y1, y2 = min(y1, y2), max(y1, y2)
                        label = '{} ({:.2f}%) ({:.3f}s) {}'.format(name.upper(), confidence*100, processing_time, currentModel.reference)
                        print(label)
                        VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, name, confidence*100, processing_time, device, source, frameIndex, bounding_box, label))
                        cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                        cv2.putText(processed_frame, label, (x1+10, y1+25), 0, 0.8, currentModel.color, 2)
        else:
            # Convert frame to RGB and PIL Image, then apply transformation
            frame_rgb = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            image = transform(pil_image).unsqueeze(0).to(device)

            # Object detection
            start_time = time.time()
            with torch.no_grad():
                predictions = currentModel.model(image)

            # Processing time
            processing_time = time.time() - start_time

            # Visualization
            scores = predictions[0]['scores']
            boxes = predictions[0]['boxes']
            labels = predictions[0]['labels']
            confidence_threshold = 0.5

            for score, box, label in zip(scores, boxes, labels):
                # and score > confidence_threshold
                if currentModel.weights.meta["categories"][label.item()] in target_classes and score > confidence_threshold:
                    x1, y1, x2, y2 = map(int, box)
                    label_text = f'{currentModel.weights.meta["categories"][label.item()].upper()} ({score*100:.2f}%) ({processing_time:.3f}s) {currentModel.reference}'
                    print(label_text)
                    VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, currentModel.weights.meta["categories"][label.item()], float(score.item())*100, processing_time, device, source, frameIndex, box, label_text))
                    cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                    cv2.putText(processed_frame, label_text, (x1, y1+20), 0, 0.8, currentModel.color, 2)

        img = Image.fromarray(processed_frame, 'RGB')
        display(img)
        clear_output(wait=True)
    return frame

def handleFrame(frame, frameIndex=-1, source=None):
    processed_frame = process(frame,frameIndex,source)

def handleRelease():
    cam.release()
    print("Source released.")

    print(device)
    print(source)

    BestPrecision=None
    BestAvgPrecision=None
    WorstPrecision=None
    BestSpeed=None
    BestAvgSpeed=None
    WorstSpeed=None

    Models=[]
    BestPrecisions=[]
    AvgPrecisions=[]
    WorstPrecisions=[]
    BestSpeeds=[]
    AvgSpeeds=[]
    WorstSpeeds=[]
    BestInferenceSpeeds=[]
    AvgInferenceSpeeds=[]
    WorstInferenceSpeeds=[]
    TotalDetectionsArr=[]
    TotalInferencesArr=[]

    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        # currentModel.printResults()

        Models.append(currentModel.reference)
        BestPrecisions.append(currentModel.getMaxPrecision())
        AvgPrecisions.append(currentModel.getAveragePrecision())
        WorstPrecisions.append(currentModel.getMinPrecision())
        BestSpeeds.append(currentModel.getMinSpeed())
        AvgSpeeds.append(currentModel.getAverageSpeed())
        WorstSpeeds.append(currentModel.getMaxSpeed())
        BestInferenceSpeeds.append(currentModel.getMinInferenceSpeed())
        AvgInferenceSpeeds.append(currentModel.getAverageInferenceSpeed())
        WorstInferenceSpeeds.append(currentModel.getMaxInferenceSpeed())
        TotalDetectionsArr.append(currentModel.detectionsCount())
        TotalInferencesArr.append(currentModel.inferencesCount())

        if BestPrecision is None or BestPrecision.getMaxPrecision()<=currentModel.getMaxPrecision():
            BestPrecision = currentModel
        if BestAvgPrecision is None or BestAvgPrecision.getAveragePrecision()<=currentModel.getAveragePrecision():
            BestAvgPrecision = currentModel
        if WorstPrecision is None or WorstPrecision.getMinPrecision()>=currentModel.getMinPrecision():
            WorstPrecision = currentModel
        if BestSpeed is None or BestSpeed.getMinSpeed()>=currentModel.getMinSpeed():
            BestSpeed = currentModel
        if BestAvgSpeed is None or BestAvgSpeed.getAverageSpeed()>=currentModel.getAverageSpeed():
            BestAvgSpeed = currentModel
        if WorstSpeed is None or WorstSpeed.getMaxSpeed()<=currentModel.getMaxSpeed():
            WorstSpeed = currentModel

    if BestPrecision is not None:
        print("Best Precision:")
        BestPrecision.printResults()
    if BestAvgPrecision is not None:
        print("Best Average Precision:")
        BestAvgPrecision.printResults()
    if WorstPrecision is not None:
        print("Worst Precision:")
        WorstPrecision.printResults()
    if BestSpeed is not None:
        print("Best Speed:")
        BestSpeed.printResults()
    if BestAvgSpeed is not None:
        print("Best Average Speed:")
        BestAvgSpeed.printResults()
    if WorstSpeed is not None:
        print("Worst Speed:")
        WorstSpeed.printResults()

    print("models=",Models)
    print("best_precisions=",BestPrecisions)
    print("avg_precisions=",AvgPrecisions)
    print("worst_precisions=",WorstPrecisions)
    print("best_speeds=",BestSpeeds)
    print("avg_speeds=",AvgSpeeds)
    print("worst_speeds=",WorstSpeeds)
    print("best_inference_speeds=",BestInferenceSpeeds)
    print("avg_inference_speeds=",AvgInferenceSpeeds)
    print("worst_inference_speeds=",WorstInferenceSpeeds)
    print("total_detections=",TotalDetectionsArr)
    print("total_inferences=",TotalInferencesArr)

inputType="video"
source = "data/video/sample.MP4" if inputType=="video" else 0
cam = cv2.VideoCapture(source)
display_handle=display(None, display_id=True)
every = 60

start = 0 if source!=0 else -1
end = int(cam.get(cv2.CAP_PROP_FRAME_COUNT)) if source!=0 else -1
if source!=0: cam.set(1, start)

frameCount=start
while_safety=0
saved_count=0

if not cam.isOpened(): print("Error: Could not open source.")
else:
    try:
        while True if end<0 else frameCount<end:
            _, frame = cam.read()
            if frame is None:
                if source!=0:
                    if while_safety > 2000: break
                    while_safety += 1
                    continue
                else:
                    print("Error: Could not capture frame.")
                    cam.release()
                    break

            if every>0:
                if (frameCount+1)%math.floor(every) == 0:
                    while_safety = 0
                    handleFrame(frame, frameCount, source)
            else:
                while_safety=0
                handleFrame(frame)
            frameCount += 1
        handleRelease()
    except KeyboardInterrupt:
        handleRelease()
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Source could not be released.")
        except:
            pass
        traceback.print_exc()


models= ['fcos_resnet', 'retinanet_resnet', 'retinanet_resnet_v2', 'faster_rcnn_resnet', 'faster_rcnn_resnet_v2', 'faster_rcnn_mobilenet_v3', 'faster_rcnn_mobilenet_v3_320', 'mask_rcnn_resnet', 'mask_rcnn_resnet_v2', 'ssd_vgg16', 'ssd_mobilenet_v3', 'yolov5nu', 'yolov5su', 'yolov5mu', 'yolov5lu', 'yolov5xu', 'yolov5n6u', 'yolov5s6u', 'yolov5m6u', 'yolov5l6u', 'yolov5x6u', 'yolov8n', 'yolov8s', 'yolov8m', 'yolov8l', 'yolov8x', 'yolov9c', 'yolov9e']
best_precisions= [84.01496410369873, 86.20967268943787, 96.10868096351624, 99.2474615573883, 99.60416555404663, 99.35287833213806, 98.90453815460205, 98.71110916137695, 99.41350817680359, 95.25400996208191, 97.86270260810852, 89.5509660243988, 94.16013956069946, 95.83573341369629, 95.45990824699402, 96.57882452011108, 89.94280099868774, 93.81154775619507, 96.59475088119507, 96.98339700698853, 96.82453870773315, 89.8779571056366, 96.51696681976318, 96.09609842300415, 96.97031378746033, 96.952885389328, 95.78875303268433, 96.19094133377075]
avg_precisions= [65.3711177119913, 69.96585333470216, 76.44102133885778, 79.92927977108063, 80.57217361740253, 78.7827680216116, 78.05266068316996, 78.70424317686181, 78.5829365535935, 78.78162997314729, 78.83798553019153, 70.05795141061147, 74.8091949175482, 75.2926494493041, 79.60578637818496, 82.98254242787759, 72.58705093827047, 81.56127782350175, 79.96549305461701, 81.12766327602523, 83.55558301721301, 74.63139560487535, 79.01419734442106, 82.5258315891348, 84.4868039307387, 87.62376243525212, 82.92970356188323, 87.14067374618308]
worst_precisions= [50.034648180007935, 51.02283954620361, 50.603169202804565, 50.007373094558716, 50.112247467041016, 50.148385763168335, 52.15638875961304, 50.028812885284424, 50.2233624458313, 51.88530683517456, 52.02556252479553, 51.16922855377197, 51.456159353256226, 50.1446008682251, 50.27618408203125, 51.19196176528931, 51.13384127616882, 50.64784288406372, 50.96127986907959, 54.25072908401489, 51.13029479980469, 50.188636779785156, 50.04086494445801, 51.17880702018738, 53.08571457862854, 56.22342228889465, 52.18823552131653, 51.28079056739807]
best_speeds= [0.08477401733398438, 0.09474658966064453, 0.08975839614868164, 0.08477234840393066, 0.11968660354614258, 0.0249326229095459, 0.019945859909057617, 0.09873628616333008, 0.18550419807434082, 0.048872947692871094, 0.04785895347595215, 0.010968685150146484, 0.009973287582397461, 0.016952037811279297, 0.025930404663085938, 0.03789710998535156, 0.018946170806884766, 0.029918432235717773, 0.054833173751831055, 0.07280302047729492, 0.11070060729980469, 0.009969472885131836, 0.009969949722290039, 0.01494908332824707, 0.0199429988861084, 0.028901100158691406, 0.017950773239135742, 0.030915021896362305]
avg_speeds= [0.18108168868131416, 0.11734556168625035, 0.10505249283530495, 0.09345614718880883, 0.43944255657459735, 0.03556693476789138, 0.025813737884163857, 0.12253570054706774, 1.1260498067214026, 0.07095449803823448, 0.08498683997562953, 0.012888886989691319, 0.016107954391061444, 0.023553108060082723, 0.036235677699247994, 0.07112061729033788, 0.025282453483259176, 0.040586525134825975, 0.10368267468043736, 0.11358723257269178, 0.13864606193133763, 0.013284573952356974, 0.01330953259621897, 0.02047098067498976, 0.02850897156673929, 0.04177810177944674, 0.02670703185232062, 0.05102737667491135]
worst_speeds= [0.4278559684753418, 0.16256451606750488, 0.14960050582885742, 0.13364148139953613, 0.48569393157958984, 0.053856849670410156, 0.051862478256225586, 0.14960074424743652, 1.7273781299591064, 0.15658164024353027, 0.11170053482055664, 0.016958236694335938, 0.23038625717163086, 0.36302804946899414, 0.6492617130279541, 1.196798324584961, 0.19945335388183594, 0.06981253623962402, 0.1436176300048828, 0.22838830947875977, 0.18450713157653809, 0.14760494232177734, 0.16456127166748047, 0.2961885929107666, 0.4428129196166992, 0.6502594947814941, 0.459758996963501, 0.8716676235198975]
best_inference_speeds= [0.08477401733398438, 0.09474658966064453, 0.08975839614868164, 0.08477234840393066, 0.11968660354614258, 0.0249326229095459, 0.019945859909057617, 0.09873628616333008, 0.18550419807434082, 0.048872947692871094, 0.04785895347595215, 0.010968685150146484, 0.009973287582397461, 0.016952037811279297, 0.025930404663085938, 0.03789710998535156, 0.018946170806884766, 0.029918432235717773, 0.054833173751831055, 0.07280302047729492, 0.11070060729980469, 0.009969472885131836, 0.009969949722290039, 0.01494908332824707, 0.0199429988861084, 0.028901100158691406, 0.017950773239135742, 0.030915021896362305]
avg_inference_speeds= [0.1754465074424284, 0.1173780667476165, 0.1049819529056549, 0.09342502685914557, 0.4369510071618216, 0.03512072847003028, 0.025376611285739474, 0.1208081841468811, 1.0711590959912254, 0.071168337112818, 0.08484530161662274, 0.012924694035151233, 0.01637426063195983, 0.024218668808808196, 0.03714990048181443, 0.07471537022363572, 0.025350435920383618, 0.041164627200678774, 0.10430167294755767, 0.11369410455226898, 0.13795097374621731, 0.013405764804166906, 0.013476930013517054, 0.020690601992319865, 0.028992960251957536, 0.04275333597546532, 0.027414216881706602, 0.05341547443753197]
worst_inference_speeds= [0.4278559684753418, 0.16256451606750488, 0.14960050582885742, 0.13364148139953613, 0.48569393157958984, 0.053856849670410156, 0.051862478256225586, 0.14960074424743652, 1.7273781299591064, 0.15658164024353027, 0.11170053482055664, 0.016958236694335938, 0.23038625717163086, 0.36302804946899414, 0.6492617130279541, 1.196798324584961, 0.19945335388183594, 0.06981253623962402, 0.1436176300048828, 0.22838830947875977, 0.18450713157653809, 0.14760494232177734, 0.16456127166748047, 0.2961885929107666, 0.4428129196166992, 0.6502594947814941, 0.459758996963501, 0.8716676235198975]
total_detections= [129, 97, 99, 187, 217, 136, 128, 190, 211, 83, 98, 78, 73, 86, 96, 96, 71, 89, 105, 112, 112, 72, 93, 93, 92, 101, 95, 103]
total_inferences= [83, 78, 80, 83, 84, 84, 81, 84, 84, 78, 83, 73, 67, 74, 84, 84, 69, 76, 79, 80, 81, 68, 82, 83, 83, 84, 84, 84]


import numpy as np
import matplotlib.pyplot as plt
import matplotlib

# Generate a color map
num_models = len(models)
cmap = matplotlib.colormaps.get_cmap('nipy_spectral')
colors = [cmap(i / num_models) for i in range(num_models)]

# Create a figure and plot with unique colors
plt.figure(figsize=(14, 10))
for i, model in enumerate(models):
    plt.scatter(avg_inference_speeds[i], avg_precisions[i], color=colors[i], label=model, edgecolor='black')

print("DEVICE: GPU (NVIDIA RTX2070S Mobile)")
print("SOURCE: data/video/sample.MP4 (1080p60fps Video)")
print("SKIP EVERY 60 FRAMES")
print("TOTALLY PROCESSED FRAMES: 84")
print("COCO Dataset, Classes truck, bus, car, train, bicycle")
print("ALL >50%")
# Axis labels and plot title
plt.xlabel('Average Speed (seconds)')
plt.ylabel('Average Precision (%)')
plt.title('Model Performance: Average Inference Speed x Average Accuracy')
plt.legend(loc='upper left', bbox_to_anchor=(1.05, 1), fontsize='small')
plt.grid(True)
plt.savefig("Analysis/Experiment1/RTX2070S_M/Experiment1a_Diagram.png", bbox_inches='tight')
plt.show()

# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_precisions, avg_precisions, worst_precisions])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Precision (%)", "Average Precision (%)", "Worst Precision (%)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

plt.title("Model Precision Metrics")
plt.savefig("Analysis/Experiment1/RTX2070S_M/Experiment1a_Precision_Table.png", bbox_inches='tight')
plt.show()


# # Set up the figure and axis for the table
# fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
# ax.axis('tight')
# ax.axis('off')

# # The table data: transpose the array to make each column a different metric
# table_data = np.transpose([models, best_speeds, avg_speeds, worst_speeds])

# # Create the table in the plot
# table = ax.table(cellText=table_data, colLabels=["Model", "Best Speed (s)", "Average Speed (s)", "Worst Speed (s)"],
#                  cellLoc='center', loc='center', colColours=["palegreen"] * 4)
# table.auto_set_font_size(False)
# table.set_fontsize(10)
# table.scale(1.2, 1.2)  # Scale table size

# plt.title("Model Speed Metrics")
# plt.show()

# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_inference_speeds, avg_inference_speeds, worst_inference_speeds])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Inference Speed (s)", "Average Inference Speed (s)", "Worst Inference Speed (s)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

plt.title("Model Inference Speed Metrics")
plt.savefig("Analysis/Experiment1/RTX2070S_M/Experiment1a_Time_Table.png", bbox_inches='tight')
plt.show()





import cv2
import torch
from torchvision.models.detection import *
from torchvision.io.image import read_image
from torchvision.utils import draw_bounding_boxes
from torchvision.transforms.functional import to_pil_image
import torchvision.transforms as transforms
from PIL import Image
import IPython
from IPython.display import display, clear_output
import time
import traceback
import math
from ultralytics import YOLO

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
if device=="cuda:0":
    torch.cuda.empty_cache()

transform = transforms.Compose([
    #transforms.Resize((300, 300)),
    transforms.ToTensor(),
])

class DetectionModel():
    def __init__(self, concept, backbone, model, weights, reference, color=None):
        self.concept = concept
        self.backbone = backbone
        self.model = model(weights) if concept=="YOLO" else model(weights=weights.DEFAULT)
        if concept!="YOLO":
            self.model.to(device)
            self.model.eval()
        self.weights = weights if concept=="YOLO" else weights.DEFAULT
        self.reference = reference
        self.color = color
        self.detections = []

    def addDetection(self, detection):
        self.detections.append(detection)

    def getDetections(self):
        return self.detections

    def clearDetections(self):
        self.detections = []

    def detectionsCount(self):
        return len(self.detections)

    def inferencesCount(self):
        length = 0
        addedFrames = []
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    addedFrames.append(self.detections[i].frame)
        return len(addedFrames)

    def getAveragePrecision(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].precision
            avg = sum/len(self.detections)
        return  avg.item() if torch.is_tensor(avg) else avg

    def getMaxPrecision(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].precision:
                max=self.detections[i].precision
        return max.item() if torch.is_tensor(max) else max

    def getMinPrecision(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].precision:
                min=self.detections[i].precision
        return 0.0 if min==9999999.99 else min.item() if torch.is_tensor(min) else min

    def getAverageSpeed(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].speed
            avg = sum/len(self.detections)
        return avg

    def getMaxSpeed(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].speed:
                max=self.detections[i].speed
        return max

    def getMinSpeed(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].speed:
                min=self.detections[i].speed
        return 0.0 if min==9999999.99 else min

    def getAverageInferenceSpeed(self):
        sum=0.0
        avg=0.0
        addedFrames=[]
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    sum+=self.detections[i].speed
                    addedFrames.append(self.detections[i].frame)
            avg = sum/len(addedFrames) if len(addedFrames)>0 else 0.0
        return avg

    def getMaxInferenceSpeed(self):
        max=0.0
        addedFrames=[]
        for i in range(len(self.detections)):
            if max<self.detections[i].speed and self.detections[i].frame not in addedFrames:
                max=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return max

    def getMinInferenceSpeed(self):
        min=9999999.99
        addedFrames=[]
        for i in range(len(self.detections)):
            if min>self.detections[i].speed and self.detections[i].frame not in addedFrames:
                min=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return 0.0 if min==9999999.99 else min

    def filterPrecisionsBelow(self, minPrecision):
        newDetections = []
        for i in range(len(self.detections)):
            if self.detections[i].precision>=minPrecision:
                newDetections.append(self.detections[i])
        self.detections = newDetections

    def printResults(self):
        print("Model:", self.reference,
              "\nBest Precision:", self.getMaxPrecision(),
              "%\nAverage Precision:", self.getAveragePrecision(),
              "%\nWorst Precision:", self.getMinPrecision(),
              "%\nBest Speed:", self.getMinSpeed(),
              "seconds\nAverage Speed:", self.getAverageSpeed(),
              "seconds\nWorst Speed:", self.getMaxSpeed(),
              "seconds\nBest Inference Speed:", self.getMinInferenceSpeed(),
              "seconds\nAverage Inference Speed:", self.getAverageInferenceSpeed(),
              "seconds\nWorst Inference Speed:", self.getMaxInferenceSpeed(),
              "seconds\nTotal Detections:", self.detectionsCount(),
              "\nTotal Inferences:", self.inferencesCount(),
              "\n\n")

class DetectionResult():
    def __init__(self, model, label, precision, speed, device=None, source=None, frame=None, boundingBox=None, details=None):
        self.model = model
        self.label = label
        self.precision = precision.item() if torch.is_tensor(precision) else precision
        self.speed = speed.item() if torch.is_tensor(speed) else speed
        self.device = device
        self.source = source
        self.frame = frame
        self.boundingBox = boundingBox
        self.details = details

target_classes = ["truck", "bus", "car", "train", "bicycle"]


VEHICLE_DETECTION_MODELS = [
    DetectionModel("FCOS", "resnet50_fpn", fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights, "fcos_resnet", (255,0,0)),
    DetectionModel("RetinaNet", "resnet50_fpn", retinanet_resnet50_fpn, RetinaNet_ResNet50_FPN_Weights, "retinanet_resnet", (255,255,0)),
    DetectionModel("RetinaNet", "resnet50_fpn_v2", retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights, "retinanet_resnet_v2", (0,234,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn", fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights, "faster_rcnn_resnet", (170,0,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn_v2", fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights, "faster_rcnn_resnet_v2", (255,127,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_fpn", fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights, "faster_rcnn_mobilenet_v3", (191,255,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_320_fpn", fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights, "faster_rcnn_mobilenet_v3_320", (0,149,255)),
    DetectionModel("MaskRCNN", "resnet50_fpn", maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights, "mask_rcnn_resnet", (255,0,170)),
    DetectionModel("MaskRCNN", "resnet50_fpn_v2", maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights, "mask_rcnn_resnet_v2", (255,212,0)),
    DetectionModel("SSD300", "vgg16", ssd300_vgg16, SSD300_VGG16_Weights, "ssd_vgg16", (106,255,0)),
    DetectionModel("SSDLite320", "mobilenet_v3_large", ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights, "ssd_mobilenet_v3", (0,64,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5nu.pt', "yolov5nu", (237,185,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5su.pt', "yolov5su", (185,215,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5mu.pt', "yolov5mu", (231,233,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5lu.pt', "yolov5lu", (220,185,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5xu.pt', "yolov5xu", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5n6u.pt', "yolov5n6u", (143,35,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5s6u.pt', "yolov5s6u", (35,98,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5m6u.pt', "yolov5m6u", (143,106,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5l6u.pt', "yolov5l6u", (107,35,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5x6u.pt', "yolov5x6u", (79,143,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8n.pt', "yolov8n", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8s.pt', "yolov8s", (115,115,115)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8m.pt', "yolov8m", (204,204,204)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8l.pt', "yolov8l", (255,0,0)), # cycle restarts
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8x.pt', "yolov8x", (255,255,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9c.pt', "yolov9c", (0,234,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9e.pt', "yolov9e", (170,0,255))#,
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10n.pt', "yolov10n", (185,237,224)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10s.pt', "yolov10s", (115,115,115)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10m.pt', "yolov10m", (204,204,204)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10b.pt', "yolov10b", (204,204,204)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10l.pt', "yolov10l", (255,0,0)), # cycle restarts
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10x.pt', "yolov10x", (255,255,0))
]

def process(frame, frameIndex=-1, source=None):
    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        processed_frame = frame.copy()

        if currentModel.concept=="YOLO":
            start_time = time.time()
            results = currentModel.model(processed_frame, agnostic_nms=True, verbose=False, device=device)[0]
            processing_time = time.time() - start_time
            for result in results:
                detection_count = result.boxes.shape[0]
                for j in range(detection_count):
                    cls = int(result.boxes.cls[j].item())
                    name = result.names[cls]
                    confidence = float(result.boxes.conf[j].item())
                    bounding_box = result.boxes.xyxy[j].cpu().numpy()
                    if name in target_classes and confidence>0.5:
                        x1, y1, x2, y2 = [int(x) for x in bounding_box]
                        # Ensure x1 < x2 and y1 < y2
                        x1, x2 = min(x1, x2), max(x1, x2)
                        y1, y2 = min(y1, y2), max(y1, y2)
                        label = '{} ({:.2f}%) ({:.3f}s) {}'.format(name.upper(), confidence*100, processing_time, currentModel.reference)
                        print(label)
                        VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, name, confidence*100, processing_time, device, source, frameIndex, bounding_box, label))
                        cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                        cv2.putText(processed_frame, label, (x1+10, y1+25), 0, 0.8, currentModel.color, 2)
        else:
            # Convert frame to RGB and PIL Image, then apply transformation
            frame_rgb = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            image = transform(pil_image).unsqueeze(0).to(device)

            # Object detection
            start_time = time.time()
            with torch.no_grad():
                predictions = currentModel.model(image)

            # Processing time
            processing_time = time.time() - start_time

            # Visualization
            scores = predictions[0]['scores']
            boxes = predictions[0]['boxes']
            labels = predictions[0]['labels']
            confidence_threshold = 0.5

            for score, box, label in zip(scores, boxes, labels):
                # and score > confidence_threshold
                if currentModel.weights.meta["categories"][label.item()] in target_classes and score > confidence_threshold:
                    x1, y1, x2, y2 = map(int, box)
                    label_text = f'{currentModel.weights.meta["categories"][label.item()].upper()} ({score*100:.2f}%) ({processing_time:.3f}s) {currentModel.reference}'
                    print(label_text)
                    VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, currentModel.weights.meta["categories"][label.item()], float(score.item())*100, processing_time, device, source, frameIndex, box, label_text))
                    cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                    cv2.putText(processed_frame, label_text, (x1, y1+20), 0, 0.8, currentModel.color, 2)

        img = Image.fromarray(processed_frame, 'RGB')
        display(img)
        clear_output(wait=True)
    return frame

def handleFrame(frame, frameIndex=-1, source=None):
    processed_frame = process(frame,frameIndex,source)

def handleRelease():
    cam.release()
    print("Source released.")

    print(device)
    print(source)

    BestPrecision=None
    BestAvgPrecision=None
    WorstPrecision=None
    BestSpeed=None
    BestAvgSpeed=None
    WorstSpeed=None

    Models=[]
    BestPrecisions=[]
    AvgPrecisions=[]
    WorstPrecisions=[]
    BestSpeeds=[]
    AvgSpeeds=[]
    WorstSpeeds=[]
    BestInferenceSpeeds=[]
    AvgInferenceSpeeds=[]
    WorstInferenceSpeeds=[]
    TotalDetectionsArr=[]
    TotalInferencesArr=[]

    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        # currentModel.printResults()

        Models.append(currentModel.reference)
        BestPrecisions.append(currentModel.getMaxPrecision())
        AvgPrecisions.append(currentModel.getAveragePrecision())
        WorstPrecisions.append(currentModel.getMinPrecision())
        BestSpeeds.append(currentModel.getMinSpeed())
        AvgSpeeds.append(currentModel.getAverageSpeed())
        WorstSpeeds.append(currentModel.getMaxSpeed())
        BestInferenceSpeeds.append(currentModel.getMinInferenceSpeed())
        AvgInferenceSpeeds.append(currentModel.getAverageInferenceSpeed())
        WorstInferenceSpeeds.append(currentModel.getMaxInferenceSpeed())
        TotalDetectionsArr.append(currentModel.detectionsCount())
        TotalInferencesArr.append(currentModel.inferencesCount())

        if BestPrecision is None or BestPrecision.getMaxPrecision()<=currentModel.getMaxPrecision():
            BestPrecision = currentModel
        if BestAvgPrecision is None or BestAvgPrecision.getAveragePrecision()<=currentModel.getAveragePrecision():
            BestAvgPrecision = currentModel
        if WorstPrecision is None or WorstPrecision.getMinPrecision()>=currentModel.getMinPrecision():
            WorstPrecision = currentModel
        if BestSpeed is None or BestSpeed.getMinSpeed()>=currentModel.getMinSpeed():
            BestSpeed = currentModel
        if BestAvgSpeed is None or BestAvgSpeed.getAverageSpeed()>=currentModel.getAverageSpeed():
            BestAvgSpeed = currentModel
        if WorstSpeed is None or WorstSpeed.getMaxSpeed()<=currentModel.getMaxSpeed():
            WorstSpeed = currentModel

    if BestPrecision is not None:
        print("Best Precision:")
        BestPrecision.printResults()
    if BestAvgPrecision is not None:
        print("Best Average Precision:")
        BestAvgPrecision.printResults()
    if WorstPrecision is not None:
        print("Worst Precision:")
        WorstPrecision.printResults()
    if BestSpeed is not None:
        print("Best Speed:")
        BestSpeed.printResults()
    if BestAvgSpeed is not None:
        print("Best Average Speed:")
        BestAvgSpeed.printResults()
    if WorstSpeed is not None:
        print("Worst Speed:")
        WorstSpeed.printResults()

    print("models=",Models)
    print("best_precisions=",BestPrecisions)
    print("avg_precisions=",AvgPrecisions)
    print("worst_precisions=",WorstPrecisions)
    print("best_speeds=",BestSpeeds)
    print("avg_speeds=",AvgSpeeds)
    print("worst_speeds=",WorstSpeeds)
    print("best_inference_speeds=",BestInferenceSpeeds)
    print("avg_inference_speeds=",AvgInferenceSpeeds)
    print("worst_inference_speeds=",WorstInferenceSpeeds)
    print("total_detections=",TotalDetectionsArr)
    print("total_inferences=",TotalInferencesArr)

inputType="video"
source = "data/video/vecteezy_car-and-truck-traffic-on-the-highway-in-europe-poland_7957364.MP4" if inputType=="video" else 0
cam = cv2.VideoCapture(source)
display_handle=display(None, display_id=True)
every = 30

start = 0 if source!=0 else -1
end = int(cam.get(cv2.CAP_PROP_FRAME_COUNT)) if source!=0 else -1
if source!=0: cam.set(1, start)

frameCount=start
while_safety=0
saved_count=0

if not cam.isOpened(): print("Error: Could not open source.")
else:
    try:
        while True if end<0 else frameCount<end:
            _, frame = cam.read()
            if frame is None:
                if source!=0:
                    if while_safety > 2000: break
                    while_safety += 1
                    continue
                else:
                    print("Error: Could not capture frame.")
                    cam.release()
                    break

            if every>0:
                if (frameCount+1)%math.floor(every) == 0:
                    while_safety = 0
                    handleFrame(frame, frameCount, source)
            else:
                while_safety=0
                handleFrame(frame)
            frameCount += 1
        handleRelease()
    except KeyboardInterrupt:
        handleRelease()
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Source could not be released.")
        except:
            pass
        traceback.print_exc()


import cv2
import torch
from torchvision.models.detection import *
from torchvision.io.image import read_image
from torchvision.utils import draw_bounding_boxes
from torchvision.transforms.functional import to_pil_image
import torchvision.transforms as transforms
from PIL import Image
import IPython
from IPython.display import display, clear_output
import time
import traceback
import math
from ultralytics import YOLO

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
if device=="cuda:0":
    torch.cuda.empty_cache()

transform = transforms.Compose([
    #transforms.Resize((300, 300)),
    transforms.ToTensor(),
])

class DetectionModel():
    def __init__(self, concept, backbone, model, weights, reference, color=None):
        self.concept = concept
        self.backbone = backbone
        self.model = model(weights) if concept=="YOLO" else model(weights=weights.DEFAULT)
        if concept!="YOLO":
            self.model.to(device)
            self.model.eval()
        self.weights = weights if concept=="YOLO" else weights.DEFAULT
        self.reference = reference
        self.color = color
        self.detections = []

    def addDetection(self, detection):
        self.detections.append(detection)

    def getDetections(self):
        return self.detections

    def clearDetections(self):
        self.detections = []

    def detectionsCount(self):
        return len(self.detections)

    def inferencesCount(self):
        length = 0
        addedFrames = []
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    addedFrames.append(self.detections[i].frame)
        return len(addedFrames)

    def getAveragePrecision(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].precision
            avg = sum/len(self.detections)
        return  avg.item() if torch.is_tensor(avg) else avg

    def getMaxPrecision(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].precision:
                max=self.detections[i].precision
        return max.item() if torch.is_tensor(max) else max

    def getMinPrecision(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].precision:
                min=self.detections[i].precision
        return 0.0 if min==9999999.99 else min.item() if torch.is_tensor(min) else min

    def getAverageSpeed(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].speed
            avg = sum/len(self.detections)
        return avg

    def getMaxSpeed(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].speed:
                max=self.detections[i].speed
        return max

    def getMinSpeed(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].speed:
                min=self.detections[i].speed
        return 0.0 if min==9999999.99 else min

    def getAverageInferenceSpeed(self):
        sum=0.0
        avg=0.0
        addedFrames=[]
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    sum+=self.detections[i].speed
                    addedFrames.append(self.detections[i].frame)
            avg = sum/len(addedFrames) if len(addedFrames)>0 else 0.0
        return avg

    def getMaxInferenceSpeed(self):
        max=0.0
        addedFrames=[]
        for i in range(len(self.detections)):
            if max<self.detections[i].speed and self.detections[i].frame not in addedFrames:
                max=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return max

    def getMinInferenceSpeed(self):
        min=9999999.99
        addedFrames=[]
        for i in range(len(self.detections)):
            if min>self.detections[i].speed and self.detections[i].frame not in addedFrames:
                min=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return 0.0 if min==9999999.99 else min

    def filterPrecisionsBelow(self, minPrecision):
        newDetections = []
        for i in range(len(self.detections)):
            if self.detections[i].precision>=minPrecision:
                newDetections.append(self.detections[i])
        self.detections = newDetections

    def printResults(self):
        print("Model:", self.reference,
              "\nBest Precision:", self.getMaxPrecision(),
              "%\nAverage Precision:", self.getAveragePrecision(),
              "%\nWorst Precision:", self.getMinPrecision(),
              "%\nBest Speed:", self.getMinSpeed(),
              "seconds\nAverage Speed:", self.getAverageSpeed(),
              "seconds\nWorst Speed:", self.getMaxSpeed(),
              "seconds\nBest Inference Speed:", self.getMinInferenceSpeed(),
              "seconds\nAverage Inference Speed:", self.getAverageInferenceSpeed(),
              "seconds\nWorst Inference Speed:", self.getMaxInferenceSpeed(),
              "seconds\nTotal Detections:", self.detectionsCount(),
              "\nTotal Inferences:", self.inferencesCount(),
              "\n\n")

class DetectionResult():
    def __init__(self, model, label, precision, speed, device=None, source=None, frame=None, boundingBox=None, details=None):
        self.model = model
        self.label = label
        self.precision = precision.item() if torch.is_tensor(precision) else precision
        self.speed = speed.item() if torch.is_tensor(speed) else speed
        self.device = device
        self.source = source
        self.frame = frame
        self.boundingBox = boundingBox
        self.details = details

target_classes = ["truck", "bus", "car", "train", "bicycle"]


VEHICLE_DETECTION_MODELS = [
    DetectionModel("FCOS", "resnet50_fpn", fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights, "fcos_resnet", (255,0,0)),
    DetectionModel("RetinaNet", "resnet50_fpn", retinanet_resnet50_fpn, RetinaNet_ResNet50_FPN_Weights, "retinanet_resnet", (255,255,0)),
    DetectionModel("RetinaNet", "resnet50_fpn_v2", retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights, "retinanet_resnet_v2", (0,234,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn", fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights, "faster_rcnn_resnet", (170,0,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn_v2", fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights, "faster_rcnn_resnet_v2", (255,127,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_fpn", fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights, "faster_rcnn_mobilenet_v3", (191,255,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_320_fpn", fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights, "faster_rcnn_mobilenet_v3_320", (0,149,255)),
    DetectionModel("MaskRCNN", "resnet50_fpn", maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights, "mask_rcnn_resnet", (255,0,170)),
    DetectionModel("MaskRCNN", "resnet50_fpn_v2", maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights, "mask_rcnn_resnet_v2", (255,212,0)),
    DetectionModel("SSD300", "vgg16", ssd300_vgg16, SSD300_VGG16_Weights, "ssd_vgg16", (106,255,0)),
    DetectionModel("SSDLite320", "mobilenet_v3_large", ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights, "ssd_mobilenet_v3", (0,64,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5nu.pt', "yolov5nu", (237,185,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5su.pt', "yolov5su", (185,215,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5mu.pt', "yolov5mu", (231,233,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5lu.pt', "yolov5lu", (220,185,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5xu.pt', "yolov5xu", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5n6u.pt', "yolov5n6u", (143,35,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5s6u.pt', "yolov5s6u", (35,98,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5m6u.pt', "yolov5m6u", (143,106,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5l6u.pt', "yolov5l6u", (107,35,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5x6u.pt', "yolov5x6u", (79,143,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8n.pt', "yolov8n", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8s.pt', "yolov8s", (115,115,115)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8m.pt', "yolov8m", (204,204,204)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8l.pt', "yolov8l", (255,0,0)), # cycle restarts
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8x.pt', "yolov8x", (255,255,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9c.pt', "yolov9c", (0,234,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9e.pt', "yolov9e", (170,0,255))#,
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10n.pt', "yolov10n", (185,237,224)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10s.pt', "yolov10s", (115,115,115)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10m.pt', "yolov10m", (204,204,204)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10b.pt', "yolov10b", (204,204,204)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10l.pt', "yolov10l", (255,0,0)), # cycle restarts
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10x.pt', "yolov10x", (255,255,0))
]

def process(frame, frameIndex=-1, source=None):
    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        processed_frame = frame.copy()

        if currentModel.concept=="YOLO":
            start_time = time.time()
            results = currentModel.model(processed_frame, agnostic_nms=True, verbose=False, device=device)[0]
            processing_time = time.time() - start_time
            for result in results:
                detection_count = result.boxes.shape[0]
                for j in range(detection_count):
                    cls = int(result.boxes.cls[j].item())
                    name = result.names[cls]
                    confidence = float(result.boxes.conf[j].item())
                    bounding_box = result.boxes.xyxy[j].cpu().numpy()
                    if name in target_classes and confidence>0.5:
                        x1, y1, x2, y2 = [int(x) for x in bounding_box]
                        # Ensure x1 < x2 and y1 < y2
                        x1, x2 = min(x1, x2), max(x1, x2)
                        y1, y2 = min(y1, y2), max(y1, y2)
                        label = '{} ({:.2f}%) ({:.3f}s) {}'.format(name.upper(), confidence*100, processing_time, currentModel.reference)
                        print(label)
                        VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, name, confidence*100, processing_time, device, source, frameIndex, bounding_box, label))
                        cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                        cv2.putText(processed_frame, label, (x1+10, y1+25), 0, 0.8, currentModel.color, 2)
        else:
            # Convert frame to RGB and PIL Image, then apply transformation
            frame_rgb = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            image = transform(pil_image).unsqueeze(0).to(device)

            # Object detection
            start_time = time.time()
            with torch.no_grad():
                predictions = currentModel.model(image)

            # Processing time
            processing_time = time.time() - start_time

            # Visualization
            scores = predictions[0]['scores']
            boxes = predictions[0]['boxes']
            labels = predictions[0]['labels']
            confidence_threshold = 0.5

            for score, box, label in zip(scores, boxes, labels):
                # and score > confidence_threshold
                if currentModel.weights.meta["categories"][label.item()] in target_classes and score > confidence_threshold:
                    x1, y1, x2, y2 = map(int, box)
                    label_text = f'{currentModel.weights.meta["categories"][label.item()].upper()} ({score*100:.2f}%) ({processing_time:.3f}s) {currentModel.reference}'
                    print(label_text)
                    VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, currentModel.weights.meta["categories"][label.item()], float(score.item())*100, processing_time, device, source, frameIndex, box, label_text))
                    cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                    cv2.putText(processed_frame, label_text, (x1, y1+20), 0, 0.8, currentModel.color, 2)

        img = Image.fromarray(processed_frame, 'RGB')
        display(img)
        clear_output(wait=True)
    return frame

def handleFrame(frame, frameIndex=-1, source=None):
    processed_frame = process(frame,frameIndex,source)

def handleRelease():
    cam.release()
    print("Source released.")

    print(device)
    print(source)

    BestPrecision=None
    BestAvgPrecision=None
    WorstPrecision=None
    BestSpeed=None
    BestAvgSpeed=None
    WorstSpeed=None

    Models=[]
    BestPrecisions=[]
    AvgPrecisions=[]
    WorstPrecisions=[]
    BestSpeeds=[]
    AvgSpeeds=[]
    WorstSpeeds=[]
    BestInferenceSpeeds=[]
    AvgInferenceSpeeds=[]
    WorstInferenceSpeeds=[]
    TotalDetectionsArr=[]
    TotalInferencesArr=[]

    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        # currentModel.printResults()

        Models.append(currentModel.reference)
        BestPrecisions.append(currentModel.getMaxPrecision())
        AvgPrecisions.append(currentModel.getAveragePrecision())
        WorstPrecisions.append(currentModel.getMinPrecision())
        BestSpeeds.append(currentModel.getMinSpeed())
        AvgSpeeds.append(currentModel.getAverageSpeed())
        WorstSpeeds.append(currentModel.getMaxSpeed())
        BestInferenceSpeeds.append(currentModel.getMinInferenceSpeed())
        AvgInferenceSpeeds.append(currentModel.getAverageInferenceSpeed())
        WorstInferenceSpeeds.append(currentModel.getMaxInferenceSpeed())
        TotalDetectionsArr.append(currentModel.detectionsCount())
        TotalInferencesArr.append(currentModel.inferencesCount())

        if BestPrecision is None or BestPrecision.getMaxPrecision()<=currentModel.getMaxPrecision():
            BestPrecision = currentModel
        if BestAvgPrecision is None or BestAvgPrecision.getAveragePrecision()<=currentModel.getAveragePrecision():
            BestAvgPrecision = currentModel
        if WorstPrecision is None or WorstPrecision.getMinPrecision()>=currentModel.getMinPrecision():
            WorstPrecision = currentModel
        if BestSpeed is None or BestSpeed.getMinSpeed()>=currentModel.getMinSpeed():
            BestSpeed = currentModel
        if BestAvgSpeed is None or BestAvgSpeed.getAverageSpeed()>=currentModel.getAverageSpeed():
            BestAvgSpeed = currentModel
        if WorstSpeed is None or WorstSpeed.getMaxSpeed()<=currentModel.getMaxSpeed():
            WorstSpeed = currentModel

    if BestPrecision is not None:
        print("Best Precision:")
        BestPrecision.printResults()
    if BestAvgPrecision is not None:
        print("Best Average Precision:")
        BestAvgPrecision.printResults()
    if WorstPrecision is not None:
        print("Worst Precision:")
        WorstPrecision.printResults()
    if BestSpeed is not None:
        print("Best Speed:")
        BestSpeed.printResults()
    if BestAvgSpeed is not None:
        print("Best Average Speed:")
        BestAvgSpeed.printResults()
    if WorstSpeed is not None:
        print("Worst Speed:")
        WorstSpeed.printResults()

    print("models=",Models)
    print("best_precisions=",BestPrecisions)
    print("avg_precisions=",AvgPrecisions)
    print("worst_precisions=",WorstPrecisions)
    print("best_speeds=",BestSpeeds)
    print("avg_speeds=",AvgSpeeds)
    print("worst_speeds=",WorstSpeeds)
    print("best_inference_speeds=",BestInferenceSpeeds)
    print("avg_inference_speeds=",AvgInferenceSpeeds)
    print("worst_inference_speeds=",WorstInferenceSpeeds)
    print("total_detections=",TotalDetectionsArr)
    print("total_inferences=",TotalInferencesArr)

inputType="video"
source = "data/video/vecteezy_car-and-truck-traffic-on-the-highway-in-europe-poland_7957364.MP4" if inputType=="video" else 0
cam = cv2.VideoCapture(source)
display_handle=display(None, display_id=True)
every = 60

start = 0 if source!=0 else -1
end = int(cam.get(cv2.CAP_PROP_FRAME_COUNT)) if source!=0 else -1
if source!=0: cam.set(1, start)

frameCount=start
while_safety=0
saved_count=0

if not cam.isOpened(): print("Error: Could not open source.")
else:
    try:
        while True if end<0 else frameCount<end:
            _, frame = cam.read()
            if frame is None:
                if source!=0:
                    if while_safety > 2000: break
                    while_safety += 1
                    continue
                else:
                    print("Error: Could not capture frame.")
                    cam.release()
                    break

            if every>0:
                if (frameCount+1)%math.floor(every) == 0:
                    while_safety = 0
                    handleFrame(frame, frameCount, source)
            else:
                while_safety=0
                handleFrame(frame)
            frameCount += 1
        handleRelease()
    except KeyboardInterrupt:
        handleRelease()
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Source could not be released.")
        except:
            pass
        traceback.print_exc()


models= ['fcos_resnet', 'retinanet_resnet', 'retinanet_resnet_v2', 'faster_rcnn_resnet', 'faster_rcnn_resnet_v2', 'faster_rcnn_mobilenet_v3', 'faster_rcnn_mobilenet_v3_320', 'mask_rcnn_resnet', 'mask_rcnn_resnet_v2', 'ssd_vgg16', 'ssd_mobilenet_v3', 'yolov5nu', 'yolov5su', 'yolov5mu', 'yolov5lu', 'yolov5xu', 'yolov5n6u', 'yolov5s6u', 'yolov5m6u', 'yolov5l6u', 'yolov5x6u', 'yolov8n', 'yolov8s', 'yolov8m', 'yolov8l', 'yolov8x', 'yolov9c', 'yolov9e']
best_precisions= [88.38133811950684, 94.31945085525513, 94.65104341506958, 99.81282353401184, 99.86346364021301, 99.15496110916138, 99.13085699081421, 99.87896084785461, 99.8472273349762, 94.76894736289978, 98.03175926208496, 91.97649955749512, 94.07821297645569, 96.37896418571472, 95.80461382865906, 97.32694029808044, 94.23967599868774, 95.54756283760071, 96.92941308021545, 96.77504301071167, 96.98180556297302, 95.45490741729736, 95.18426060676575, 96.5444028377533, 96.16448879241943, 96.99692130088806, 96.3855504989624, 97.04502820968628]
avg_precisions= [66.38103212629046, 70.31693950531975, 70.46555602872694, 79.88080137459806, 85.4470392210143, 78.6277437402356, 80.3541276190016, 81.81524723768234, 84.55544219017028, 75.39923849858735, 71.74323116030011, 66.32874078220792, 75.24835873733868, 75.41420954768941, 76.96107663805523, 77.8765459571566, 72.08149901458195, 76.98340443894267, 78.38425488882167, 79.01229052381082, 80.49516825265782, 70.95425857437982, 76.98350898597552, 76.53433014949162, 78.4970962640011, 79.66656745343968, 79.37961742281914, 79.35656789523452]
worst_precisions= [50.543564558029175, 50.14103651046753, 50.106269121170044, 50.60614347457886, 50.11531114578247, 50.00733137130737, 50.494617223739624, 50.68878531455994, 50.22873282432556, 54.57260012626648, 53.013479709625244, 50.18038749694824, 51.69910788536072, 50.16434192657471, 50.79196095466614, 51.13615393638611, 50.563931465148926, 52.833545207977295, 50.4055917263031, 50.28080940246582, 50.939661264419556, 51.90621018409729, 50.88534355163574, 52.156347036361694, 51.11284852027893, 53.60074043273926, 50.20449161529541, 50.40521025657654]
best_speeds= [0.6382386684417725, 0.5604538917541504, 0.8646144866943359, 0.9152219295501709, 2.0403692722320557, 0.10471200942993164, 0.05285477638244629, 1.6530146598815918, 1.243567705154419, 0.0897524356842041, 0.08277130126953125, 0.014960289001464844, 0.035897254943847656, 0.13063788414001465, 0.14061260223388672, 0.09772610664367676, 0.02691054344177246, 0.03589987754821777, 0.2024402618408203, 0.18349242210388184, 0.3310844898223877, 0.013963937759399414, 0.014955997467041016, 0.057839393615722656, 0.15058374404907227, 0.16953134536743164, 0.026927709579467773, 0.4427759647369385]
avg_speeds= [0.7040490553929255, 0.5952778922186958, 0.8776050322764629, 1.0019240675046461, 2.0755519475255695, 0.11690438178277784, 0.05861784793712475, 1.8473746348172426, 2.418345308303833, 0.14386658919484993, 0.11282556397574288, 0.055429928832583956, 0.10418752106753262, 0.21040175728878732, 0.22498212541852677, 0.3976292712347848, 0.055504598787852695, 0.12353333085775375, 0.33920290649578133, 0.374850256876512, 0.8691614853438511, 0.03864504893620809, 0.05987431173739226, 0.14293832778930665, 0.2583776026061087, 0.30908776366192364, 0.12951990630891588, 0.8330343054301703]
worst_speeds= [0.8636839389801025, 0.7075440883636475, 0.89253830909729, 1.0261685848236084, 2.0962164402008057, 0.12465786933898926, 0.06183362007141113, 1.9613065719604492, 3.5452349185943604, 0.2582881450653076, 0.1256554126739502, 0.16952729225158691, 0.40488147735595703, 0.5524759292602539, 0.556464672088623, 1.1219031810760498, 0.18349409103393555, 0.4377925395965576, 0.6811206340789795, 1.157801866531372, 2.9329679012298584, 0.12066435813903809, 0.2303621768951416, 0.5594537258148193, 0.7379612922668457, 0.6871013641357422, 0.5195643901824951, 2.041989803314209]
best_inference_speeds= [0.6382386684417725, 0.5604538917541504, 0.8646144866943359, 0.9152219295501709, 2.0403692722320557, 0.10471200942993164, 0.05285477638244629, 1.6530146598815918, 1.243567705154419, 0.0897524356842041, 0.08277130126953125, 0.014960289001464844, 0.035897254943847656, 0.13063788414001465, 0.14061260223388672, 0.09772610664367676, 0.02691054344177246, 0.03589987754821777, 0.2024402618408203, 0.18349242210388184, 0.3310844898223877, 0.013963937759399414, 0.014955997467041016, 0.057839393615722656, 0.15058374404907227, 0.16953134536743164, 0.026927709579467773, 0.4427759647369385]
avg_inference_speeds= [0.7038709163665772, 0.5960550785064698, 0.8773785591125488, 1.0011879920959472, 2.0748752117156983, 0.11707816123962403, 0.05804133415222168, 1.8465641498565675, 2.4143404960632324, 0.12525467872619628, 0.1110239823659261, 0.04786772727966308, 0.11089181900024414, 0.2162027359008789, 0.2285691261291504, 0.39131813049316405, 0.0588346004486084, 0.12764692306518555, 0.32729597091674806, 0.3813459396362305, 0.8580435276031494, 0.03630166053771973, 0.06023077964782715, 0.15955901145935059, 0.2690571784973145, 0.2937880039215088, 0.13283581733703614, 0.7957282543182373]
worst_inference_speeds= [0.8636839389801025, 0.7075440883636475, 0.89253830909729, 1.0261685848236084, 2.0962164402008057, 0.12465786933898926, 0.06183362007141113, 1.9613065719604492, 3.5452349185943604, 0.2582881450653076, 0.1256554126739502, 0.16952729225158691, 0.40488147735595703, 0.5524759292602539, 0.556464672088623, 1.1219031810760498, 0.18349409103393555, 0.4377925395965576, 0.6811206340789795, 1.157801866531372, 2.9329679012298584, 0.12066435813903809, 0.2303621768951416, 0.5594537258148193, 0.7379612922668457, 0.6871013641357422, 0.5195643901824951, 2.041989803314209]
total_detections= [91, 63, 74, 129, 140, 62, 27, 128, 125, 19, 7, 36, 44, 59, 63, 70, 56, 64, 93, 88, 93, 36, 46, 60, 66, 69, 72, 67]
total_inferences= [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]








import cv2
import torch
from torchvision.models.detection import *
from torchvision.io.image import read_image
from torchvision.utils import draw_bounding_boxes
from torchvision.transforms.functional import to_pil_image
import torchvision.transforms as transforms
from PIL import Image
import IPython
from IPython.display import display, clear_output
import time
import traceback
import math
from ultralytics import YOLO

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

transform = transforms.Compose([
    #transforms.Resize((300, 300)),
    transforms.ToTensor(),
])

class DetectionModel():
    def __init__(self, concept, backbone, model, weights, reference, color=None):
        self.concept = concept
        self.backbone = backbone
        self.model = model(weights) if concept=="YOLO" else model(weights=weights.DEFAULT)
        if concept!="YOLO":
            self.model.to(device)
            self.model.eval()
        self.weights = weights if concept=="YOLO" else weights.DEFAULT
        self.reference = reference
        self.color = color
        self.detections = []

    def addDetection(self, detection):
        self.detections.append(detection)

    def getDetections(self):
        return self.detections

    def clearDetections(self):
        self.detections = []

    def detectionsCount(self):
        return len(self.detections)

    def inferencesCount(self):
        length = 0
        addedFrames = []
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    addedFrames.append(self.detections[i].frame)
        return len(addedFrames)

    def getAveragePrecision(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].precision
            avg = sum/len(self.detections)
        return  avg.item() if torch.is_tensor(avg) else avg

    def getMaxPrecision(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].precision:
                max=self.detections[i].precision
        return max.item() if torch.is_tensor(max) else max

    def getMinPrecision(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].precision:
                min=self.detections[i].precision
        return 0.0 if min==9999999.99 else min.item() if torch.is_tensor(min) else min

    def getAverageSpeed(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].speed
            avg = sum/len(self.detections)
        return avg

    def getMaxSpeed(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].speed:
                max=self.detections[i].speed
        return max

    def getMinSpeed(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].speed:
                min=self.detections[i].speed
        return 0.0 if min==9999999.99 else min

    def getAverageInferenceSpeed(self):
        sum=0.0
        avg=0.0
        addedFrames=[]
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    sum+=self.detections[i].speed
                    addedFrames.append(self.detections[i].frame)
            avg = sum/len(addedFrames) if len(addedFrames)>0 else 0.0
        return avg

    def getMaxInferenceSpeed(self):
        max=0.0
        addedFrames=[]
        for i in range(len(self.detections)):
            if max<self.detections[i].speed and self.detections[i].frame not in addedFrames:
                max=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return max

    def getMinInferenceSpeed(self):
        min=9999999.99
        addedFrames=[]
        for i in range(len(self.detections)):
            if min>self.detections[i].speed and self.detections[i].frame not in addedFrames:
                min=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return 0.0 if min==9999999.99 else min

    def filterPrecisionsBelow(self, minPrecision):
        newDetections = []
        for i in range(len(self.detections)):
            if self.detections[i].precision>=minPrecision:
                newDetections.append(self.detections[i])
        self.detections = newDetections

    def printResults(self):
        print("Model:", self.reference,
              "\nBest Precision:", self.getMaxPrecision(),
              "%\nAverage Precision:", self.getAveragePrecision(),
              "%\nWorst Precision:", self.getMinPrecision(),
              "%\nBest Speed:", self.getMinSpeed(),
              "seconds\nAverage Speed:", self.getAverageSpeed(),
              "seconds\nWorst Speed:", self.getMaxSpeed(),
              "seconds\nBest Inference Speed:", self.getMinInferenceSpeed(),
              "seconds\nAverage Inference Speed:", self.getAverageInferenceSpeed(),
              "seconds\nWorst Inference Speed:", self.getMaxInferenceSpeed(),
              "seconds\nTotal Detections:", self.detectionsCount(),
              "\nTotal Inferences:", self.inferencesCount(),
              "\n\n")

class DetectionResult():
    def __init__(self, model, label, precision, speed, device=None, source=None, frame=None, boundingBox=None, details=None):
        self.model = model
        self.label = label
        self.precision = precision.item() if torch.is_tensor(precision) else precision
        self.speed = speed.item() if torch.is_tensor(speed) else speed
        self.device = device
        self.source = source
        self.frame = frame
        self.boundingBox = boundingBox
        self.details = details

target_classes = ["truck", "bus", "car", "train", "bicycle"]


VEHICLE_DETECTION_MODELS = [
    DetectionModel("FCOS", "resnet50_fpn", fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights, "fcos_resnet", (255,0,0)),
    DetectionModel("RetinaNet", "resnet50_fpn", retinanet_resnet50_fpn, RetinaNet_ResNet50_FPN_Weights, "retinanet_resnet", (255,255,0)),
    DetectionModel("RetinaNet", "resnet50_fpn_v2", retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights, "retinanet_resnet_v2", (0,234,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn", fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights, "faster_rcnn_resnet", (170,0,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn_v2", fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights, "faster_rcnn_resnet_v2", (255,127,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_fpn", fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights, "faster_rcnn_mobilenet_v3", (191,255,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_320_fpn", fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights, "faster_rcnn_mobilenet_v3_320", (0,149,255)),
    DetectionModel("MaskRCNN", "resnet50_fpn", maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights, "mask_rcnn_resnet", (255,0,170)),
    DetectionModel("MaskRCNN", "resnet50_fpn_v2", maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights, "mask_rcnn_resnet_v2", (255,212,0)),
    DetectionModel("SSD300", "vgg16", ssd300_vgg16, SSD300_VGG16_Weights, "ssd_vgg16", (106,255,0)),
    DetectionModel("SSDLite320", "mobilenet_v3_large", ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights, "ssd_mobilenet_v3", (0,64,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5nu.pt', "yolov5nu", (237,185,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5su.pt', "yolov5su", (185,215,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5mu.pt', "yolov5mu", (231,233,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5lu.pt', "yolov5lu", (220,185,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5xu.pt', "yolov5xu", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5n6u.pt', "yolov5n6u", (143,35,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5s6u.pt', "yolov5s6u", (35,98,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5m6u.pt', "yolov5m6u", (143,106,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5l6u.pt', "yolov5l6u", (107,35,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5x6u.pt', "yolov5x6u", (79,143,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8n.pt', "yolov8n", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8s.pt', "yolov8s", (115,115,115)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8m.pt', "yolov8m", (204,204,204)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8l.pt', "yolov8l", (255,0,0)), # cycle restarts
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8x.pt', "yolov8x", (255,255,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9c.pt', "yolov9c", (0,234,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9e.pt', "yolov9e", (170,0,255))#,
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10n.pt', "yolov10n", (185,237,224)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10s.pt', "yolov10s", (115,115,115)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10m.pt', "yolov10m", (204,204,204)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10b.pt', "yolov10b", (204,204,204)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10l.pt', "yolov10l", (255,0,0)), # cycle restarts
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10x.pt', "yolov10x", (255,255,0))
]

def process(frame, frameIndex=-1, source=None):
    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        processed_frame = frame.copy()

        if currentModel.concept=="YOLO":
            start_time = time.time()
            results = currentModel.model(processed_frame, agnostic_nms=True, verbose=False, device=device)[0]
            processing_time = time.time() - start_time
            for result in results:
                detection_count = result.boxes.shape[0]
                for j in range(detection_count):
                    cls = int(result.boxes.cls[j].item())
                    name = result.names[cls]
                    confidence = float(result.boxes.conf[j].item())
                    bounding_box = result.boxes.xyxy[j].cpu().numpy()
                    if name in target_classes and confidence>0.5:
                        x1, y1, x2, y2 = [int(x) for x in bounding_box]
                        # Ensure x1 < x2 and y1 < y2
                        x1, x2 = min(x1, x2), max(x1, x2)
                        y1, y2 = min(y1, y2), max(y1, y2)
                        label = '{} ({:.2f}%) ({:.3f}s) {}'.format(name.upper(), confidence*100, processing_time, currentModel.reference)
                        print(label)
                        VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, name, confidence*100, processing_time, device, source, frameIndex, bounding_box, label))
                        cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                        cv2.putText(processed_frame, label, (x1+10, y1+25), 0, 0.8, currentModel.color, 2)
        else:
            # Convert frame to RGB and PIL Image, then apply transformation
            frame_rgb = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            image = transform(pil_image).unsqueeze(0).to(device)

            # Object detection
            start_time = time.time()
            with torch.no_grad():
                predictions = currentModel.model(image)

            # Processing time
            processing_time = time.time() - start_time

            # Visualization
            scores = predictions[0]['scores']
            boxes = predictions[0]['boxes']
            labels = predictions[0]['labels']
            confidence_threshold = 0.5

            for score, box, label in zip(scores, boxes, labels):
                # and score > confidence_threshold
                if currentModel.weights.meta["categories"][label.item()] in target_classes and score > confidence_threshold:
                    x1, y1, x2, y2 = map(int, box)
                    label_text = f'{currentModel.weights.meta["categories"][label.item()].upper()} ({score*100:.2f}%) ({processing_time:.3f}s) {currentModel.reference}'
                    print(label_text)
                    VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, currentModel.weights.meta["categories"][label.item()], float(score.item())*100, processing_time, device, source, frameIndex, box, label_text))
                    cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                    cv2.putText(processed_frame, label_text, (x1, y1+20), 0, 0.8, currentModel.color, 2)

        img = Image.fromarray(processed_frame, 'RGB')
        display(img)
        clear_output(wait=True)
    return frame

def handleFrame(frame, frameIndex=-1, source=None):
    processed_frame = process(frame,frameIndex,source)

def handleRelease():
    cam.release()
    print("Source released.")

    print(device)
    print(source)

    BestPrecision=None
    BestAvgPrecision=None
    WorstPrecision=None
    BestSpeed=None
    BestAvgSpeed=None
    WorstSpeed=None

    Models=[]
    BestPrecisions=[]
    AvgPrecisions=[]
    WorstPrecisions=[]
    BestSpeeds=[]
    AvgSpeeds=[]
    WorstSpeeds=[]
    BestInferenceSpeeds=[]
    AvgInferenceSpeeds=[]
    WorstInferenceSpeeds=[]
    TotalDetectionsArr=[]
    TotalInferencesArr=[]

    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        # currentModel.printResults()

        Models.append(currentModel.reference)
        BestPrecisions.append(currentModel.getMaxPrecision())
        AvgPrecisions.append(currentModel.getAveragePrecision())
        WorstPrecisions.append(currentModel.getMinPrecision())
        BestSpeeds.append(currentModel.getMinSpeed())
        AvgSpeeds.append(currentModel.getAverageSpeed())
        WorstSpeeds.append(currentModel.getMaxSpeed())
        BestInferenceSpeeds.append(currentModel.getMinInferenceSpeed())
        AvgInferenceSpeeds.append(currentModel.getAverageInferenceSpeed())
        WorstInferenceSpeeds.append(currentModel.getMaxInferenceSpeed())
        TotalDetectionsArr.append(currentModel.detectionsCount())
        TotalInferencesArr.append(currentModel.inferencesCount())

        if BestPrecision is None or BestPrecision.getMaxPrecision()<=currentModel.getMaxPrecision():
            BestPrecision = currentModel
        if BestAvgPrecision is None or BestAvgPrecision.getAveragePrecision()<=currentModel.getAveragePrecision():
            BestAvgPrecision = currentModel
        if WorstPrecision is None or WorstPrecision.getMinPrecision()>=currentModel.getMinPrecision():
            WorstPrecision = currentModel
        if BestSpeed is None or BestSpeed.getMinSpeed()>=currentModel.getMinSpeed():
            BestSpeed = currentModel
        if BestAvgSpeed is None or BestAvgSpeed.getAverageSpeed()>=currentModel.getAverageSpeed():
            BestAvgSpeed = currentModel
        if WorstSpeed is None or WorstSpeed.getMaxSpeed()<=currentModel.getMaxSpeed():
            WorstSpeed = currentModel

    if BestPrecision is not None:
        print("Best Precision:")
        BestPrecision.printResults()
    if BestAvgPrecision is not None:
        print("Best Average Precision:")
        BestAvgPrecision.printResults()
    if WorstPrecision is not None:
        print("Worst Precision:")
        WorstPrecision.printResults()
    if BestSpeed is not None:
        print("Best Speed:")
        BestSpeed.printResults()
    if BestAvgSpeed is not None:
        print("Best Average Speed:")
        BestAvgSpeed.printResults()
    if WorstSpeed is not None:
        print("Worst Speed:")
        WorstSpeed.printResults()

    print("models=",Models)
    print("best_precisions=",BestPrecisions)
    print("avg_precisions=",AvgPrecisions)
    print("worst_precisions=",WorstPrecisions)
    print("best_speeds=",BestSpeeds)
    print("avg_speeds=",AvgSpeeds)
    print("worst_speeds=",WorstSpeeds)
    print("best_inference_speeds=",BestInferenceSpeeds)
    print("avg_inference_speeds=",AvgInferenceSpeeds)
    print("worst_inference_speeds=",WorstInferenceSpeeds)
    print("total_detections=",TotalDetectionsArr)
    print("total_inferences=",TotalInferencesArr)

inputType="video"
source = "data/video/2034115-hd_1920_1080_30fps.MP4" if inputType=="video" else 0
cam = cv2.VideoCapture(source)
display_handle=display(None, display_id=True)
every = 30

start = 0 if source!=0 else -1
end = int(cam.get(cv2.CAP_PROP_FRAME_COUNT)) if source!=0 else -1
if source!=0: cam.set(1, start)

frameCount=start
while_safety=0
saved_count=0

if not cam.isOpened(): print("Error: Could not open source.")
else:
    try:
        while True if end<0 else frameCount<end:
            _, frame = cam.read()
            if frame is None:
                if source!=0:
                    if while_safety > 2000: break
                    while_safety += 1
                    continue
                else:
                    print("Error: Could not capture frame.")
                    cam.release()
                    break

            if every>0:
                if (frameCount+1)%math.floor(every) == 0:
                    while_safety = 0
                    handleFrame(frame, frameCount, source)
            else:
                while_safety=0
                handleFrame(frame)
            frameCount += 1
        handleRelease()
    except KeyboardInterrupt:
        handleRelease()
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Source could not be released.")
        except:
            pass
        traceback.print_exc()


models= ['fcos_resnet', 'retinanet_resnet', 'retinanet_resnet_v2', 'faster_rcnn_resnet', 'faster_rcnn_resnet_v2', 'faster_rcnn_mobilenet_v3', 'faster_rcnn_mobilenet_v3_320', 'mask_rcnn_resnet', 'mask_rcnn_resnet_v2', 'ssd_vgg16', 'ssd_mobilenet_v3', 'yolov5nu', 'yolov5su', 'yolov5mu', 'yolov5lu', 'yolov5xu', 'yolov5n6u', 'yolov5s6u', 'yolov5m6u', 'yolov5l6u', 'yolov5x6u', 'yolov8n', 'yolov8s', 'yolov8m', 'yolov8l', 'yolov8x', 'yolov9c', 'yolov9e']
best_precisions= [84.83301997184753, 91.69307351112366, 95.4441785812378, 99.9228835105896, 99.9826729297638, 99.82061386108398, 99.8578667640686, 99.87086057662964, 99.93312358856201, 99.47913885116577, 99.56985712051392, 93.70522499084473, 94.43736672401428, 95.35672068595886, 95.55718898773193, 95.68715691566467, 96.31597399711609, 94.2226231098175, 95.38546800613403, 96.37797474861145, 96.30561470985413, 94.39494609832764, 95.28464078903198, 95.23571133613586, 95.44462561607361, 96.01415395736694, 96.25153541564941, 96.6651976108551]
avg_precisions= [65.8841269995485, 71.22555674401475, 73.83507868689948, 86.84455658679822, 88.96739099840386, 87.90522949378378, 85.20609168611334, 87.1236540860944, 89.62056282966856, 84.46967896789012, 84.35685243657841, 76.05521628554438, 77.53465430190165, 78.89073387700684, 81.06716857400052, 82.04506759622456, 79.49444311005729, 82.7447441394019, 83.24607770680349, 83.53853318534914, 84.0977500422729, 76.1033314355412, 81.14591127634048, 80.57522682865638, 81.92216429087493, 83.3882709881207, 83.31239580558741, 83.2613467378954]
worst_precisions= [50.04851818084717, 50.08883476257324, 50.05960464477539, 50.46467185020447, 50.51953196525574, 50.00492334365845, 50.13511776924133, 50.2067506313324, 51.425570249557495, 51.760417222976685, 50.511109828948975, 50.1340389251709, 50.268709659576416, 50.881582498550415, 50.4483699798584, 50.82172751426697, 50.26649832725525, 50.05990266799927, 52.43103504180908, 51.135653257369995, 50.0474750995636, 51.420825719833374, 51.97308659553528, 50.375568866729736, 50.06181001663208, 50.03392696380615, 52.08258628845215, 50.00579357147217]
best_speeds= [1.772261142730713, 1.9537744522094727, 1.918870210647583, 1.9697339534759521, 2.8184654712677, 0.6422829627990723, 0.13164830207824707, 2.6738526821136475, 3.420854091644287, 0.2792532444000244, 0.07679367065429688, 0.06382942199707031, 0.1107025146484375, 0.20844173431396484, 0.32512617111206055, 0.5654873847961426, 0.17752361297607422, 0.32811951637268066, 0.7160835266113281, 1.390282154083252, 2.091407537460327, 0.05984044075012207, 0.1216733455657959, 0.2293853759765625, 0.39095091819763184, 0.5146214962005615, 0.33410167694091797, 0.6871645450592041]
avg_speeds= [1.8611055646623884, 2.1093088199140686, 2.0133943569038717, 2.0991812836765047, 3.001668084751476, 0.6696288012639647, 0.14383380511808677, 2.8026075092228977, 3.6657400320446683, 0.3127592597344909, 0.08797798618193596, 0.08088694323956126, 0.12492477521300316, 0.2365391643679872, 0.4078424220861391, 0.6724827205185342, 0.20579915245374045, 0.38959266786618085, 0.8293435707752839, 1.5117045005813974, 2.480863849488262, 0.07674876207150287, 0.1433800506591797, 0.26005130712226904, 0.448448278882482, 0.6398735121563748, 0.3833047025214692, 0.7641591466633619]
worst_speeds= [1.931828260421753, 2.2110888957977295, 2.1602251529693604, 2.2320315837860107, 3.1176652908325195, 0.6971096992492676, 0.16954708099365234, 2.9451253414154053, 3.8646697998046875, 0.34108829498291016, 0.14760518074035645, 0.19248414039611816, 0.24534392356872559, 0.5265905857086182, 0.9893538951873779, 1.7592954635620117, 0.28024983406066895, 0.6003937721252441, 1.266613483428955, 2.3586935997009277, 4.219716548919678, 0.12665581703186035, 0.29421162605285645, 0.5575101375579834, 0.8966021537780762, 1.3693382740020752, 0.6951432228088379, 1.4690711498260498]
best_inference_speeds= [1.772261142730713, 1.9537744522094727, 1.918870210647583, 1.9697339534759521, 2.8184654712677, 0.6422829627990723, 0.13164830207824707, 2.6738526821136475, 3.420854091644287, 0.2792532444000244, 0.07679367065429688, 0.06382942199707031, 0.1107025146484375, 0.20844173431396484, 0.32512617111206055, 0.5654873847961426, 0.17752361297607422, 0.32811951637268066, 0.7160835266113281, 1.390282154083252, 2.091407537460327, 0.05984044075012207, 0.1216733455657959, 0.2293853759765625, 0.39095091819763184, 0.5146214962005615, 0.33410167694091797, 0.6871645450592041]
avg_inference_speeds= [1.8619220171655928, 2.11074926171984, 2.0134740471839905, 2.0977853621755327, 2.9982701199395314, 0.6687822086470467, 0.143544180052621, 2.7996576002665927, 3.6666622247014726, 0.3124497617994036, 0.08851132222584315, 0.08095976284572057, 0.12580485003335135, 0.23451398951666697, 0.4074086972645351, 0.6756554331098285, 0.20448718752179826, 0.38885081665856497, 0.8303138613700867, 1.51366354737963, 2.4818632432392667, 0.07704149825232369, 0.1415484803063529, 0.2601592540740967, 0.4479438747678484, 0.6399661387716021, 0.38283136061259676, 0.7678030133247375]
worst_inference_speeds= [1.931828260421753, 2.2110888957977295, 2.1602251529693604, 2.2320315837860107, 3.1176652908325195, 0.6971096992492676, 0.16954708099365234, 2.9451253414154053, 3.8646697998046875, 0.34108829498291016, 0.14760518074035645, 0.19248414039611816, 0.24534392356872559, 0.5265905857086182, 0.9893538951873779, 1.7592954635620117, 0.28024983406066895, 0.6003937721252441, 1.266613483428955, 2.3586935997009277, 4.219716548919678, 0.12665581703186035, 0.29421162605285645, 0.5575101375579834, 0.8966021537780762, 1.3693382740020752, 0.6951432228088379, 1.4690711498260498]
total_detections= [280, 233, 211, 299, 319, 233, 169, 308, 315, 99, 93, 142, 192, 196, 215, 226, 168, 223, 231, 244, 239, 161, 200, 223, 222, 222, 217, 226]
total_inferences= [28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28]


import cv2
import torch
from torchvision.models.detection import *
from torchvision.io.image import read_image
from torchvision.utils import draw_bounding_boxes
from torchvision.transforms.functional import to_pil_image
import torchvision.transforms as transforms
from PIL import Image
import IPython
from IPython.display import display, clear_output
import time
import traceback
import math
from ultralytics import YOLO

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

transform = transforms.Compose([
    #transforms.Resize((300, 300)),
    transforms.ToTensor(),
])

class DetectionModel():
    def __init__(self, concept, backbone, model, weights, reference, color=None):
        self.concept = concept
        self.backbone = backbone
        self.model = model(weights) if concept=="YOLO" else model(weights=weights.DEFAULT)
        if concept!="YOLO":
            self.model.to(device)
            self.model.eval()
        self.weights = weights if concept=="YOLO" else weights.DEFAULT
        self.reference = reference
        self.color = color
        self.detections = []

    def addDetection(self, detection):
        self.detections.append(detection)

    def getDetections(self):
        return self.detections

    def clearDetections(self):
        self.detections = []

    def detectionsCount(self):
        return len(self.detections)

    def inferencesCount(self):
        length = 0
        addedFrames = []
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    addedFrames.append(self.detections[i].frame)
        return len(addedFrames)

    def getAveragePrecision(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].precision
            avg = sum/len(self.detections)
        return  avg.item() if torch.is_tensor(avg) else avg

    def getMaxPrecision(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].precision:
                max=self.detections[i].precision
        return max.item() if torch.is_tensor(max) else max

    def getMinPrecision(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].precision:
                min=self.detections[i].precision
        return 0.0 if min==9999999.99 else min.item() if torch.is_tensor(min) else min

    def getAverageSpeed(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].speed
            avg = sum/len(self.detections)
        return avg

    def getMaxSpeed(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].speed:
                max=self.detections[i].speed
        return max

    def getMinSpeed(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].speed:
                min=self.detections[i].speed
        return 0.0 if min==9999999.99 else min

    def getAverageInferenceSpeed(self):
        sum=0.0
        avg=0.0
        addedFrames=[]
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    sum+=self.detections[i].speed
                    addedFrames.append(self.detections[i].frame)
            avg = sum/len(addedFrames) if len(addedFrames)>0 else 0.0
        return avg

    def getMaxInferenceSpeed(self):
        max=0.0
        addedFrames=[]
        for i in range(len(self.detections)):
            if max<self.detections[i].speed and self.detections[i].frame not in addedFrames:
                max=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return max

    def getMinInferenceSpeed(self):
        min=9999999.99
        addedFrames=[]
        for i in range(len(self.detections)):
            if min>self.detections[i].speed and self.detections[i].frame not in addedFrames:
                min=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return 0.0 if min==9999999.99 else min

    def filterPrecisionsBelow(self, minPrecision):
        newDetections = []
        for i in range(len(self.detections)):
            if self.detections[i].precision>=minPrecision:
                newDetections.append(self.detections[i])
        self.detections = newDetections

    def printResults(self):
        print("Model:", self.reference,
              "\nBest Precision:", self.getMaxPrecision(),
              "%\nAverage Precision:", self.getAveragePrecision(),
              "%\nWorst Precision:", self.getMinPrecision(),
              "%\nBest Speed:", self.getMinSpeed(),
              "seconds\nAverage Speed:", self.getAverageSpeed(),
              "seconds\nWorst Speed:", self.getMaxSpeed(),
              "seconds\nBest Inference Speed:", self.getMinInferenceSpeed(),
              "seconds\nAverage Inference Speed:", self.getAverageInferenceSpeed(),
              "seconds\nWorst Inference Speed:", self.getMaxInferenceSpeed(),
              "seconds\nTotal Detections:", self.detectionsCount(),
              "\nTotal Inferences:", self.inferencesCount(),
              "\n\n")

class DetectionResult():
    def __init__(self, model, label, precision, speed, device=None, source=None, frame=None, boundingBox=None, details=None):
        self.model = model
        self.label = label
        self.precision = precision.item() if torch.is_tensor(precision) else precision
        self.speed = speed.item() if torch.is_tensor(speed) else speed
        self.device = device
        self.source = source
        self.frame = frame
        self.boundingBox = boundingBox
        self.details = details

target_classes = ["truck", "bus", "car", "train", "bicycle"]


VEHICLE_DETECTION_MODELS = [
    DetectionModel("FCOS", "resnet50_fpn", fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights, "fcos_resnet", (255,0,0)),
    DetectionModel("RetinaNet", "resnet50_fpn", retinanet_resnet50_fpn, RetinaNet_ResNet50_FPN_Weights, "retinanet_resnet", (255,255,0)),
    DetectionModel("RetinaNet", "resnet50_fpn_v2", retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights, "retinanet_resnet_v2", (0,234,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn", fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights, "faster_rcnn_resnet", (170,0,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn_v2", fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights, "faster_rcnn_resnet_v2", (255,127,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_fpn", fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights, "faster_rcnn_mobilenet_v3", (191,255,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_320_fpn", fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights, "faster_rcnn_mobilenet_v3_320", (0,149,255)),
    DetectionModel("MaskRCNN", "resnet50_fpn", maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights, "mask_rcnn_resnet", (255,0,170)),
    DetectionModel("MaskRCNN", "resnet50_fpn_v2", maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights, "mask_rcnn_resnet_v2", (255,212,0)),
    DetectionModel("SSD300", "vgg16", ssd300_vgg16, SSD300_VGG16_Weights, "ssd_vgg16", (106,255,0)),
    DetectionModel("SSDLite320", "mobilenet_v3_large", ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights, "ssd_mobilenet_v3", (0,64,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5nu.pt', "yolov5nu", (237,185,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5su.pt', "yolov5su", (185,215,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5mu.pt', "yolov5mu", (231,233,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5lu.pt', "yolov5lu", (220,185,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5xu.pt', "yolov5xu", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5n6u.pt', "yolov5n6u", (143,35,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5s6u.pt', "yolov5s6u", (35,98,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5m6u.pt', "yolov5m6u", (143,106,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5l6u.pt', "yolov5l6u", (107,35,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5x6u.pt', "yolov5x6u", (79,143,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8n.pt', "yolov8n", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8s.pt', "yolov8s", (115,115,115)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8m.pt', "yolov8m", (204,204,204)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8l.pt', "yolov8l", (255,0,0)), # cycle restarts
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8x.pt', "yolov8x", (255,255,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9c.pt', "yolov9c", (0,234,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9e.pt', "yolov9e", (170,0,255))#,
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10n.pt', "yolov10n", (185,237,224)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10s.pt', "yolov10s", (115,115,115)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10m.pt', "yolov10m", (204,204,204)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10b.pt', "yolov10b", (204,204,204)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10l.pt', "yolov10l", (255,0,0)), # cycle restarts
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10x.pt', "yolov10x", (255,255,0))
]

def process(frame, frameIndex=-1, source=None):
    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        processed_frame = frame.copy()

        if currentModel.concept=="YOLO":
            start_time = time.time()
            results = currentModel.model(processed_frame, agnostic_nms=True, verbose=False, device=device)[0]
            processing_time = time.time() - start_time
            for result in results:
                detection_count = result.boxes.shape[0]
                for j in range(detection_count):
                    cls = int(result.boxes.cls[j].item())
                    name = result.names[cls]
                    confidence = float(result.boxes.conf[j].item())
                    bounding_box = result.boxes.xyxy[j].cpu().numpy()
                    if name in target_classes and confidence>0.5:
                        x1, y1, x2, y2 = [int(x) for x in bounding_box]
                        # Ensure x1 < x2 and y1 < y2
                        x1, x2 = min(x1, x2), max(x1, x2)
                        y1, y2 = min(y1, y2), max(y1, y2)
                        label = '{} ({:.2f}%) ({:.3f}s) {}'.format(name.upper(), confidence*100, processing_time, currentModel.reference)
                        print(label)
                        VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, name, confidence*100, processing_time, device, source, frameIndex, bounding_box, label))
                        cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                        cv2.putText(processed_frame, label, (x1+10, y1+25), 0, 0.8, currentModel.color, 2)
        else:
            # Convert frame to RGB and PIL Image, then apply transformation
            frame_rgb = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            image = transform(pil_image).unsqueeze(0).to(device)

            # Object detection
            start_time = time.time()
            with torch.no_grad():
                predictions = currentModel.model(image)

            # Processing time
            processing_time = time.time() - start_time

            # Visualization
            scores = predictions[0]['scores']
            boxes = predictions[0]['boxes']
            labels = predictions[0]['labels']
            confidence_threshold = 0.5

            for score, box, label in zip(scores, boxes, labels):
                # and score > confidence_threshold
                if currentModel.weights.meta["categories"][label.item()] in target_classes and score > confidence_threshold:
                    x1, y1, x2, y2 = map(int, box)
                    label_text = f'{currentModel.weights.meta["categories"][label.item()].upper()} ({score*100:.2f}%) ({processing_time:.3f}s) {currentModel.reference}'
                    print(label_text)
                    VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, currentModel.weights.meta["categories"][label.item()], float(score.item())*100, processing_time, device, source, frameIndex, box, label_text))
                    cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                    cv2.putText(processed_frame, label_text, (x1, y1+20), 0, 0.8, currentModel.color, 2)

        img = Image.fromarray(processed_frame, 'RGB')
        display(img)
        clear_output(wait=True)
    return frame

def handleFrame(frame, frameIndex=-1, source=None):
    processed_frame = process(frame,frameIndex,source)

def handleRelease():
    cam.release()
    print("Source released.")

    print(device)
    print(source)

    BestPrecision=None
    BestAvgPrecision=None
    WorstPrecision=None
    BestSpeed=None
    BestAvgSpeed=None
    WorstSpeed=None

    Models=[]
    BestPrecisions=[]
    AvgPrecisions=[]
    WorstPrecisions=[]
    BestSpeeds=[]
    AvgSpeeds=[]
    WorstSpeeds=[]
    BestInferenceSpeeds=[]
    AvgInferenceSpeeds=[]
    WorstInferenceSpeeds=[]
    TotalDetectionsArr=[]
    TotalInferencesArr=[]

    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        # currentModel.printResults()

        Models.append(currentModel.reference)
        BestPrecisions.append(currentModel.getMaxPrecision())
        AvgPrecisions.append(currentModel.getAveragePrecision())
        WorstPrecisions.append(currentModel.getMinPrecision())
        BestSpeeds.append(currentModel.getMinSpeed())
        AvgSpeeds.append(currentModel.getAverageSpeed())
        WorstSpeeds.append(currentModel.getMaxSpeed())
        BestInferenceSpeeds.append(currentModel.getMinInferenceSpeed())
        AvgInferenceSpeeds.append(currentModel.getAverageInferenceSpeed())
        WorstInferenceSpeeds.append(currentModel.getMaxInferenceSpeed())
        TotalDetectionsArr.append(currentModel.detectionsCount())
        TotalInferencesArr.append(currentModel.inferencesCount())

        if BestPrecision is None or BestPrecision.getMaxPrecision()<=currentModel.getMaxPrecision():
            BestPrecision = currentModel
        if BestAvgPrecision is None or BestAvgPrecision.getAveragePrecision()<=currentModel.getAveragePrecision():
            BestAvgPrecision = currentModel
        if WorstPrecision is None or WorstPrecision.getMinPrecision()>=currentModel.getMinPrecision():
            WorstPrecision = currentModel
        if BestSpeed is None or BestSpeed.getMinSpeed()>=currentModel.getMinSpeed():
            BestSpeed = currentModel
        if BestAvgSpeed is None or BestAvgSpeed.getAverageSpeed()>=currentModel.getAverageSpeed():
            BestAvgSpeed = currentModel
        if WorstSpeed is None or WorstSpeed.getMaxSpeed()<=currentModel.getMaxSpeed():
            WorstSpeed = currentModel

    if BestPrecision is not None:
        print("Best Precision:")
        BestPrecision.printResults()
    if BestAvgPrecision is not None:
        print("Best Average Precision:")
        BestAvgPrecision.printResults()
    if WorstPrecision is not None:
        print("Worst Precision:")
        WorstPrecision.printResults()
    if BestSpeed is not None:
        print("Best Speed:")
        BestSpeed.printResults()
    if BestAvgSpeed is not None:
        print("Best Average Speed:")
        BestAvgSpeed.printResults()
    if WorstSpeed is not None:
        print("Worst Speed:")
        WorstSpeed.printResults()

    print("models=",Models)
    print("best_precisions=",BestPrecisions)
    print("avg_precisions=",AvgPrecisions)
    print("worst_precisions=",WorstPrecisions)
    print("best_speeds=",BestSpeeds)
    print("avg_speeds=",AvgSpeeds)
    print("worst_speeds=",WorstSpeeds)
    print("best_inference_speeds=",BestInferenceSpeeds)
    print("avg_inference_speeds=",AvgInferenceSpeeds)
    print("worst_inference_speeds=",WorstInferenceSpeeds)
    print("total_detections=",TotalDetectionsArr)
    print("total_inferences=",TotalInferencesArr)

inputType="video"
source = "data/video/2034115-hd_1920_1080_30fps.MP4" if inputType=="video" else 0
cam = cv2.VideoCapture(source)
display_handle=display(None, display_id=True)
every = 60

start = 0 if source!=0 else -1
end = int(cam.get(cv2.CAP_PROP_FRAME_COUNT)) if source!=0 else -1
if source!=0: cam.set(1, start)

frameCount=start
while_safety=0
saved_count=0

if not cam.isOpened(): print("Error: Could not open source.")
else:
    try:
        while True if end<0 else frameCount<end:
            _, frame = cam.read()
            if frame is None:
                if source!=0:
                    if while_safety > 2000: break
                    while_safety += 1
                    continue
                else:
                    print("Error: Could not capture frame.")
                    cam.release()
                    break

            if every>0:
                if (frameCount+1)%math.floor(every) == 0:
                    while_safety = 0
                    handleFrame(frame, frameCount, source)
            else:
                while_safety=0
                handleFrame(frame)
            frameCount += 1
        handleRelease()
    except KeyboardInterrupt:
        handleRelease()
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Source could not be released.")
        except:
            pass
        traceback.print_exc()


models= ['fcos_resnet', 'retinanet_resnet', 'retinanet_resnet_v2', 'faster_rcnn_resnet', 'faster_rcnn_resnet_v2', 'faster_rcnn_mobilenet_v3', 'faster_rcnn_mobilenet_v3_320', 'mask_rcnn_resnet', 'mask_rcnn_resnet_v2', 'ssd_vgg16', 'ssd_mobilenet_v3', 'yolov5nu', 'yolov5su', 'yolov5mu', 'yolov5lu', 'yolov5xu', 'yolov5n6u', 'yolov5s6u', 'yolov5m6u', 'yolov5l6u', 'yolov5x6u', 'yolov8n', 'yolov8s', 'yolov8m', 'yolov8l', 'yolov8x', 'yolov9c', 'yolov9e']
best_precisions= [84.57725048065186, 91.69307351112366, 93.28371286392212, 99.9228835105896, 99.9826729297638, 99.74753260612488, 99.8578667640686, 99.87086057662964, 99.93312358856201, 99.46802258491516, 99.56985712051392, 93.70522499084473, 94.43736672401428, 95.35672068595886, 95.46077251434326, 95.68715691566467, 96.31597399711609, 93.82950067520142, 95.38546800613403, 96.13023400306702, 96.30561470985413, 93.9609169960022, 95.28464078903198, 95.23571133613586, 95.3204870223999, 95.91988325119019, 96.25153541564941, 96.36331796646118]
avg_precisions= [65.94902793566386, 70.84196634914564, 73.63909642295081, 86.4511575474835, 89.52314011512264, 88.47208885770095, 85.42203738028745, 86.61798575481811, 89.51740340822062, 84.00142624023113, 85.84064522454905, 76.09292977098106, 76.883238052067, 77.95032883427807, 81.49384682618299, 82.0728320425207, 79.44612061535871, 82.0345533435995, 82.69094741862753, 82.69783654173867, 83.25561424096425, 75.928024083008, 79.98889805090548, 79.56610859504768, 82.27724576437915, 83.48517846838337, 83.05709251651058, 82.0063928240224]
worst_precisions= [50.04851818084717, 50.08883476257324, 50.05960464477539, 50.50647258758545, 50.90797543525696, 50.00492334365845, 50.13511776924133, 50.2067506313324, 52.372658252716064, 57.18659162521362, 51.46986246109009, 50.1340389251709, 50.268709659576416, 50.881582498550415, 50.4483699798584, 51.38620734214783, 50.26649832725525, 50.05990266799927, 52.771931886672974, 51.135653257369995, 50.39917230606079, 51.420825719833374, 51.97308659553528, 50.81787705421448, 50.06181001663208, 50.354957580566406, 52.26316452026367, 50.41883587837219]
best_speeds= [1.82411527633667, 1.8690025806427002, 1.8430724143981934, 2.0086302757263184, 2.858358383178711, 0.6352994441986084, 0.13164734840393066, 2.676841974258423, 3.4627461433410645, 0.2573120594024658, 0.08377528190612793, 0.07180547714233398, 0.11768484115600586, 0.22539639472961426, 0.39394640922546387, 0.6093688011169434, 0.18749690055847168, 0.38197827339172363, 0.7619602680206299, 1.3882880210876465, 2.3397445678710938, 0.06981325149536133, 0.12666106224060059, 0.24534320831298828, 0.3929479122161865, 0.6103677749633789, 0.3400893211364746, 0.6602320671081543]
avg_speeds= [1.8877215403097647, 2.1051621374876603, 2.0063451398717294, 2.1658735067252346, 3.0455029256882207, 0.6768122827797606, 0.15566713551440872, 2.835315530950373, 3.736828650638556, 0.3127592725956694, 0.09286761838336323, 0.09019196897313214, 0.13977240010311728, 0.2605491569361736, 0.45524729571296174, 0.7467114491896196, 0.21465918164194367, 0.41597724827853116, 0.8628216722737188, 1.5463071103955879, 2.6100546300411223, 0.08578153598455736, 0.15082620611094466, 0.28061433136463165, 0.47415672849725793, 0.6991476277324641, 0.40207201683962784, 0.7901404175842017]
worst_speeds= [2.167362689971924, 2.245987892150879, 2.1978378295898438, 2.3038411140441895, 3.191466808319092, 0.7270565032958984, 0.1795194149017334, 3.4298295974731445, 4.3114728927612305, 0.33809614181518555, 0.11369490623474121, 0.1825120449066162, 0.27725648880004883, 0.5744638442993164, 1.102052927017212, 1.8350930213928223, 0.31914591789245605, 0.6263225078582764, 1.3244588375091553, 2.5003137588500977, 4.241659879684448, 0.15957355499267578, 0.2832214832305908, 0.5674827098846436, 0.9335043430328369, 1.3842954635620117, 0.7180793285369873, 1.4610931873321533]
best_inference_speeds= [1.82411527633667, 1.8690025806427002, 1.8430724143981934, 2.0086302757263184, 2.858358383178711, 0.6352994441986084, 0.13164734840393066, 2.676841974258423, 3.4627461433410645, 0.2573120594024658, 0.08377528190612793, 0.07180547714233398, 0.11768484115600586, 0.22539639472961426, 0.39394640922546387, 0.6093688011169434, 0.18749690055847168, 0.38197827339172363, 0.7619602680206299, 1.3882880210876465, 2.3397445678710938, 0.06981325149536133, 0.12666106224060059, 0.24534320831298828, 0.3929479122161865, 0.6103677749633789, 0.3400893211364746, 0.6602320671081543]
avg_inference_speeds= [1.8930888175964355, 2.103660532406398, 2.0014136007853915, 2.166130474635533, 3.0476378543036327, 0.6762624297823224, 0.1561530658176967, 2.844965066228594, 3.737939272608076, 0.31252127034323557, 0.09332149369376046, 0.08890347821371895, 0.14026652063642228, 0.26108598709106445, 0.45827317237854004, 0.7449356658118111, 0.21492387567247664, 0.4182374136788504, 0.8703141212463379, 1.552633319582258, 2.6199947084699358, 0.08562678950173515, 0.15144937379019602, 0.2812465599605015, 0.47708020891462055, 0.6979892083576748, 0.4010694367544992, 0.7820505244391305]
worst_inference_speeds= [2.167362689971924, 2.245987892150879, 2.1978378295898438, 2.3038411140441895, 3.191466808319092, 0.7270565032958984, 0.1795194149017334, 3.4298295974731445, 4.3114728927612305, 0.33809614181518555, 0.11369490623474121, 0.1825120449066162, 0.27725648880004883, 0.5744638442993164, 1.102052927017212, 1.8350930213928223, 0.31914591789245605, 0.6263225078582764, 1.3244588375091553, 2.5003137588500977, 4.241659879684448, 0.15957355499267578, 0.2832214832305908, 0.5674827098846436, 0.9335043430328369, 1.3842954635620117, 0.7180793285369873, 1.4610931873321533]
total_detections= [135, 115, 101, 149, 155, 114, 83, 154, 157, 47, 43, 69, 95, 97, 103, 110, 81, 110, 115, 122, 120, 81, 99, 112, 108, 107, 108, 114]
total_inferences= [14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]





pip install ultralytics


from google.colab import drive
drive.mount('/data/video', force_remount=True)


pip install ultralytics


import cv2
import torch
from torchvision.models.detection import *
from torchvision.io.image import read_image
from torchvision.utils import draw_bounding_boxes
from torchvision.transforms.functional import to_pil_image
import torchvision.transforms as transforms
from PIL import Image
import IPython
from IPython.display import display, clear_output
import time
import traceback
import math
from ultralytics import YOLO

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

transform = transforms.Compose([
    #transforms.Resize((300, 300)),
    transforms.ToTensor(),
])

class DetectionModel():
    def __init__(self, concept, backbone, model, weights, reference, color=None):
        self.concept = concept
        self.backbone = backbone
        self.model = model(weights) if concept=="YOLO" else model(weights=weights.DEFAULT)
        if concept!="YOLO":
            self.model.to(device)
            self.model.eval()
        self.weights = weights if concept=="YOLO" else weights.DEFAULT
        self.reference = reference
        self.color = color
        self.detections = []

    def addDetection(self, detection):
        self.detections.append(detection)

    def getDetections(self):
        return self.detections

    def clearDetections(self):
        self.detections = []

    def detectionsCount(self):
        return len(self.detections)

    def inferencesCount(self):
        length = 0
        addedFrames = []
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    addedFrames.append(self.detections[i].frame)
        return len(addedFrames)

    def getAveragePrecision(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].precision
            avg = sum/len(self.detections)
        return  avg.item() if torch.is_tensor(avg) else avg

    def getMaxPrecision(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].precision:
                max=self.detections[i].precision
        return max.item() if torch.is_tensor(max) else max

    def getMinPrecision(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].precision:
                min=self.detections[i].precision
        return 0.0 if min==9999999.99 else min.item() if torch.is_tensor(min) else min

    def getAverageSpeed(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].speed
            avg = sum/len(self.detections)
        return avg

    def getMaxSpeed(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].speed:
                max=self.detections[i].speed
        return max

    def getMinSpeed(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].speed:
                min=self.detections[i].speed
        return 0.0 if min==9999999.99 else min

    def getAverageInferenceSpeed(self):
        sum=0.0
        avg=0.0
        addedFrames=[]
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    sum+=self.detections[i].speed
                    addedFrames.append(self.detections[i].frame)
            avg = sum/len(addedFrames) if len(addedFrames)>0 else 0.0
        return avg

    def getMaxInferenceSpeed(self):
        max=0.0
        addedFrames=[]
        for i in range(len(self.detections)):
            if max<self.detections[i].speed and self.detections[i].frame not in addedFrames:
                max=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return max

    def getMinInferenceSpeed(self):
        min=9999999.99
        addedFrames=[]
        for i in range(len(self.detections)):
            if min>self.detections[i].speed and self.detections[i].frame not in addedFrames:
                min=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return 0.0 if min==9999999.99 else min

    def filterPrecisionsBelow(self, minPrecision):
        newDetections = []
        for i in range(len(self.detections)):
            if self.detections[i].precision>=minPrecision:
                newDetections.append(self.detections[i])
        self.detections = newDetections

    def printResults(self):
        print("Model:", self.reference,
              "\nBest Precision:", self.getMaxPrecision(),
              "%\nAverage Precision:", self.getAveragePrecision(),
              "%\nWorst Precision:", self.getMinPrecision(),
              "%\nBest Speed:", self.getMinSpeed(),
              "seconds\nAverage Speed:", self.getAverageSpeed(),
              "seconds\nWorst Speed:", self.getMaxSpeed(),
              "seconds\nBest Inference Speed:", self.getMinInferenceSpeed(),
              "seconds\nAverage Inference Speed:", self.getAverageInferenceSpeed(),
              "seconds\nWorst Inference Speed:", self.getMaxInferenceSpeed(),
              "seconds\nTotal Detections:", self.detectionsCount(),
              "\nTotal Inferences:", self.inferencesCount(),
              "\n\n")

class DetectionResult():
    def __init__(self, model, label, precision, speed, device=None, source=None, frame=None, boundingBox=None, details=None):
        self.model = model
        self.label = label
        self.precision = precision.item() if torch.is_tensor(precision) else precision
        self.speed = speed.item() if torch.is_tensor(speed) else speed
        self.device = device
        self.source = source
        self.frame = frame
        self.boundingBox = boundingBox
        self.details = details

target_classes = ["truck", "bus", "car", "train", "bicycle"]


VEHICLE_DETECTION_MODELS = [
    DetectionModel("FCOS", "resnet50_fpn", fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights, "fcos_resnet", (255,0,0)),
    DetectionModel("RetinaNet", "resnet50_fpn", retinanet_resnet50_fpn, RetinaNet_ResNet50_FPN_Weights, "retinanet_resnet", (255,255,0)),
    DetectionModel("RetinaNet", "resnet50_fpn_v2", retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights, "retinanet_resnet_v2", (0,234,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn", fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights, "faster_rcnn_resnet", (170,0,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn_v2", fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights, "faster_rcnn_resnet_v2", (255,127,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_fpn", fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights, "faster_rcnn_mobilenet_v3", (191,255,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_320_fpn", fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights, "faster_rcnn_mobilenet_v3_320", (0,149,255)),
    DetectionModel("MaskRCNN", "resnet50_fpn", maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights, "mask_rcnn_resnet", (255,0,170)),
    DetectionModel("MaskRCNN", "resnet50_fpn_v2", maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights, "mask_rcnn_resnet_v2", (255,212,0)),
    DetectionModel("SSD300", "vgg16", ssd300_vgg16, SSD300_VGG16_Weights, "ssd_vgg16", (106,255,0)),
    DetectionModel("SSDLite320", "mobilenet_v3_large", ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights, "ssd_mobilenet_v3", (0,64,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5nu.pt', "yolov5nu", (237,185,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5su.pt', "yolov5su", (185,215,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5mu.pt', "yolov5mu", (231,233,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5lu.pt', "yolov5lu", (220,185,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5xu.pt', "yolov5xu", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5n6u.pt', "yolov5n6u", (143,35,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5s6u.pt', "yolov5s6u", (35,98,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5m6u.pt', "yolov5m6u", (143,106,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5l6u.pt', "yolov5l6u", (107,35,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5x6u.pt', "yolov5x6u", (79,143,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8n.pt', "yolov8n", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8s.pt', "yolov8s", (115,115,115)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8m.pt', "yolov8m", (204,204,204)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8l.pt', "yolov8l", (255,0,0)), # cycle restarts
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8x.pt', "yolov8x", (255,255,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9c.pt', "yolov9c", (0,234,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9e.pt', "yolov9e", (170,0,255))#,
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10n.pt', "yolov10n", (185,237,224)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10s.pt', "yolov10s", (115,115,115)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10m.pt', "yolov10m", (204,204,204)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10b.pt', "yolov10b", (204,204,204)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10l.pt', "yolov10l", (255,0,0)), # cycle restarts
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10x.pt', "yolov10x", (255,255,0))
]

def process(frame, frameIndex=-1, source=None):
    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        processed_frame = frame.copy()

        if currentModel.concept=="YOLO":
            start_time = time.time()
            results = currentModel.model(processed_frame, agnostic_nms=True, verbose=False, device=device)[0]
            processing_time = time.time() - start_time
            for result in results:
                detection_count = result.boxes.shape[0]
                for j in range(detection_count):
                    cls = int(result.boxes.cls[j].item())
                    name = result.names[cls]
                    confidence = float(result.boxes.conf[j].item())
                    bounding_box = result.boxes.xyxy[j].cpu().numpy()
                    if name in target_classes and confidence>0.5:
                        x1, y1, x2, y2 = [int(x) for x in bounding_box]
                        # Ensure x1 < x2 and y1 < y2
                        x1, x2 = min(x1, x2), max(x1, x2)
                        y1, y2 = min(y1, y2), max(y1, y2)
                        label = '{} ({:.2f}%) ({:.3f}s) {}'.format(name.upper(), confidence*100, processing_time, currentModel.reference)
                        print(label)
                        VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, name, confidence*100, processing_time, device, source, frameIndex, bounding_box, label))
                        cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                        cv2.putText(processed_frame, label, (x1+10, y1+25), 0, 0.8, currentModel.color, 2)
        else:
            # Convert frame to RGB and PIL Image, then apply transformation
            frame_rgb = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            image = transform(pil_image).unsqueeze(0).to(device)

            # Object detection
            start_time = time.time()
            with torch.no_grad():
                predictions = currentModel.model(image)

            # Processing time
            processing_time = time.time() - start_time

            # Visualization
            scores = predictions[0]['scores']
            boxes = predictions[0]['boxes']
            labels = predictions[0]['labels']
            confidence_threshold = 0.5

            for score, box, label in zip(scores, boxes, labels):
                # and score > confidence_threshold
                if currentModel.weights.meta["categories"][label.item()] in target_classes and score > confidence_threshold:
                    x1, y1, x2, y2 = map(int, box)
                    label_text = f'{currentModel.weights.meta["categories"][label.item()].upper()} ({score*100:.2f}%) ({processing_time:.3f}s) {currentModel.reference}'
                    print(label_text)
                    VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, currentModel.weights.meta["categories"][label.item()], float(score.item())*100, processing_time, device, source, frameIndex, box, label_text))
                    cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                    cv2.putText(processed_frame, label_text, (x1, y1+20), 0, 0.8, currentModel.color, 2)

        # img = Image.fromarray(processed_frame, 'RGB')
        # display(img)
        clear_output(wait=True)
    return frame

def handleFrame(frame, frameIndex=-1, source=None):
    processed_frame = process(frame,frameIndex,source)

def handleRelease():
    cam.release()
    print("Source released.")

    print(device)
    print(source)

    BestPrecision=None
    BestAvgPrecision=None
    WorstPrecision=None
    BestSpeed=None
    BestAvgSpeed=None
    WorstSpeed=None

    Models=[]
    BestPrecisions=[]
    AvgPrecisions=[]
    WorstPrecisions=[]
    BestSpeeds=[]
    AvgSpeeds=[]
    WorstSpeeds=[]
    BestInferenceSpeeds=[]
    AvgInferenceSpeeds=[]
    WorstInferenceSpeeds=[]
    TotalDetectionsArr=[]
    TotalInferencesArr=[]

    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        # currentModel.printResults()

        Models.append(currentModel.reference)
        BestPrecisions.append(currentModel.getMaxPrecision())
        AvgPrecisions.append(currentModel.getAveragePrecision())
        WorstPrecisions.append(currentModel.getMinPrecision())
        BestSpeeds.append(currentModel.getMinSpeed())
        AvgSpeeds.append(currentModel.getAverageSpeed())
        WorstSpeeds.append(currentModel.getMaxSpeed())
        BestInferenceSpeeds.append(currentModel.getMinInferenceSpeed())
        AvgInferenceSpeeds.append(currentModel.getAverageInferenceSpeed())
        WorstInferenceSpeeds.append(currentModel.getMaxInferenceSpeed())
        TotalDetectionsArr.append(currentModel.detectionsCount())
        TotalInferencesArr.append(currentModel.inferencesCount())

        if BestPrecision is None or BestPrecision.getMaxPrecision()<=currentModel.getMaxPrecision():
            BestPrecision = currentModel
        if BestAvgPrecision is None or BestAvgPrecision.getAveragePrecision()<=currentModel.getAveragePrecision():
            BestAvgPrecision = currentModel
        if WorstPrecision is None or WorstPrecision.getMinPrecision()>=currentModel.getMinPrecision():
            WorstPrecision = currentModel
        if BestSpeed is None or BestSpeed.getMinSpeed()>=currentModel.getMinSpeed():
            BestSpeed = currentModel
        if BestAvgSpeed is None or BestAvgSpeed.getAverageSpeed()>=currentModel.getAverageSpeed():
            BestAvgSpeed = currentModel
        if WorstSpeed is None or WorstSpeed.getMaxSpeed()<=currentModel.getMaxSpeed():
            WorstSpeed = currentModel

    if BestPrecision is not None:
        print("Best Precision:")
        BestPrecision.printResults()
    if BestAvgPrecision is not None:
        print("Best Average Precision:")
        BestAvgPrecision.printResults()
    if WorstPrecision is not None:
        print("Worst Precision:")
        WorstPrecision.printResults()
    if BestSpeed is not None:
        print("Best Speed:")
        BestSpeed.printResults()
    if BestAvgSpeed is not None:
        print("Best Average Speed:")
        BestAvgSpeed.printResults()
    if WorstSpeed is not None:
        print("Worst Speed:")
        WorstSpeed.printResults()

    print("models=",Models)
    print("best_precisions=",BestPrecisions)
    print("avg_precisions=",AvgPrecisions)
    print("worst_precisions=",WorstPrecisions)
    print("best_speeds=",BestSpeeds)
    print("avg_speeds=",AvgSpeeds)
    print("worst_speeds=",WorstSpeeds)
    print("best_inference_speeds=",BestInferenceSpeeds)
    print("avg_inference_speeds=",AvgInferenceSpeeds)
    print("worst_inference_speeds=",WorstInferenceSpeeds)
    print("total_detections=",TotalDetectionsArr)
    print("total_inferences=",TotalInferencesArr)

inputType="video"
source = "/data/video/MyDrive/sample.MP4" if inputType=="video" else 0
cam = cv2.VideoCapture(source)
display_handle=display(None, display_id=True)
every = 60

start = 0 if source!=0 else -1
end = int(cam.get(cv2.CAP_PROP_FRAME_COUNT)) if source!=0 else -1
if source!=0: cam.set(1, start)

frameCount=start
while_safety=0
saved_count=0

if not cam.isOpened(): print("Error: Could not open source.")
else:
    try:
        while True if end<0 else frameCount<end:
            _, frame = cam.read()
            if frame is None:
                if source!=0:
                    if while_safety > 2000: break
                    while_safety += 1
                    continue
                else:
                    print("Error: Could not capture frame.")
                    cam.release()
                    break

            if every>0:
                if (frameCount+1)%math.floor(every) == 0:
                    while_safety = 0
                    handleFrame(frame, frameCount, source)
            else:
                while_safety=0
                handleFrame(frame)
            frameCount += 1
        handleRelease()
    except KeyboardInterrupt:
        handleRelease()
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Source could not be released.")
        except:
            pass
        traceback.print_exc()


models= ['fcos_resnet', 'retinanet_resnet', 'retinanet_resnet_v2', 'faster_rcnn_resnet', 'faster_rcnn_resnet_v2', 'faster_rcnn_mobilenet_v3', 'faster_rcnn_mobilenet_v3_320', 'mask_rcnn_resnet', 'mask_rcnn_resnet_v2', 'ssd_vgg16', 'ssd_mobilenet_v3', 'yolov5nu', 'yolov5su', 'yolov5mu', 'yolov5lu', 'yolov5xu', 'yolov5n6u', 'yolov5s6u', 'yolov5m6u', 'yolov5l6u', 'yolov5x6u', 'yolov8n', 'yolov8s', 'yolov8m', 'yolov8l', 'yolov8x', 'yolov9c', 'yolov9e']
best_precisions= [84.01265740394592, 86.20252013206482, 96.11201882362366, 99.2476761341095, 99.60365891456604, 99.35745596885681, 98.9031732082367, 98.71225357055664, 99.41251277923584, 95.2492356300354, 97.8669285774231, 89.52351808547974, 94.16148066520691, 95.83455920219421, 95.45853734016418, 96.57518863677979, 89.92495536804199, 93.81579756736755, 96.59639000892639, 96.98560833930969, 96.82217240333557, 89.88815546035767, 96.51601314544678, 96.09537124633789, 96.96800708770752, 96.95228934288025, 95.79017162322998, 96.18934392929077]
avg_precisions= [65.36963979403178, 69.95759360569039, 76.43036914594245, 80.08687169321122, 80.30805315601226, 78.78401559941909, 78.06719532236457, 78.85641903473586, 78.57733654184929, 78.78552344908198, 78.83810534769175, 70.03259773437793, 74.7807652166445, 75.29445547004079, 79.59968329717715, 82.98176024109125, 72.61395219346167, 81.56108286943328, 79.96383485339936, 81.13267347216606, 83.55253285595349, 74.99833207734873, 79.01026375832096, 82.52469487087701, 84.47839196609414, 87.62051829017035, 82.93654416736804, 87.13674591582955]
worst_precisions= [50.05987286567688, 51.01221203804016, 50.575339794158936, 50.2974271774292, 50.01000165939331, 50.01301169395447, 52.073848247528076, 50.3439724445343, 50.29400587081909, 51.90426707267761, 51.95341110229492, 51.14137530326843, 51.496726274490356, 50.112175941467285, 50.28736591339111, 51.17032527923584, 51.04907751083374, 50.61156153678894, 50.99416971206665, 54.2685329914093, 51.08715891838074, 53.85284423828125, 50.046658515930176, 51.26423239707947, 53.032368421554565, 56.16891384124756, 52.2072970867157, 51.13374590873718]
best_speeds= [0.027570247650146484, 0.025523900985717773, 0.023625612258911133, 0.0258328914642334, 0.02673792839050293, 0.020676136016845703, 0.020448684692382812, 0.0344996452331543, 0.0364384651184082, 0.026411771774291992, 0.03598141670227051, 0.012332439422607422, 0.012055635452270508, 0.01430058479309082, 0.01625347137451172, 0.01838994026184082, 0.019496679306030273, 0.018715858459472656, 0.020995616912841797, 0.02437758445739746, 0.026850461959838867, 0.011270999908447266, 0.011161088943481445, 0.012923479080200195, 0.01503300666809082, 0.015572309494018555, 0.019713878631591797, 0.032340288162231445]
avg_speeds= [0.030402743539144825, 0.027985196752646536, 0.026274310217963323, 0.02812805227054063, 0.029763437297246228, 0.02219715714454651, 0.02179792709648609, 0.04143134752909342, 0.045246096018931314, 0.028882308178637402, 0.05761516337492028, 0.013713735800523024, 0.015914874534084373, 0.020238726638084233, 0.02472056696812312, 0.031504819790522255, 0.026443504951369594, 0.02122709724340546, 0.023836521875290645, 0.026575109788349698, 0.029893781457628523, 0.014780373640463387, 0.015191557586833994, 0.019045716972761258, 0.02316626258518385, 0.024907758920499595, 0.027054214477539064, 0.04462223608516953]
worst_speeds= [0.044843196868896484, 0.038454532623291016, 0.032283782958984375, 0.040503740310668945, 0.03454089164733887, 0.0330204963684082, 0.03421640396118164, 0.0670924186706543, 0.06231093406677246, 0.05208396911621094, 0.07115364074707031, 0.01728224754333496, 0.2001495361328125, 0.3749516010284424, 0.6622486114501953, 1.0648107528686523, 0.3646969795227051, 0.03447270393371582, 0.030077219009399414, 0.0326995849609375, 0.04141044616699219, 0.16252732276916504, 0.23115134239196777, 0.4065821170806885, 0.5713574886322021, 0.7857024669647217, 0.48970890045166016, 0.955810546875]
best_inference_speeds= [0.027570247650146484, 0.025523900985717773, 0.023625612258911133, 0.0258328914642334, 0.02673792839050293, 0.020676136016845703, 0.020448684692382812, 0.0344996452331543, 0.0364384651184082, 0.026411771774291992, 0.03598141670227051, 0.012332439422607422, 0.012055635452270508, 0.01430058479309082, 0.01625347137451172, 0.01838994026184082, 0.019496679306030273, 0.018715858459472656, 0.020995616912841797, 0.02437758445739746, 0.026850461959838867, 0.011270999908447266, 0.011161088943481445, 0.012923479080200195, 0.01503300666809082, 0.015572309494018555, 0.019713878631591797, 0.032340288162231445]
avg_inference_speeds= [0.030413624751998717, 0.027894203479473408, 0.026211848855018614, 0.02811739818159356, 0.02970679033370245, 0.022283582460312618, 0.021838868105853046, 0.04091263385046096, 0.04538238048553467, 0.028942945675972182, 0.057230415114437244, 0.013731407792600866, 0.016169441280080313, 0.020835802361771866, 0.025656297093346006, 0.03312332573391143, 0.026525563088016235, 0.021248820580934222, 0.023846285252631466, 0.02651485800743103, 0.029905325100745683, 0.01492286440151841, 0.015478654605586355, 0.019544710595923734, 0.0238335247499397, 0.026367584864298504, 0.02768383991150629, 0.04659086749667213]
worst_inference_speeds= [0.044843196868896484, 0.038454532623291016, 0.032283782958984375, 0.040503740310668945, 0.03454089164733887, 0.0330204963684082, 0.03421640396118164, 0.0670924186706543, 0.06231093406677246, 0.05208396911621094, 0.07115364074707031, 0.01728224754333496, 0.2001495361328125, 0.3749516010284424, 0.6622486114501953, 1.0648107528686523, 0.3646969795227051, 0.03447270393371582, 0.030077219009399414, 0.0326995849609375, 0.04141044616699219, 0.16252732276916504, 0.23115134239196777, 0.4065821170806885, 0.5713574886322021, 0.7857024669647217, 0.48970890045166016, 0.955810546875]
total_detections= [129, 97, 99, 186, 219, 136, 128, 189, 211, 83, 98, 78, 73, 86, 96, 96, 71, 89, 105, 112, 112, 71, 93, 93, 92, 101, 95, 103]
total_inferences= [83, 78, 80, 83, 84, 84, 81, 84, 84, 78, 83, 73, 67, 74, 84, 84, 69, 76, 79, 80, 81, 67, 82, 83, 83, 84, 84, 84]


import numpy as np
import matplotlib.pyplot as plt
import matplotlib

# Generate a color map
num_models = len(models)
cmap = matplotlib.colormaps.get_cmap('nipy_spectral')
colors = [cmap(i / num_models) for i in range(num_models)]

# Create a figure and plot with unique colors
plt.figure(figsize=(14, 10))
for i, model in enumerate(models):
    plt.scatter(avg_inference_speeds[i], avg_precisions[i], color=colors[i], label=model, edgecolor='black')

print("DEVICE: GPU (NVIDIA A100)")
print("SOURCE: data/video/sample.MP4 (1080p60fps Video)")
print("SKIP EVERY 60 FRAMES")
print("TOTALLY PROCESSED FRAMES: 84")
print("COCO Dataset, Classes truck, bus, car, train, bicycle")
print("ALL >50%")
# Axis labels and plot title
plt.xlabel('Average Speed (seconds)')
plt.ylabel('Average Precision (%)')
plt.title('Model Performance: Average Inference Speed x Average Accuracy')
plt.legend(loc='upper left', bbox_to_anchor=(1.05, 1), fontsize='small')
plt.grid(True)
plt.savefig("Analysis/Experiment1/A100/Experiment1a_Diagram.png", bbox_inches='tight')
plt.show()

# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_precisions, avg_precisions, worst_precisions])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Precision (%)", "Average Precision (%)", "Worst Precision (%)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

plt.title("Model Precision Metrics")
plt.savefig("Analysis/Experiment1/A100/Experiment1a_Precision_Table.png", bbox_inches='tight')
plt.show()


# # Set up the figure and axis for the table
# fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
# ax.axis('tight')
# ax.axis('off')

# # The table data: transpose the array to make each column a different metric
# table_data = np.transpose([models, best_speeds, avg_speeds, worst_speeds])

# # Create the table in the plot
# table = ax.table(cellText=table_data, colLabels=["Model", "Best Speed (s)", "Average Speed (s)", "Worst Speed (s)"],
#                  cellLoc='center', loc='center', colColours=["palegreen"] * 4)
# table.auto_set_font_size(False)
# table.set_fontsize(10)
# table.scale(1.2, 1.2)  # Scale table size

# plt.title("Model Speed Metrics")
# plt.show()

# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_inference_speeds, avg_inference_speeds, worst_inference_speeds])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Inference Speed (s)", "Average Inference Speed (s)", "Worst Inference Speed (s)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

plt.title("Model Inference Speed Metrics")
plt.savefig("Analysis/Experiment1/A100/Experiment1a_Time_Table.png", bbox_inches='tight')
plt.show()

data = []
for i in range(len(models)):
    data.append([models[i], f'{avg_precisions[i]:.2f}', f'{avg_inference_speeds[i]:.5f}'])

# LaTeX model names mapping
latex_model_names = {
    'fcos_resnet': 'FCOS ResNet50 FPN',
    'retinanet_resnet': 'RetinaNet ResNet50 FPN',
    'retinanet_resnet_v2': 'RetinaNet ResNet50 FPN V2',
    'faster_rcnn_resnet': 'Faster R-CNN ResNet50 FPN',
    'faster_rcnn_resnet_v2': 'Faster R-CNN ResNet50 FPN V2',
    'faster_rcnn_mobilenet_v3': 'Faster R-CNN MobileNet V3 L',
    'faster_rcnn_mobilenet_v3_320': 'Faster R-CNN MobileNet V3 L 320',
    'mask_rcnn_resnet': 'Mask R-CNN ResNet50',
    'mask_rcnn_resnet_v2': 'Mask R-CNN ResNet50 FPN V2',
    'ssd_vgg16': 'SSD VGG16',
    'ssd_mobilenet_v3': 'SSDLite MobileNet V3 Large',
    'yolov5nu': 'YOLOv5nu',
    'yolov5su': 'YOLOv5su',
    'yolov5mu': 'YOLOv5mu',
    'yolov5lu': 'YOLOv5lu',
    'yolov5xu': 'YOLOv5xu',
    'yolov5n6u': 'YOLOv5n6u',
    'yolov5s6u': 'YOLOv5s6u',
    'yolov5m6u': 'YOLOv5m6u',
    'yolov5l6u': 'YOLOv5l6u',
    'yolov5x6u': 'YOLOv5x6u',
    'yolov8n': 'YOLOv8n',
    'yolov8s': 'YOLOv8s',
    'yolov8m': 'YOLOv8m',
    'yolov8l': 'YOLOv8l',
    'yolov8x': 'YOLOv8x',
    'yolov9c': 'YOLOv9c',
    'yolov9e': 'YOLOv9e'
}

# Sort data by average precision in descending order
data_sorted = sorted(data, key=lambda x: float(x[1]), reverse=True)

# Generate LaTeX table
latex_table = "\\begin{table}[H]\n\\centering\n\\begin{tabular}{lrr}\n\\toprule\n"
latex_table += "\\multicolumn{3}{c}{Vehicle Detection Models Performance}\\\\ \\cmidrule{1-3}\n"
latex_table += "Model & Average Precision (\\%) & Average Inference Speed (s)\\\\\n\\midrule\n"

best_prec_index = 0
best_speed = [99999.99, 0]
for i in range(len(data_sorted)):
    if float(data_sorted[i][2])<best_speed[0]:
        best_speed = [float(data_sorted[i][2]), i]

count = 0
for entry in data_sorted:
    model = latex_model_names[entry[0]]
    avg_precision = float(entry[1])
    avg_speed = float(entry[2])

    if count==best_prec_index:
        if count==best_speed[1]:
            latex_table += f"\\textbf{{{model}}} & \\textbf{{{avg_precision:.2f}}} & \\textbf{{{avg_speed:.5f}}} \\\\ \\addlinespace\n"
        else:
            latex_table += f"\\textbf{{{model}}} & \\textbf{{{avg_precision:.2f}}} & {avg_speed:.5f} \\\\ \\addlinespace\n"
    else:
        if count==best_speed[1]:
            latex_table += f"\\textbf{{{model}}} & {avg_precision:.2f} & \\textbf{{{avg_speed:.5f}}} \\\\ \\addlinespace\n"
        else:
            latex_table += f"{model} & {avg_precision:.2f} & {avg_speed:.5f} \\\\ \\addlinespace\n"

    count+=1

latex_table += "\\bottomrule\n\\end{tabular}\n\\caption{Vehicle Detection Models Performance}\n\\label{table:vehicle_detection_performance}\n\\end{table}"

print(latex_table)


import cv2
import torch
from torchvision.models.detection import *
from torchvision.io.image import read_image
from torchvision.utils import draw_bounding_boxes
from torchvision.transforms.functional import to_pil_image
import torchvision.transforms as transforms
from PIL import Image
import IPython
from IPython.display import display, clear_output
import time
import traceback
import math
from ultralytics import YOLO

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

transform = transforms.Compose([
    #transforms.Resize((300, 300)),
    transforms.ToTensor(),
])

class DetectionModel():
    def __init__(self, concept, backbone, model, weights, reference, color=None):
        self.concept = concept
        self.backbone = backbone
        self.model = model(weights) if concept=="YOLO" else model(weights=weights.DEFAULT)
        if concept!="YOLO":
            self.model.to(device)
            self.model.eval()
        self.weights = weights if concept=="YOLO" else weights.DEFAULT
        self.reference = reference
        self.color = color
        self.detections = []

    def addDetection(self, detection):
        self.detections.append(detection)

    def getDetections(self):
        return self.detections

    def clearDetections(self):
        self.detections = []

    def detectionsCount(self):
        return len(self.detections)

    def inferencesCount(self):
        length = 0
        addedFrames = []
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    addedFrames.append(self.detections[i].frame)
        return len(addedFrames)

    def getAveragePrecision(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].precision
            avg = sum/len(self.detections)
        return  avg.item() if torch.is_tensor(avg) else avg

    def getMaxPrecision(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].precision:
                max=self.detections[i].precision
        return max.item() if torch.is_tensor(max) else max

    def getMinPrecision(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].precision:
                min=self.detections[i].precision
        return 0.0 if min==9999999.99 else min.item() if torch.is_tensor(min) else min

    def getAverageSpeed(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].speed
            avg = sum/len(self.detections)
        return avg

    def getMaxSpeed(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].speed:
                max=self.detections[i].speed
        return max

    def getMinSpeed(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].speed:
                min=self.detections[i].speed
        return 0.0 if min==9999999.99 else min

    def getAverageInferenceSpeed(self):
        sum=0.0
        avg=0.0
        addedFrames=[]
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    sum+=self.detections[i].speed
                    addedFrames.append(self.detections[i].frame)
            avg = sum/len(addedFrames) if len(addedFrames)>0 else 0.0
        return avg

    def getMaxInferenceSpeed(self):
        max=0.0
        addedFrames=[]
        for i in range(len(self.detections)):
            if max<self.detections[i].speed and self.detections[i].frame not in addedFrames:
                max=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return max

    def getMinInferenceSpeed(self):
        min=9999999.99
        addedFrames=[]
        for i in range(len(self.detections)):
            if min>self.detections[i].speed and self.detections[i].frame not in addedFrames:
                min=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return 0.0 if min==9999999.99 else min

    def filterPrecisionsBelow(self, minPrecision):
        newDetections = []
        for i in range(len(self.detections)):
            if self.detections[i].precision>=minPrecision:
                newDetections.append(self.detections[i])
        self.detections = newDetections

    def printResults(self):
        print("Model:", self.reference,
              "\nBest Precision:", self.getMaxPrecision(),
              "%\nAverage Precision:", self.getAveragePrecision(),
              "%\nWorst Precision:", self.getMinPrecision(),
              "%\nBest Speed:", self.getMinSpeed(),
              "seconds\nAverage Speed:", self.getAverageSpeed(),
              "seconds\nWorst Speed:", self.getMaxSpeed(),
              "seconds\nBest Inference Speed:", self.getMinInferenceSpeed(),
              "seconds\nAverage Inference Speed:", self.getAverageInferenceSpeed(),
              "seconds\nWorst Inference Speed:", self.getMaxInferenceSpeed(),
              "seconds\nTotal Detections:", self.detectionsCount(),
              "\nTotal Inferences:", self.inferencesCount(),
              "\n\n")

class DetectionResult():
    def __init__(self, model, label, precision, speed, device=None, source=None, frame=None, boundingBox=None, details=None):
        self.model = model
        self.label = label
        self.precision = precision.item() if torch.is_tensor(precision) else precision
        self.speed = speed.item() if torch.is_tensor(speed) else speed
        self.device = device
        self.source = source
        self.frame = frame
        self.boundingBox = boundingBox
        self.details = details

target_classes = ["truck", "bus", "car", "train", "bicycle"]


VEHICLE_DETECTION_MODELS = [
    DetectionModel("FCOS", "resnet50_fpn", fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights, "fcos_resnet", (255,0,0)),
    DetectionModel("RetinaNet", "resnet50_fpn", retinanet_resnet50_fpn, RetinaNet_ResNet50_FPN_Weights, "retinanet_resnet", (255,255,0)),
    DetectionModel("RetinaNet", "resnet50_fpn_v2", retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights, "retinanet_resnet_v2", (0,234,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn", fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights, "faster_rcnn_resnet", (170,0,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn_v2", fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights, "faster_rcnn_resnet_v2", (255,127,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_fpn", fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights, "faster_rcnn_mobilenet_v3", (191,255,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_320_fpn", fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights, "faster_rcnn_mobilenet_v3_320", (0,149,255)),
    DetectionModel("MaskRCNN", "resnet50_fpn", maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights, "mask_rcnn_resnet", (255,0,170)),
    DetectionModel("MaskRCNN", "resnet50_fpn_v2", maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights, "mask_rcnn_resnet_v2", (255,212,0)),
    DetectionModel("SSD300", "vgg16", ssd300_vgg16, SSD300_VGG16_Weights, "ssd_vgg16", (106,255,0)),
    DetectionModel("SSDLite320", "mobilenet_v3_large", ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights, "ssd_mobilenet_v3", (0,64,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5nu.pt', "yolov5nu", (237,185,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5su.pt', "yolov5su", (185,215,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5mu.pt', "yolov5mu", (231,233,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5lu.pt', "yolov5lu", (220,185,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5xu.pt', "yolov5xu", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5n6u.pt', "yolov5n6u", (143,35,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5s6u.pt', "yolov5s6u", (35,98,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5m6u.pt', "yolov5m6u", (143,106,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5l6u.pt', "yolov5l6u", (107,35,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5x6u.pt', "yolov5x6u", (79,143,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8n.pt', "yolov8n", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8s.pt', "yolov8s", (115,115,115)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8m.pt', "yolov8m", (204,204,204)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8l.pt', "yolov8l", (255,0,0)), # cycle restarts
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8x.pt', "yolov8x", (255,255,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9c.pt', "yolov9c", (0,234,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9e.pt', "yolov9e", (170,0,255))#,
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10n.pt', "yolov10n", (185,237,224)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10s.pt', "yolov10s", (115,115,115)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10m.pt', "yolov10m", (204,204,204)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10b.pt', "yolov10b", (204,204,204)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10l.pt', "yolov10l", (255,0,0)), # cycle restarts
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10x.pt', "yolov10x", (255,255,0))
]

def process(frame, frameIndex=-1, source=None):
    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        processed_frame = frame.copy()

        if currentModel.concept=="YOLO":
            start_time = time.time()
            results = currentModel.model(processed_frame, agnostic_nms=True, verbose=False, device=device)[0]
            processing_time = time.time() - start_time
            for result in results:
                detection_count = result.boxes.shape[0]
                for j in range(detection_count):
                    cls = int(result.boxes.cls[j].item())
                    name = result.names[cls]
                    confidence = float(result.boxes.conf[j].item())
                    bounding_box = result.boxes.xyxy[j].cpu().numpy()
                    if name in target_classes and confidence>0.5:
                        x1, y1, x2, y2 = [int(x) for x in bounding_box]
                        # Ensure x1 < x2 and y1 < y2
                        x1, x2 = min(x1, x2), max(x1, x2)
                        y1, y2 = min(y1, y2), max(y1, y2)
                        label = '{} ({:.2f}%) ({:.3f}s) {}'.format(name.upper(), confidence*100, processing_time, currentModel.reference)
                        print(label)
                        VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, name, confidence*100, processing_time, device, source, frameIndex, bounding_box, label))
                        cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                        cv2.putText(processed_frame, label, (x1+10, y1+25), 0, 0.8, currentModel.color, 2)
        else:
            # Convert frame to RGB and PIL Image, then apply transformation
            frame_rgb = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            image = transform(pil_image).unsqueeze(0).to(device)

            # Object detection
            start_time = time.time()
            with torch.no_grad():
                predictions = currentModel.model(image)

            # Processing time
            processing_time = time.time() - start_time

            # Visualization
            scores = predictions[0]['scores']
            boxes = predictions[0]['boxes']
            labels = predictions[0]['labels']
            confidence_threshold = 0.5

            for score, box, label in zip(scores, boxes, labels):
                # and score > confidence_threshold
                if currentModel.weights.meta["categories"][label.item()] in target_classes and score > confidence_threshold:
                    x1, y1, x2, y2 = map(int, box)
                    label_text = f'{currentModel.weights.meta["categories"][label.item()].upper()} ({score*100:.2f}%) ({processing_time:.3f}s) {currentModel.reference}'
                    print(label_text)
                    VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, currentModel.weights.meta["categories"][label.item()], float(score.item())*100, processing_time, device, source, frameIndex, box, label_text))
                    cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                    cv2.putText(processed_frame, label_text, (x1, y1+20), 0, 0.8, currentModel.color, 2)

        # img = Image.fromarray(processed_frame, 'RGB')
        # display(img)
        clear_output(wait=True)
    return frame

def handleFrame(frame, frameIndex=-1, source=None):
    processed_frame = process(frame,frameIndex,source)

def handleRelease():
    cam.release()
    print("Source released.")

    print(device)
    print(source)

    BestPrecision=None
    BestAvgPrecision=None
    WorstPrecision=None
    BestSpeed=None
    BestAvgSpeed=None
    WorstSpeed=None

    Models=[]
    BestPrecisions=[]
    AvgPrecisions=[]
    WorstPrecisions=[]
    BestSpeeds=[]
    AvgSpeeds=[]
    WorstSpeeds=[]
    BestInferenceSpeeds=[]
    AvgInferenceSpeeds=[]
    WorstInferenceSpeeds=[]
    TotalDetectionsArr=[]
    TotalInferencesArr=[]

    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        # currentModel.printResults()

        Models.append(currentModel.reference)
        BestPrecisions.append(currentModel.getMaxPrecision())
        AvgPrecisions.append(currentModel.getAveragePrecision())
        WorstPrecisions.append(currentModel.getMinPrecision())
        BestSpeeds.append(currentModel.getMinSpeed())
        AvgSpeeds.append(currentModel.getAverageSpeed())
        WorstSpeeds.append(currentModel.getMaxSpeed())
        BestInferenceSpeeds.append(currentModel.getMinInferenceSpeed())
        AvgInferenceSpeeds.append(currentModel.getAverageInferenceSpeed())
        WorstInferenceSpeeds.append(currentModel.getMaxInferenceSpeed())
        TotalDetectionsArr.append(currentModel.detectionsCount())
        TotalInferencesArr.append(currentModel.inferencesCount())

        if BestPrecision is None or BestPrecision.getMaxPrecision()<=currentModel.getMaxPrecision():
            BestPrecision = currentModel
        if BestAvgPrecision is None or BestAvgPrecision.getAveragePrecision()<=currentModel.getAveragePrecision():
            BestAvgPrecision = currentModel
        if WorstPrecision is None or WorstPrecision.getMinPrecision()>=currentModel.getMinPrecision():
            WorstPrecision = currentModel
        if BestSpeed is None or BestSpeed.getMinSpeed()>=currentModel.getMinSpeed():
            BestSpeed = currentModel
        if BestAvgSpeed is None or BestAvgSpeed.getAverageSpeed()>=currentModel.getAverageSpeed():
            BestAvgSpeed = currentModel
        if WorstSpeed is None or WorstSpeed.getMaxSpeed()<=currentModel.getMaxSpeed():
            WorstSpeed = currentModel

    if BestPrecision is not None:
        print("Best Precision:")
        BestPrecision.printResults()
    if BestAvgPrecision is not None:
        print("Best Average Precision:")
        BestAvgPrecision.printResults()
    if WorstPrecision is not None:
        print("Worst Precision:")
        WorstPrecision.printResults()
    if BestSpeed is not None:
        print("Best Speed:")
        BestSpeed.printResults()
    if BestAvgSpeed is not None:
        print("Best Average Speed:")
        BestAvgSpeed.printResults()
    if WorstSpeed is not None:
        print("Worst Speed:")
        WorstSpeed.printResults()

    print("models=",Models)
    print("best_precisions=",BestPrecisions)
    print("avg_precisions=",AvgPrecisions)
    print("worst_precisions=",WorstPrecisions)
    print("best_speeds=",BestSpeeds)
    print("avg_speeds=",AvgSpeeds)
    print("worst_speeds=",WorstSpeeds)
    print("best_inference_speeds=",BestInferenceSpeeds)
    print("avg_inference_speeds=",AvgInferenceSpeeds)
    print("worst_inference_speeds=",WorstInferenceSpeeds)
    print("total_detections=",TotalDetectionsArr)
    print("total_inferences=",TotalInferencesArr)

inputType="video"
source = "/data/video/MyDrive/sample.MP4" if inputType=="video" else 0
cam = cv2.VideoCapture(source)
display_handle=display(None, display_id=True)
every = 30

start = 0 if source!=0 else -1
end = int(cam.get(cv2.CAP_PROP_FRAME_COUNT)) if source!=0 else -1
if source!=0: cam.set(1, start)

frameCount=start
while_safety=0
saved_count=0

if not cam.isOpened(): print("Error: Could not open source.")
else:
    try:
        while True if end<0 else frameCount<end:
            _, frame = cam.read()
            if frame is None:
                if source!=0:
                    if while_safety > 2000: break
                    while_safety += 1
                    continue
                else:
                    print("Error: Could not capture frame.")
                    cam.release()
                    break

            if every>0:
                if (frameCount+1)%math.floor(every) == 0:
                    while_safety = 0
                    handleFrame(frame, frameCount, source)
            else:
                while_safety=0
                handleFrame(frame)
            frameCount += 1
        handleRelease()
    except KeyboardInterrupt:
        handleRelease()
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Source could not be released.")
        except:
            pass
        traceback.print_exc()


models= ['fcos_resnet', 'retinanet_resnet', 'retinanet_resnet_v2', 'faster_rcnn_resnet', 'faster_rcnn_resnet_v2', 'faster_rcnn_mobilenet_v3', 'faster_rcnn_mobilenet_v3_320', 'mask_rcnn_resnet', 'mask_rcnn_resnet_v2', 'ssd_vgg16', 'ssd_mobilenet_v3', 'yolov5nu', 'yolov5su', 'yolov5mu', 'yolov5lu', 'yolov5xu', 'yolov5n6u', 'yolov5s6u', 'yolov5m6u', 'yolov5l6u', 'yolov5x6u', 'yolov8n', 'yolov8s', 'yolov8m', 'yolov8l', 'yolov8x', 'yolov9c', 'yolov9e']
best_precisions= [84.01265740394592, 88.0074143409729, 96.11201882362366, 99.2476761341095, 99.67622756958008, 99.35745596885681, 98.9031732082367, 98.84726405143738, 99.59176182746887, 95.2492356300354, 97.8669285774231, 90.20088911056519, 94.16148066520691, 95.83455920219421, 97.12750315666199, 96.57518863677979, 89.92495536804199, 94.47564482688904, 96.59639000892639, 96.98560833930969, 96.82217240333557, 89.88815546035767, 96.51601314544678, 96.09537124633789, 96.96800708770752, 96.95228934288025, 95.96418142318726, 97.48859405517578]
avg_precisions= [65.43369123568901, 70.0324793656667, 75.85664047878616, 79.54838366646153, 79.66388660882201, 79.36338588817796, 77.99482395055266, 78.97953536059406, 78.64085898512886, 78.07364024801882, 79.79839080944657, 70.25242567062378, 74.78257812358238, 75.18818885939461, 80.1675331340265, 82.86621247728665, 71.7956124925289, 80.70994180239988, 80.02131668055002, 81.2741842534807, 82.92481622525624, 74.1588165362676, 79.52686240384867, 82.74784623620344, 84.02630616636837, 87.19815492045646, 83.0683293796721, 86.72942798494716]
worst_precisions= [50.03591179847717, 50.81883668899536, 50.52684545516968, 50.00559687614441, 50.01000165939331, 50.01301169395447, 50.599926710128784, 50.112515687942505, 50.14427304267883, 50.554364919662476, 51.80081129074097, 51.14137530326843, 51.496726274490356, 50.112175941467285, 50.28736591339111, 50.56321620941162, 50.43715238571167, 50.61156153678894, 50.04019737243652, 50.40223002433777, 50.01728534698486, 50.75660943984985, 50.046658515930176, 51.26423239707947, 52.22552418708801, 51.82181000709534, 50.462186336517334, 50.077080726623535]
best_speeds= [0.027644872665405273, 0.02587413787841797, 0.023566007614135742, 0.025889873504638672, 0.02659463882446289, 0.02038407325744629, 0.020329713821411133, 0.03192257881164551, 0.036510467529296875, 0.026212215423583984, 0.03560447692871094, 0.012197017669677734, 0.011764287948608398, 0.013895034790039062, 0.01591324806213379, 0.01826786994934082, 0.019175052642822266, 0.018574237823486328, 0.02128124237060547, 0.023772716522216797, 0.026639461517333984, 0.011053323745727539, 0.011191368103027344, 0.013138055801391602, 0.01499485969543457, 0.015213489532470703, 0.019521474838256836, 0.03244900703430176]
avg_speeds= [0.029150198056147648, 0.027244939158360165, 0.026093681247866884, 0.027591095821751073, 0.029093195285115923, 0.02196166853406536, 0.021440874446522106, 0.03931176211382892, 0.043849005017961774, 0.027934739689627095, 0.057551792512337364, 0.014452159020208544, 0.01461857557296753, 0.017481678553989955, 0.021082774671927963, 0.02572701871395111, 0.022002682393910934, 0.02229271577985099, 0.02610098140340456, 0.03077175440611663, 0.03624026690210615, 0.01343622366587321, 0.013594974528302203, 0.016490099901821525, 0.019749334151732093, 0.02078573844012092, 0.02406627538973692, 0.03907600462724621]
worst_speeds= [0.03884601593017578, 0.0301511287689209, 0.031243085861206055, 0.03315019607543945, 0.03593873977661133, 0.04246878623962402, 0.028371572494506836, 0.05147814750671387, 0.0669412612915039, 0.03157949447631836, 0.07221364974975586, 0.1345655918121338, 0.19080853462219238, 0.35675549507141113, 0.6467523574829102, 1.069488525390625, 0.19431591033935547, 0.29901742935180664, 0.5694301128387451, 0.9752030372619629, 1.5963263511657715, 0.12777161598205566, 0.20153522491455078, 0.35999107360839844, 0.5553417205810547, 0.7688348293304443, 0.46082496643066406, 0.8728563785552979]
best_inference_speeds= [0.027644872665405273, 0.02587413787841797, 0.023566007614135742, 0.025889873504638672, 0.02659463882446289, 0.02038407325744629, 0.020329713821411133, 0.03192257881164551, 0.036510467529296875, 0.026212215423583984, 0.03560447692871094, 0.012197017669677734, 0.011764287948608398, 0.013895034790039062, 0.01591324806213379, 0.01826786994934082, 0.019175052642822266, 0.018574237823486328, 0.02128124237060547, 0.023772716522216797, 0.026639461517333984, 0.011053323745727539, 0.011191368103027344, 0.013138055801391602, 0.01499485969543457, 0.015213489532470703, 0.019521474838256836, 0.03244900703430176]
avg_inference_speeds= [0.0291229285389544, 0.027250375503148787, 0.02604589733896376, 0.027591247044637533, 0.029162290550413587, 0.02192744237934044, 0.021404224893321163, 0.03877167332740057, 0.043950061003367104, 0.027945057038337953, 0.05707716360324767, 0.014504869241972227, 0.014692193490487558, 0.017811153108710485, 0.02154397533600589, 0.026529295001915115, 0.022091456081556236, 0.02246131107306025, 0.026932239532470703, 0.03230651369634664, 0.039018111199325654, 0.013504936013902936, 0.013721469007892373, 0.0167180720917479, 0.02009827114013304, 0.021534755116417295, 0.024419428354286284, 0.03998017878759475]
worst_inference_speeds= [0.03884601593017578, 0.0301511287689209, 0.031243085861206055, 0.03315019607543945, 0.03593873977661133, 0.04246878623962402, 0.028371572494506836, 0.05147814750671387, 0.0669412612915039, 0.03157949447631836, 0.07221364974975586, 0.1345655918121338, 0.19080853462219238, 0.35675549507141113, 0.6467523574829102, 1.069488525390625, 0.19431591033935547, 0.29901742935180664, 0.5694301128387451, 0.9752030372619629, 1.5963263511657715, 0.12777161598205566, 0.20153522491455078, 0.35999107360839844, 0.5553417205810547, 0.7688348293304443, 0.46082496643066406, 0.8728563785552979]
total_detections= [260, 192, 196, 381, 448, 268, 253, 370, 420, 167, 192, 155, 148, 175, 189, 192, 147, 178, 213, 216, 224, 150, 182, 187, 187, 204, 189, 207]
total_inferences= [166, 156, 158, 167, 168, 167, 161, 168, 168, 155, 164, 148, 135, 151, 166, 167, 138, 157, 160, 159, 161, 140, 162, 167, 166, 168, 166, 168]


import numpy as np
import matplotlib.pyplot as plt
import matplotlib

# Generate a color map
num_models = len(models)
cmap = matplotlib.colormaps.get_cmap('nipy_spectral')
colors = [cmap(i / num_models) for i in range(num_models)]

# Create a figure and plot with unique colors
plt.figure(figsize=(14, 10))
for i, model in enumerate(models):
    plt.scatter(avg_inference_speeds[i], avg_precisions[i], color=colors[i], label=model, edgecolor='black')

print("DEVICE: GPU (NVIDIA A100)")
print("SOURCE: data/video/sample.MP4 (1080p60fps Video)")
print("SKIP EVERY 60 FRAMES")
print("TOTALLY PROCESSED FRAMES: 84")
print("COCO Dataset, Classes truck, bus, car, train, bicycle")
print("ALL >50%")
# Axis labels and plot title
plt.xlabel('Average Speed (seconds)')
plt.ylabel('Average Precision (%)')
plt.title('Model Performance: Average Inference Speed x Average Accuracy')
plt.legend(loc='upper left', bbox_to_anchor=(1.05, 1), fontsize='small')
plt.grid(True)
plt.savefig("Analysis/Experiment1/A100/A100_Experiment1b_Diagram.png", bbox_inches='tight')
plt.show()

# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_precisions, avg_precisions, worst_precisions])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Precision (%)", "Average Precision (%)", "Worst Precision (%)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

plt.title("Model Precision Metrics")
plt.savefig("Analysis/Experiment1/A100/A100_Experiment1b_Precision_Table.png", bbox_inches='tight')
plt.show()


# # Set up the figure and axis for the table
# fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
# ax.axis('tight')
# ax.axis('off')

# # The table data: transpose the array to make each column a different metric
# table_data = np.transpose([models, best_speeds, avg_speeds, worst_speeds])

# # Create the table in the plot
# table = ax.table(cellText=table_data, colLabels=["Model", "Best Speed (s)", "Average Speed (s)", "Worst Speed (s)"],
#                  cellLoc='center', loc='center', colColours=["palegreen"] * 4)
# table.auto_set_font_size(False)
# table.set_fontsize(10)
# table.scale(1.2, 1.2)  # Scale table size

# plt.title("Model Speed Metrics")
# plt.show()

# Set up the figure and axis for the table
fig, ax = plt.subplots(figsize=(14, 12))  # Adjust size as needed
ax.axis('tight')
ax.axis('off')

# The table data: transpose the array to make each column a different metric
table_data = np.transpose([models, best_inference_speeds, avg_inference_speeds, worst_inference_speeds])

# Create the table in the plot
table = ax.table(cellText=table_data, colLabels=["Model", "Best Inference Speed (s)", "Average Inference Speed (s)", "Worst Inference Speed (s)"],
                 cellLoc='center', loc='center', colColours=["palegreen"] * 4)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)  # Scale table size

plt.title("Model Inference Speed Metrics")
plt.savefig("Analysis/Experiment1/A100/A100_Experiment1b_Time_Table.png", bbox_inches='tight')
plt.show()

data = []
for i in range(len(models)):
    data.append([models[i], f'{avg_precisions[i]:.2f}', f'{avg_inference_speeds[i]:.5f}'])

# LaTeX model names mapping
latex_model_names = {
    'fcos_resnet': 'FCOS ResNet50 FPN',
    'retinanet_resnet': 'RetinaNet ResNet50 FPN',
    'retinanet_resnet_v2': 'RetinaNet ResNet50 FPN V2',
    'faster_rcnn_resnet': 'Faster R-CNN ResNet50 FPN',
    'faster_rcnn_resnet_v2': 'Faster R-CNN ResNet50 FPN V2',
    'faster_rcnn_mobilenet_v3': 'Faster R-CNN MobileNet V3 L',
    'faster_rcnn_mobilenet_v3_320': 'Faster R-CNN MobileNet V3 L 320',
    'mask_rcnn_resnet': 'Mask R-CNN ResNet50',
    'mask_rcnn_resnet_v2': 'Mask R-CNN ResNet50 FPN V2',
    'ssd_vgg16': 'SSD VGG16',
    'ssd_mobilenet_v3': 'SSDLite MobileNet V3 Large',
    'yolov5nu': 'YOLOv5nu',
    'yolov5su': 'YOLOv5su',
    'yolov5mu': 'YOLOv5mu',
    'yolov5lu': 'YOLOv5lu',
    'yolov5xu': 'YOLOv5xu',
    'yolov5n6u': 'YOLOv5n6u',
    'yolov5s6u': 'YOLOv5s6u',
    'yolov5m6u': 'YOLOv5m6u',
    'yolov5l6u': 'YOLOv5l6u',
    'yolov5x6u': 'YOLOv5x6u',
    'yolov8n': 'YOLOv8n',
    'yolov8s': 'YOLOv8s',
    'yolov8m': 'YOLOv8m',
    'yolov8l': 'YOLOv8l',
    'yolov8x': 'YOLOv8x',
    'yolov9c': 'YOLOv9c',
    'yolov9e': 'YOLOv9e'
}

# Sort data by average precision in descending order
data_sorted = sorted(data, key=lambda x: float(x[1]), reverse=True)

# Generate LaTeX table
latex_table = "\\begin{table}[H]\n\\centering\n\\begin{tabular}{lrr}\n\\toprule\n"
latex_table += "\\multicolumn{3}{c}{A100 Experiment 1b: Vehicle Detection Models Performance}\\\\ \\cmidrule{1-3}\n"
latex_table += "Model & Average Precision (\\%) & Average Inference Speed (s)\\\\\n\\midrule\n"

best_prec_index = 0
best_speed = [99999.99, 0]
for i in range(len(data_sorted)):
    if float(data_sorted[i][2])<best_speed[0]:
        best_speed = [float(data_sorted[i][2]), i]

count = 0
for entry in data_sorted:
    model = latex_model_names[entry[0]]
    avg_precision = float(entry[1])
    avg_speed = float(entry[2])

    if count==best_prec_index:
        if count==best_speed[1]:
            latex_table += f"\\textbf{{{model}}} & \\textbf{{{avg_precision:.2f}}} & \\textbf{{{avg_speed:.5f}}} \\\\ \\addlinespace\n"
        else:
            latex_table += f"\\textbf{{{model}}} & \\textbf{{{avg_precision:.2f}}} & {avg_speed:.5f} \\\\ \\addlinespace\n"
    else:
        if count==best_speed[1]:
            latex_table += f"\\textbf{{{model}}} & {avg_precision:.2f} & \\textbf{{{avg_speed:.5f}}} \\\\ \\addlinespace\n"
        else:
            latex_table += f"{model} & {avg_precision:.2f} & {avg_speed:.5f} \\\\ \\addlinespace\n"

    count+=1

latex_table += "\\bottomrule\n\\end{tabular}\n\\caption{Vehicle Detection Models Performance}\n\\label{table:vehicle_detection_performance}\n\\end{table}"

print(latex_table)


import cv2
import torch
from torchvision.models.detection import *
from torchvision.io.image import read_image
from torchvision.utils import draw_bounding_boxes
from torchvision.transforms.functional import to_pil_image
import torchvision.transforms as transforms
from PIL import Image
import IPython
from IPython.display import display, clear_output
import time
import traceback
import math
from ultralytics import YOLO

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

transform = transforms.Compose([
    #transforms.Resize((300, 300)),
    transforms.ToTensor(),
])

class DetectionModel():
    def __init__(self, concept, backbone, model, weights, reference, color=None):
        self.concept = concept
        self.backbone = backbone
        self.model = model(weights) if concept=="YOLO" else model(weights=weights.DEFAULT)
        if concept!="YOLO":
            self.model.to(device)
            self.model.eval()
        self.weights = weights if concept=="YOLO" else weights.DEFAULT
        self.reference = reference
        self.color = color
        self.detections = []

    def addDetection(self, detection):
        self.detections.append(detection)

    def getDetections(self):
        return self.detections

    def clearDetections(self):
        self.detections = []

    def detectionsCount(self):
        return len(self.detections)

    def inferencesCount(self):
        length = 0
        addedFrames = []
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    addedFrames.append(self.detections[i].frame)
        return len(addedFrames)

    def getAveragePrecision(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].precision
            avg = sum/len(self.detections)
        return  avg.item() if torch.is_tensor(avg) else avg

    def getMaxPrecision(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].precision:
                max=self.detections[i].precision
        return max.item() if torch.is_tensor(max) else max

    def getMinPrecision(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].precision:
                min=self.detections[i].precision
        return 0.0 if min==9999999.99 else min.item() if torch.is_tensor(min) else min

    def getAverageSpeed(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].speed
            avg = sum/len(self.detections)
        return avg

    def getMaxSpeed(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].speed:
                max=self.detections[i].speed
        return max

    def getMinSpeed(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].speed:
                min=self.detections[i].speed
        return 0.0 if min==9999999.99 else min

    def getAverageInferenceSpeed(self):
        sum=0.0
        avg=0.0
        addedFrames=[]
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    sum+=self.detections[i].speed
                    addedFrames.append(self.detections[i].frame)
            avg = sum/len(addedFrames) if len(addedFrames)>0 else 0.0
        return avg

    def getMaxInferenceSpeed(self):
        max=0.0
        addedFrames=[]
        for i in range(len(self.detections)):
            if max<self.detections[i].speed and self.detections[i].frame not in addedFrames:
                max=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return max

    def getMinInferenceSpeed(self):
        min=9999999.99
        addedFrames=[]
        for i in range(len(self.detections)):
            if min>self.detections[i].speed and self.detections[i].frame not in addedFrames:
                min=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return 0.0 if min==9999999.99 else min

    def filterPrecisionsBelow(self, minPrecision):
        newDetections = []
        for i in range(len(self.detections)):
            if self.detections[i].precision>=minPrecision:
                newDetections.append(self.detections[i])
        self.detections = newDetections

    def printResults(self):
        print("Model:", self.reference,
              "\nBest Precision:", self.getMaxPrecision(),
              "%\nAverage Precision:", self.getAveragePrecision(),
              "%\nWorst Precision:", self.getMinPrecision(),
              "%\nBest Speed:", self.getMinSpeed(),
              "seconds\nAverage Speed:", self.getAverageSpeed(),
              "seconds\nWorst Speed:", self.getMaxSpeed(),
              "seconds\nBest Inference Speed:", self.getMinInferenceSpeed(),
              "seconds\nAverage Inference Speed:", self.getAverageInferenceSpeed(),
              "seconds\nWorst Inference Speed:", self.getMaxInferenceSpeed(),
              "seconds\nTotal Detections:", self.detectionsCount(),
              "\nTotal Inferences:", self.inferencesCount(),
              "\n\n")

class DetectionResult():
    def __init__(self, model, label, precision, speed, device=None, source=None, frame=None, boundingBox=None, details=None):
        self.model = model
        self.label = label
        self.precision = precision.item() if torch.is_tensor(precision) else precision
        self.speed = speed.item() if torch.is_tensor(speed) else speed
        self.device = device
        self.source = source
        self.frame = frame
        self.boundingBox = boundingBox
        self.details = details

target_classes = ["truck", "bus", "car", "train", "bicycle"]


VEHICLE_DETECTION_MODELS = [
    DetectionModel("FCOS", "resnet50_fpn", fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights, "fcos_resnet", (255,0,0)),
    DetectionModel("RetinaNet", "resnet50_fpn", retinanet_resnet50_fpn, RetinaNet_ResNet50_FPN_Weights, "retinanet_resnet", (255,255,0)),
    DetectionModel("RetinaNet", "resnet50_fpn_v2", retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights, "retinanet_resnet_v2", (0,234,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn", fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights, "faster_rcnn_resnet", (170,0,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn_v2", fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights, "faster_rcnn_resnet_v2", (255,127,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_fpn", fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights, "faster_rcnn_mobilenet_v3", (191,255,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_320_fpn", fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights, "faster_rcnn_mobilenet_v3_320", (0,149,255)),
    DetectionModel("MaskRCNN", "resnet50_fpn", maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights, "mask_rcnn_resnet", (255,0,170)),
    DetectionModel("MaskRCNN", "resnet50_fpn_v2", maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights, "mask_rcnn_resnet_v2", (255,212,0)),
    DetectionModel("SSD300", "vgg16", ssd300_vgg16, SSD300_VGG16_Weights, "ssd_vgg16", (106,255,0)),
    DetectionModel("SSDLite320", "mobilenet_v3_large", ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights, "ssd_mobilenet_v3", (0,64,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5nu.pt', "yolov5nu", (237,185,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5su.pt', "yolov5su", (185,215,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5mu.pt', "yolov5mu", (231,233,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5lu.pt', "yolov5lu", (220,185,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5xu.pt', "yolov5xu", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5n6u.pt', "yolov5n6u", (143,35,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5s6u.pt', "yolov5s6u", (35,98,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5m6u.pt', "yolov5m6u", (143,106,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5l6u.pt', "yolov5l6u", (107,35,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5x6u.pt', "yolov5x6u", (79,143,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8n.pt', "yolov8n", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8s.pt', "yolov8s", (115,115,115)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8m.pt', "yolov8m", (204,204,204)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8l.pt', "yolov8l", (255,0,0)), # cycle restarts
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8x.pt', "yolov8x", (255,255,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9c.pt', "yolov9c", (0,234,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9e.pt', "yolov9e", (170,0,255))#,
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10n.pt', "yolov10n", (185,237,224)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10s.pt', "yolov10s", (115,115,115)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10m.pt', "yolov10m", (204,204,204)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10b.pt', "yolov10b", (204,204,204)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10l.pt', "yolov10l", (255,0,0)), # cycle restarts
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10x.pt', "yolov10x", (255,255,0))
]

def process(frame, frameIndex=-1, source=None):
    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        processed_frame = frame.copy()

        if currentModel.concept=="YOLO":
            start_time = time.time()
            results = currentModel.model(processed_frame, agnostic_nms=True, verbose=False, device=device)[0]
            processing_time = time.time() - start_time
            for result in results:
                detection_count = result.boxes.shape[0]
                for j in range(detection_count):
                    cls = int(result.boxes.cls[j].item())
                    name = result.names[cls]
                    confidence = float(result.boxes.conf[j].item())
                    bounding_box = result.boxes.xyxy[j].cpu().numpy()
                    if name in target_classes and confidence>0.5:
                        x1, y1, x2, y2 = [int(x) for x in bounding_box]
                        # Ensure x1 < x2 and y1 < y2
                        x1, x2 = min(x1, x2), max(x1, x2)
                        y1, y2 = min(y1, y2), max(y1, y2)
                        label = '{} ({:.2f}%) ({:.3f}s) {}'.format(name.upper(), confidence*100, processing_time, currentModel.reference)
                        print(label)
                        VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, name, confidence*100, processing_time, device, source, frameIndex, bounding_box, label))
                        cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                        cv2.putText(processed_frame, label, (x1+10, y1+25), 0, 0.8, currentModel.color, 2)
        else:
            # Convert frame to RGB and PIL Image, then apply transformation
            frame_rgb = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            image = transform(pil_image).unsqueeze(0).to(device)

            # Object detection
            start_time = time.time()
            with torch.no_grad():
                predictions = currentModel.model(image)

            # Processing time
            processing_time = time.time() - start_time

            # Visualization
            scores = predictions[0]['scores']
            boxes = predictions[0]['boxes']
            labels = predictions[0]['labels']
            confidence_threshold = 0.5

            for score, box, label in zip(scores, boxes, labels):
                # and score > confidence_threshold
                if currentModel.weights.meta["categories"][label.item()] in target_classes and score > confidence_threshold:
                    x1, y1, x2, y2 = map(int, box)
                    label_text = f'{currentModel.weights.meta["categories"][label.item()].upper()} ({score*100:.2f}%) ({processing_time:.3f}s) {currentModel.reference}'
                    print(label_text)
                    VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, currentModel.weights.meta["categories"][label.item()], float(score.item())*100, processing_time, device, source, frameIndex, box, label_text))
                    cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                    cv2.putText(processed_frame, label_text, (x1, y1+20), 0, 0.8, currentModel.color, 2)

        # img = Image.fromarray(processed_frame, 'RGB')
        # display(img)
        clear_output(wait=True)
    return frame

def handleFrame(frame, frameIndex=-1, source=None):
    processed_frame = process(frame,frameIndex,source)

def handleRelease():
    cam.release()
    print("Source released.")

    print(device)
    print(source)

    BestPrecision=None
    BestAvgPrecision=None
    WorstPrecision=None
    BestSpeed=None
    BestAvgSpeed=None
    WorstSpeed=None

    Models=[]
    BestPrecisions=[]
    AvgPrecisions=[]
    WorstPrecisions=[]
    BestSpeeds=[]
    AvgSpeeds=[]
    WorstSpeeds=[]
    BestInferenceSpeeds=[]
    AvgInferenceSpeeds=[]
    WorstInferenceSpeeds=[]
    TotalDetectionsArr=[]
    TotalInferencesArr=[]

    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        # currentModel.printResults()

        Models.append(currentModel.reference)
        BestPrecisions.append(currentModel.getMaxPrecision())
        AvgPrecisions.append(currentModel.getAveragePrecision())
        WorstPrecisions.append(currentModel.getMinPrecision())
        BestSpeeds.append(currentModel.getMinSpeed())
        AvgSpeeds.append(currentModel.getAverageSpeed())
        WorstSpeeds.append(currentModel.getMaxSpeed())
        BestInferenceSpeeds.append(currentModel.getMinInferenceSpeed())
        AvgInferenceSpeeds.append(currentModel.getAverageInferenceSpeed())
        WorstInferenceSpeeds.append(currentModel.getMaxInferenceSpeed())
        TotalDetectionsArr.append(currentModel.detectionsCount())
        TotalInferencesArr.append(currentModel.inferencesCount())

        if BestPrecision is None or BestPrecision.getMaxPrecision()<=currentModel.getMaxPrecision():
            BestPrecision = currentModel
        if BestAvgPrecision is None or BestAvgPrecision.getAveragePrecision()<=currentModel.getAveragePrecision():
            BestAvgPrecision = currentModel
        if WorstPrecision is None or WorstPrecision.getMinPrecision()>=currentModel.getMinPrecision():
            WorstPrecision = currentModel
        if BestSpeed is None or BestSpeed.getMinSpeed()>=currentModel.getMinSpeed():
            BestSpeed = currentModel
        if BestAvgSpeed is None or BestAvgSpeed.getAverageSpeed()>=currentModel.getAverageSpeed():
            BestAvgSpeed = currentModel
        if WorstSpeed is None or WorstSpeed.getMaxSpeed()<=currentModel.getMaxSpeed():
            WorstSpeed = currentModel

    if BestPrecision is not None:
        print("Best Precision:")
        BestPrecision.printResults()
    if BestAvgPrecision is not None:
        print("Best Average Precision:")
        BestAvgPrecision.printResults()
    if WorstPrecision is not None:
        print("Worst Precision:")
        WorstPrecision.printResults()
    if BestSpeed is not None:
        print("Best Speed:")
        BestSpeed.printResults()
    if BestAvgSpeed is not None:
        print("Best Average Speed:")
        BestAvgSpeed.printResults()
    if WorstSpeed is not None:
        print("Worst Speed:")
        WorstSpeed.printResults()

    print("models=",Models)
    print("best_precisions=",BestPrecisions)
    print("avg_precisions=",AvgPrecisions)
    print("worst_precisions=",WorstPrecisions)
    print("best_speeds=",BestSpeeds)
    print("avg_speeds=",AvgSpeeds)
    print("worst_speeds=",WorstSpeeds)
    print("best_inference_speeds=",BestInferenceSpeeds)
    print("avg_inference_speeds=",AvgInferenceSpeeds)
    print("worst_inference_speeds=",WorstInferenceSpeeds)
    print("total_detections=",TotalDetectionsArr)
    print("total_inferences=",TotalInferencesArr)

inputType="video"
source = "/data/video/MyDrive/vecteezy_car-and-truck-traffic-on-the-highway-in-europe-poland_7957364.mp4" if inputType=="video" else 0
cam = cv2.VideoCapture(source)
display_handle=display(None, display_id=True)
every = 30

start = 0 if source!=0 else -1
end = int(cam.get(cv2.CAP_PROP_FRAME_COUNT)) if source!=0 else -1
if source!=0: cam.set(1, start)

frameCount=start
while_safety=0
saved_count=0

if not cam.isOpened(): print("Error: Could not open source.")
else:
    try:
        while True if end<0 else frameCount<end:
            _, frame = cam.read()
            if frame is None:
                if source!=0:
                    if while_safety > 2000: break
                    while_safety += 1
                    continue
                else:
                    print("Error: Could not capture frame.")
                    cam.release()
                    break

            if every>0:
                if (frameCount+1)%math.floor(every) == 0:
                    while_safety = 0
                    handleFrame(frame, frameCount, source)
            else:
                while_safety=0
                handleFrame(frame)
            frameCount += 1
        handleRelease()
    except KeyboardInterrupt:
        handleRelease()
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Source could not be released.")
        except:
            pass
        traceback.print_exc()


models= ['fcos_resnet', 'retinanet_resnet', 'retinanet_resnet_v2', 'faster_rcnn_resnet', 'faster_rcnn_resnet_v2', 'faster_rcnn_mobilenet_v3', 'faster_rcnn_mobilenet_v3_320', 'mask_rcnn_resnet', 'mask_rcnn_resnet_v2', 'ssd_vgg16', 'ssd_mobilenet_v3', 'yolov5nu', 'yolov5su', 'yolov5mu', 'yolov5lu', 'yolov5xu', 'yolov5n6u', 'yolov5s6u', 'yolov5m6u', 'yolov5l6u', 'yolov5x6u', 'yolov8n', 'yolov8s', 'yolov8m', 'yolov8l', 'yolov8x', 'yolov9c', 'yolov9e']
best_precisions= [88.37912082672119, 96.52217626571655, 95.00285983085632, 99.93743300437927, 99.870765209198, 99.3215560913086, 99.72881078720093, 99.91304278373718, 99.91193413734436, 98.04015755653381, 98.03220629692078, 91.97141528129578, 94.08358335494995, 96.37671709060669, 95.80487608909607, 97.32545018196106, 94.2404568195343, 95.5467164516449, 96.93294167518616, 97.27240800857544, 96.98469638824463, 95.45567035675049, 95.1836884021759, 96.54003381729126, 96.16286754608154, 96.99667692184448, 96.51110172271729, 97.04293608665466]
avg_precisions= [65.88705201302805, 70.20452270141014, 70.43078180990722, 79.78523632952275, 84.7792845012413, 81.15352049469948, 84.7442609613592, 81.2189576970307, 84.39474126467339, 74.38385561108589, 79.74010620798383, 68.63643800890124, 75.48393322073895, 75.31666706005733, 76.93025107567126, 78.24570026329096, 72.94357526409733, 77.37849240994636, 78.42945521697402, 79.05302064759391, 80.31645810416856, 72.16571577607769, 76.03780777767452, 76.25649813714067, 78.37393034749957, 79.44695245528567, 79.53204833869393, 79.57995613416035]
worst_precisions= [50.043582916259766, 50.12778639793396, 50.0189483165741, 50.02419948577881, 50.128406286239624, 50.06841421127319, 50.54776668548584, 50.483447313308716, 50.09356141090393, 51.82750225067139, 53.007447719573975, 50.100016593933105, 51.66192650794983, 50.18165111541748, 50.16138553619385, 50.728100538253784, 50.442707538604736, 51.36644244194031, 50.34624934196472, 50.277066230773926, 50.70681571960449, 50.66300630569458, 50.9341835975647, 50.09634494781494, 50.49591660499573, 50.46571493148804, 50.14040470123291, 50.463950634002686]
best_speeds= [0.02910447120666504, 0.027803421020507812, 0.026973724365234375, 0.02784585952758789, 0.03053569793701172, 0.023972034454345703, 0.0209958553314209, 0.0894765853881836, 0.0778656005859375, 0.028266191482543945, 0.03903698921203613, 0.013564109802246094, 0.013356447219848633, 0.01595449447631836, 0.01803302764892578, 0.02010941505432129, 0.02121281623840332, 0.020664453506469727, 0.02321791648864746, 0.026016712188720703, 0.029688119888305664, 0.011809587478637695, 0.01187443733215332, 0.013791561126708984, 0.016121625900268555, 0.01610851287841797, 0.020633220672607422, 0.03331732749938965]
avg_speeds= [0.030717670276600828, 0.029118781823378345, 0.027742795254054823, 0.029778264861070474, 0.031102572050359514, 0.024888163805007933, 0.022098432887684216, 0.09708416597924759, 0.08949755063423744, 0.029828232526779175, 0.04615051405770438, 0.026154582564895217, 0.03234674619591754, 0.05170178810755412, 0.07723794166858379, 0.12797056513724567, 0.03663289869153822, 0.05306455015226175, 0.0884820856153965, 0.1327733543940953, 0.19778968900910224, 0.022694930638352484, 0.03322323404177271, 0.050782775491233764, 0.07117407179590482, 0.08447340951449629, 0.06741421949778888, 0.12445919425399216]
worst_speeds= [0.03397250175476074, 0.031121253967285156, 0.031102895736694336, 0.036997318267822266, 0.03276872634887695, 0.02665233612060547, 0.02463507652282715, 0.1273181438446045, 0.09782862663269043, 0.031244993209838867, 0.07379627227783203, 0.14313840866088867, 0.19800662994384766, 0.36751317977905273, 0.6548187732696533, 1.0826294422149658, 0.20237159729003906, 0.3030669689178467, 0.5835707187652588, 0.9912185668945312, 1.625669240951538, 0.13418340682983398, 0.21115326881408691, 0.3762471675872803, 0.5677235126495361, 0.779289722442627, 0.4650897979736328, 0.8801186084747314]
best_inference_speeds= [0.02910447120666504, 0.027803421020507812, 0.026973724365234375, 0.02784585952758789, 0.03053569793701172, 0.023972034454345703, 0.0209958553314209, 0.0894765853881836, 0.0778656005859375, 0.028266191482543945, 0.03903698921203613, 0.013564109802246094, 0.013356447219848633, 0.01595449447631836, 0.01803302764892578, 0.02010941505432129, 0.02121281623840332, 0.020664453506469727, 0.02321791648864746, 0.026016712188720703, 0.029688119888305664, 0.011809587478637695, 0.01187443733215332, 0.013791561126708984, 0.016121625900268555, 0.01610851287841797, 0.020633220672607422, 0.03331732749938965]
avg_inference_speeds= [0.030703139305114747, 0.029053592681884767, 0.027749156951904295, 0.029843759536743165, 0.03109254837036133, 0.02492680549621582, 0.021956562995910645, 0.09749453067779541, 0.0894050121307373, 0.02961423397064209, 0.05258495467049735, 0.026854991912841797, 0.03262383937835693, 0.05166444778442383, 0.0821387529373169, 0.12719953060150146, 0.04005641937255859, 0.051118874549865724, 0.08034460544586182, 0.12448627948760986, 0.19028282165527344, 0.024871516227722167, 0.03299317359924316, 0.051590895652770995, 0.07279999256134033, 0.0944180965423584, 0.06761684417724609, 0.12126193046569825]
worst_inference_speeds= [0.03397250175476074, 0.031121253967285156, 0.031102895736694336, 0.036997318267822266, 0.03276872634887695, 0.02665233612060547, 0.02463507652282715, 0.1273181438446045, 0.09782862663269043, 0.031244993209838867, 0.07379627227783203, 0.14313840866088867, 0.19800662994384766, 0.36751317977905273, 0.6548187732696533, 1.0826294422149658, 0.20237159729003906, 0.3030669689178467, 0.5835707187652588, 0.9912185668945312, 1.625669240951538, 0.13418340682983398, 0.21115326881408691, 0.3762471675872803, 0.5677235126495361, 0.779289722442627, 0.4650897979736328, 0.8801186084747314]
total_detections= [186, 130, 152, 262, 288, 120, 55, 263, 260, 40, 14, 74, 92, 120, 130, 139, 111, 131, 192, 175, 191, 73, 99, 123, 134, 138, 141, 135]
total_inferences= [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 7, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]


import cv2
import torch
from torchvision.models.detection import *
from torchvision.io.image import read_image
from torchvision.utils import draw_bounding_boxes
from torchvision.transforms.functional import to_pil_image
import torchvision.transforms as transforms
from PIL import Image
import IPython
from IPython.display import display, clear_output
import time
import traceback
import math
from ultralytics import YOLO

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

transform = transforms.Compose([
    #transforms.Resize((300, 300)),
    transforms.ToTensor(),
])

class DetectionModel():
    def __init__(self, concept, backbone, model, weights, reference, color=None):
        self.concept = concept
        self.backbone = backbone
        self.model = model(weights) if concept=="YOLO" else model(weights=weights.DEFAULT)
        if concept!="YOLO":
            self.model.to(device)
            self.model.eval()
        self.weights = weights if concept=="YOLO" else weights.DEFAULT
        self.reference = reference
        self.color = color
        self.detections = []

    def addDetection(self, detection):
        self.detections.append(detection)

    def getDetections(self):
        return self.detections

    def clearDetections(self):
        self.detections = []

    def detectionsCount(self):
        return len(self.detections)

    def inferencesCount(self):
        length = 0
        addedFrames = []
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    addedFrames.append(self.detections[i].frame)
        return len(addedFrames)

    def getAveragePrecision(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].precision
            avg = sum/len(self.detections)
        return  avg.item() if torch.is_tensor(avg) else avg

    def getMaxPrecision(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].precision:
                max=self.detections[i].precision
        return max.item() if torch.is_tensor(max) else max

    def getMinPrecision(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].precision:
                min=self.detections[i].precision
        return 0.0 if min==9999999.99 else min.item() if torch.is_tensor(min) else min

    def getAverageSpeed(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].speed
            avg = sum/len(self.detections)
        return avg

    def getMaxSpeed(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].speed:
                max=self.detections[i].speed
        return max

    def getMinSpeed(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].speed:
                min=self.detections[i].speed
        return 0.0 if min==9999999.99 else min

    def getAverageInferenceSpeed(self):
        sum=0.0
        avg=0.0
        addedFrames=[]
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    sum+=self.detections[i].speed
                    addedFrames.append(self.detections[i].frame)
            avg = sum/len(addedFrames) if len(addedFrames)>0 else 0.0
        return avg

    def getMaxInferenceSpeed(self):
        max=0.0
        addedFrames=[]
        for i in range(len(self.detections)):
            if max<self.detections[i].speed and self.detections[i].frame not in addedFrames:
                max=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return max

    def getMinInferenceSpeed(self):
        min=9999999.99
        addedFrames=[]
        for i in range(len(self.detections)):
            if min>self.detections[i].speed and self.detections[i].frame not in addedFrames:
                min=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return 0.0 if min==9999999.99 else min

    def filterPrecisionsBelow(self, minPrecision):
        newDetections = []
        for i in range(len(self.detections)):
            if self.detections[i].precision>=minPrecision:
                newDetections.append(self.detections[i])
        self.detections = newDetections

    def printResults(self):
        print("Model:", self.reference,
              "\nBest Precision:", self.getMaxPrecision(),
              "%\nAverage Precision:", self.getAveragePrecision(),
              "%\nWorst Precision:", self.getMinPrecision(),
              "%\nBest Speed:", self.getMinSpeed(),
              "seconds\nAverage Speed:", self.getAverageSpeed(),
              "seconds\nWorst Speed:", self.getMaxSpeed(),
              "seconds\nBest Inference Speed:", self.getMinInferenceSpeed(),
              "seconds\nAverage Inference Speed:", self.getAverageInferenceSpeed(),
              "seconds\nWorst Inference Speed:", self.getMaxInferenceSpeed(),
              "seconds\nTotal Detections:", self.detectionsCount(),
              "\nTotal Inferences:", self.inferencesCount(),
              "\n\n")

class DetectionResult():
    def __init__(self, model, label, precision, speed, device=None, source=None, frame=None, boundingBox=None, details=None):
        self.model = model
        self.label = label
        self.precision = precision.item() if torch.is_tensor(precision) else precision
        self.speed = speed.item() if torch.is_tensor(speed) else speed
        self.device = device
        self.source = source
        self.frame = frame
        self.boundingBox = boundingBox
        self.details = details

target_classes = ["truck", "bus", "car", "train", "bicycle"]


VEHICLE_DETECTION_MODELS = [
    DetectionModel("FCOS", "resnet50_fpn", fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights, "fcos_resnet", (255,0,0)),
    DetectionModel("RetinaNet", "resnet50_fpn", retinanet_resnet50_fpn, RetinaNet_ResNet50_FPN_Weights, "retinanet_resnet", (255,255,0)),
    DetectionModel("RetinaNet", "resnet50_fpn_v2", retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights, "retinanet_resnet_v2", (0,234,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn", fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights, "faster_rcnn_resnet", (170,0,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn_v2", fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights, "faster_rcnn_resnet_v2", (255,127,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_fpn", fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights, "faster_rcnn_mobilenet_v3", (191,255,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_320_fpn", fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights, "faster_rcnn_mobilenet_v3_320", (0,149,255)),
    DetectionModel("MaskRCNN", "resnet50_fpn", maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights, "mask_rcnn_resnet", (255,0,170)),
    DetectionModel("MaskRCNN", "resnet50_fpn_v2", maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights, "mask_rcnn_resnet_v2", (255,212,0)),
    DetectionModel("SSD300", "vgg16", ssd300_vgg16, SSD300_VGG16_Weights, "ssd_vgg16", (106,255,0)),
    DetectionModel("SSDLite320", "mobilenet_v3_large", ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights, "ssd_mobilenet_v3", (0,64,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5nu.pt', "yolov5nu", (237,185,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5su.pt', "yolov5su", (185,215,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5mu.pt', "yolov5mu", (231,233,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5lu.pt', "yolov5lu", (220,185,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5xu.pt', "yolov5xu", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5n6u.pt', "yolov5n6u", (143,35,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5s6u.pt', "yolov5s6u", (35,98,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5m6u.pt', "yolov5m6u", (143,106,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5l6u.pt', "yolov5l6u", (107,35,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5x6u.pt', "yolov5x6u", (79,143,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8n.pt', "yolov8n", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8s.pt', "yolov8s", (115,115,115)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8m.pt', "yolov8m", (204,204,204)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8l.pt', "yolov8l", (255,0,0)), # cycle restarts
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8x.pt', "yolov8x", (255,255,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9c.pt', "yolov9c", (0,234,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9e.pt', "yolov9e", (170,0,255))#,
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10n.pt', "yolov10n", (185,237,224)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10s.pt', "yolov10s", (115,115,115)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10m.pt', "yolov10m", (204,204,204)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10b.pt', "yolov10b", (204,204,204)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10l.pt', "yolov10l", (255,0,0)), # cycle restarts
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10x.pt', "yolov10x", (255,255,0))
]

def process(frame, frameIndex=-1, source=None):
    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        processed_frame = frame.copy()

        if currentModel.concept=="YOLO":
            start_time = time.time()
            results = currentModel.model(processed_frame, agnostic_nms=True, verbose=False, device=device)[0]
            processing_time = time.time() - start_time
            for result in results:
                detection_count = result.boxes.shape[0]
                for j in range(detection_count):
                    cls = int(result.boxes.cls[j].item())
                    name = result.names[cls]
                    confidence = float(result.boxes.conf[j].item())
                    bounding_box = result.boxes.xyxy[j].cpu().numpy()
                    if name in target_classes and confidence>0.5:
                        x1, y1, x2, y2 = [int(x) for x in bounding_box]
                        # Ensure x1 < x2 and y1 < y2
                        x1, x2 = min(x1, x2), max(x1, x2)
                        y1, y2 = min(y1, y2), max(y1, y2)
                        label = '{} ({:.2f}%) ({:.3f}s) {}'.format(name.upper(), confidence*100, processing_time, currentModel.reference)
                        print(label)
                        VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, name, confidence*100, processing_time, device, source, frameIndex, bounding_box, label))
                        cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                        cv2.putText(processed_frame, label, (x1+10, y1+25), 0, 0.8, currentModel.color, 2)
        else:
            # Convert frame to RGB and PIL Image, then apply transformation
            frame_rgb = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            image = transform(pil_image).unsqueeze(0).to(device)

            # Object detection
            start_time = time.time()
            with torch.no_grad():
                predictions = currentModel.model(image)

            # Processing time
            processing_time = time.time() - start_time

            # Visualization
            scores = predictions[0]['scores']
            boxes = predictions[0]['boxes']
            labels = predictions[0]['labels']
            confidence_threshold = 0.5

            for score, box, label in zip(scores, boxes, labels):
                # and score > confidence_threshold
                if currentModel.weights.meta["categories"][label.item()] in target_classes and score > confidence_threshold:
                    x1, y1, x2, y2 = map(int, box)
                    label_text = f'{currentModel.weights.meta["categories"][label.item()].upper()} ({score*100:.2f}%) ({processing_time:.3f}s) {currentModel.reference}'
                    print(label_text)
                    VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, currentModel.weights.meta["categories"][label.item()], float(score.item())*100, processing_time, device, source, frameIndex, box, label_text))
                    cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                    cv2.putText(processed_frame, label_text, (x1, y1+20), 0, 0.8, currentModel.color, 2)

        # img = Image.fromarray(processed_frame, 'RGB')
        # display(img)
        clear_output(wait=True)
    return frame

def handleFrame(frame, frameIndex=-1, source=None):
    processed_frame = process(frame,frameIndex,source)

def handleRelease():
    cam.release()
    print("Source released.")

    print(device)
    print(source)

    BestPrecision=None
    BestAvgPrecision=None
    WorstPrecision=None
    BestSpeed=None
    BestAvgSpeed=None
    WorstSpeed=None

    Models=[]
    BestPrecisions=[]
    AvgPrecisions=[]
    WorstPrecisions=[]
    BestSpeeds=[]
    AvgSpeeds=[]
    WorstSpeeds=[]
    BestInferenceSpeeds=[]
    AvgInferenceSpeeds=[]
    WorstInferenceSpeeds=[]
    TotalDetectionsArr=[]
    TotalInferencesArr=[]

    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        # currentModel.printResults()

        Models.append(currentModel.reference)
        BestPrecisions.append(currentModel.getMaxPrecision())
        AvgPrecisions.append(currentModel.getAveragePrecision())
        WorstPrecisions.append(currentModel.getMinPrecision())
        BestSpeeds.append(currentModel.getMinSpeed())
        AvgSpeeds.append(currentModel.getAverageSpeed())
        WorstSpeeds.append(currentModel.getMaxSpeed())
        BestInferenceSpeeds.append(currentModel.getMinInferenceSpeed())
        AvgInferenceSpeeds.append(currentModel.getAverageInferenceSpeed())
        WorstInferenceSpeeds.append(currentModel.getMaxInferenceSpeed())
        TotalDetectionsArr.append(currentModel.detectionsCount())
        TotalInferencesArr.append(currentModel.inferencesCount())

        if BestPrecision is None or BestPrecision.getMaxPrecision()<=currentModel.getMaxPrecision():
            BestPrecision = currentModel
        if BestAvgPrecision is None or BestAvgPrecision.getAveragePrecision()<=currentModel.getAveragePrecision():
            BestAvgPrecision = currentModel
        if WorstPrecision is None or WorstPrecision.getMinPrecision()>=currentModel.getMinPrecision():
            WorstPrecision = currentModel
        if BestSpeed is None or BestSpeed.getMinSpeed()>=currentModel.getMinSpeed():
            BestSpeed = currentModel
        if BestAvgSpeed is None or BestAvgSpeed.getAverageSpeed()>=currentModel.getAverageSpeed():
            BestAvgSpeed = currentModel
        if WorstSpeed is None or WorstSpeed.getMaxSpeed()<=currentModel.getMaxSpeed():
            WorstSpeed = currentModel

    if BestPrecision is not None:
        print("Best Precision:")
        BestPrecision.printResults()
    if BestAvgPrecision is not None:
        print("Best Average Precision:")
        BestAvgPrecision.printResults()
    if WorstPrecision is not None:
        print("Worst Precision:")
        WorstPrecision.printResults()
    if BestSpeed is not None:
        print("Best Speed:")
        BestSpeed.printResults()
    if BestAvgSpeed is not None:
        print("Best Average Speed:")
        BestAvgSpeed.printResults()
    if WorstSpeed is not None:
        print("Worst Speed:")
        WorstSpeed.printResults()

    print("models=",Models)
    print("best_precisions=",BestPrecisions)
    print("avg_precisions=",AvgPrecisions)
    print("worst_precisions=",WorstPrecisions)
    print("best_speeds=",BestSpeeds)
    print("avg_speeds=",AvgSpeeds)
    print("worst_speeds=",WorstSpeeds)
    print("best_inference_speeds=",BestInferenceSpeeds)
    print("avg_inference_speeds=",AvgInferenceSpeeds)
    print("worst_inference_speeds=",WorstInferenceSpeeds)
    print("total_detections=",TotalDetectionsArr)
    print("total_inferences=",TotalInferencesArr)

inputType="video"
source = "/data/video/MyDrive/vecteezy_car-and-truck-traffic-on-the-highway-in-europe-poland_7957364.mp4" if inputType=="video" else 0
cam = cv2.VideoCapture(source)
display_handle=display(None, display_id=True)
every = 60

start = 0 if source!=0 else -1
end = int(cam.get(cv2.CAP_PROP_FRAME_COUNT)) if source!=0 else -1
if source!=0: cam.set(1, start)

frameCount=start
while_safety=0
saved_count=0

if not cam.isOpened(): print("Error: Could not open source.")
else:
    try:
        while True if end<0 else frameCount<end:
            _, frame = cam.read()
            if frame is None:
                if source!=0:
                    if while_safety > 2000: break
                    while_safety += 1
                    continue
                else:
                    print("Error: Could not capture frame.")
                    cam.release()
                    break

            if every>0:
                if (frameCount+1)%math.floor(every) == 0:
                    while_safety = 0
                    handleFrame(frame, frameCount, source)
            else:
                while_safety=0
                handleFrame(frame)
            frameCount += 1
        handleRelease()
    except KeyboardInterrupt:
        handleRelease()
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Source could not be released.")
        except:
            pass
        traceback.print_exc()


models= ['fcos_resnet', 'retinanet_resnet', 'retinanet_resnet_v2', 'faster_rcnn_resnet', 'faster_rcnn_resnet_v2', 'faster_rcnn_mobilenet_v3', 'faster_rcnn_mobilenet_v3_320', 'mask_rcnn_resnet', 'mask_rcnn_resnet_v2', 'ssd_vgg16', 'ssd_mobilenet_v3', 'yolov5nu', 'yolov5su', 'yolov5mu', 'yolov5lu', 'yolov5xu', 'yolov5n6u', 'yolov5s6u', 'yolov5m6u', 'yolov5l6u', 'yolov5x6u', 'yolov8n', 'yolov8s', 'yolov8m', 'yolov8l', 'yolov8x', 'yolov9c', 'yolov9e']
best_precisions= [88.37912082672119, 94.32494640350342, 94.65073943138123, 99.81302618980408, 99.86346364021301, 99.15416836738586, 99.12877678871155, 99.87890124320984, 99.84723925590515, 94.7755753993988, 98.03220629692078, 91.97141528129578, 94.08358335494995, 96.37671709060669, 95.80487608909607, 97.32545018196106, 94.2404568195343, 95.5467164516449, 96.93294167518616, 96.77342772483826, 96.98469638824463, 95.45567035675049, 95.1836884021759, 96.54003381729126, 96.16286754608154, 96.99667692184448, 96.38516306877136, 97.04293608665466]
avg_precisions= [66.38063192367554, 70.31318630490985, 70.46555804239738, 79.88651907721231, 85.44781701905387, 79.09517854940695, 80.36167047641895, 81.76098577678204, 84.54405398368836, 75.39541940940053, 71.7406017439706, 66.31644831763373, 75.25983642448078, 75.4131555557251, 76.95834333934481, 77.87207067012787, 72.07708433270454, 76.98223683983088, 78.38476498921712, 79.00591912594709, 80.49532110973071, 70.9620753924052, 76.97889778925025, 76.53297672669093, 78.50014666716258, 79.66539807941602, 79.37682767709096, 79.35527091595664]
worst_precisions= [50.54320693016052, 50.12778639793396, 50.10308027267456, 50.62161087989807, 50.128406286239624, 50.06841421127319, 50.54776668548584, 50.6435751914978, 50.19542574882507, 54.591089487075806, 53.007447719573975, 50.100016593933105, 51.66192650794983, 50.18165111541748, 50.7318913936615, 51.12574100494385, 50.442707538604736, 52.71173119544983, 50.34624934196472, 50.277066230773926, 50.951606035232544, 51.84769034385681, 50.9341835975647, 52.19009518623352, 51.165127754211426, 53.625547885894775, 50.14040470123291, 50.463950634002686]
best_speeds= [0.028692245483398438, 0.027371883392333984, 0.026258230209350586, 0.02766728401184082, 0.030400753021240234, 0.024339914321899414, 0.021395206451416016, 0.08197569847106934, 0.0739438533782959, 0.027767658233642578, 0.03849196434020996, 0.01270294189453125, 0.012412309646606445, 0.014627456665039062, 0.01733875274658203, 0.01888442039489746, 0.021814346313476562, 0.021096467971801758, 0.02306962013244629, 0.025700092315673828, 0.029650211334228516, 0.012342691421508789, 0.011818170547485352, 0.013787269592285156, 0.015688419342041016, 0.01607346534729004, 0.020392656326293945, 0.03363323211669922]
avg_speeds= [0.032740778975434356, 0.02835801669529506, 0.02733731914210964, 0.02981897657231767, 0.031231100218636648, 0.025111710438962844, 0.024256918165418837, 0.08618805184960365, 0.07738753318786622, 0.06514417497735274, 0.0431340081351144, 0.04526828394995795, 0.048079160126772796, 0.08407416990247824, 0.14211240268888928, 0.23015942232949393, 0.05443724138396127, 0.07503534108400345, 0.1490620259315737, 0.20928146080537277, 0.3586106813082131, 0.039640446503957115, 0.0510989583056906, 0.07357945839564005, 0.11600023327451764, 0.19052840315777322, 0.1276368796825409, 0.22261105722455837]
worst_speeds= [0.04188108444213867, 0.02892136573791504, 0.028197050094604492, 0.031426429748535156, 0.03255724906921387, 0.025613069534301758, 0.03428220748901367, 0.09318113327026367, 0.0817861557006836, 0.37972331047058105, 0.061370849609375, 0.14016079902648926, 0.20264649391174316, 0.3820760250091553, 0.66884446144104, 1.0705275535583496, 0.2026355266571045, 0.3043632507324219, 0.5757238864898682, 0.9660162925720215, 1.635282278060913, 0.13269662857055664, 0.2083415985107422, 0.36543869972229004, 0.5587365627288818, 0.7637174129486084, 0.5634026527404785, 0.8691024780273438]
best_inference_speeds= [0.028692245483398438, 0.027371883392333984, 0.026258230209350586, 0.02766728401184082, 0.030400753021240234, 0.024339914321899414, 0.021395206451416016, 0.08197569847106934, 0.0739438533782959, 0.027767658233642578, 0.03849196434020996, 0.01270294189453125, 0.012412309646606445, 0.014627456665039062, 0.01733875274658203, 0.01888442039489746, 0.021814346313476562, 0.021096467971801758, 0.02306962013244629, 0.025700092315673828, 0.029650211334228516, 0.012342691421508789, 0.011818170547485352, 0.013787269592285156, 0.015688419342041016, 0.01607346534729004, 0.020392656326293945, 0.03363323211669922]
avg_inference_speeds= [0.0327303409576416, 0.028403329849243163, 0.027341032028198244, 0.029828548431396484, 0.031239891052246095, 0.02510676383972168, 0.02505040168762207, 0.0862342357635498, 0.07745833396911621, 0.09858412742614746, 0.04772082964579264, 0.03895082473754883, 0.05151910781860351, 0.0890388011932373, 0.1483431339263916, 0.23018937110900878, 0.058318042755126955, 0.07856030464172363, 0.13481707572937013, 0.21561760902404786, 0.35174169540405276, 0.03701186180114746, 0.05187168121337891, 0.08517580032348633, 0.1258373260498047, 0.1667386531829834, 0.13058724403381347, 0.20269393920898438]
worst_inference_speeds= [0.04188108444213867, 0.02892136573791504, 0.028197050094604492, 0.031426429748535156, 0.03255724906921387, 0.025613069534301758, 0.03428220748901367, 0.09318113327026367, 0.0817861557006836, 0.37972331047058105, 0.061370849609375, 0.14016079902648926, 0.20264649391174316, 0.3820760250091553, 0.66884446144104, 1.0705275535583496, 0.2026355266571045, 0.3043632507324219, 0.5757238864898682, 0.9660162925720215, 1.635282278060913, 0.13269662857055664, 0.2083415985107422, 0.36543869972229004, 0.5587365627288818, 0.7637174129486084, 0.5634026527404785, 0.8691024780273438]
total_detections= [91, 63, 74, 129, 140, 61, 27, 128, 125, 19, 7, 36, 44, 59, 63, 70, 56, 64, 93, 88, 93, 36, 46, 60, 66, 69, 72, 67]
total_inferences= [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]


import cv2
import torch
from torchvision.models.detection import *
from torchvision.io.image import read_image
from torchvision.utils import draw_bounding_boxes
from torchvision.transforms.functional import to_pil_image
import torchvision.transforms as transforms
from PIL import Image
import IPython
from IPython.display import display, clear_output
import time
import traceback
import math
from ultralytics import YOLO

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

transform = transforms.Compose([
    #transforms.Resize((300, 300)),
    transforms.ToTensor(),
])

class DetectionModel():
    def __init__(self, concept, backbone, model, weights, reference, color=None):
        self.concept = concept
        self.backbone = backbone
        self.model = model(weights) if concept=="YOLO" else model(weights=weights.DEFAULT)
        if concept!="YOLO":
            self.model.to(device)
            self.model.eval()
        self.weights = weights if concept=="YOLO" else weights.DEFAULT
        self.reference = reference
        self.color = color
        self.detections = []

    def addDetection(self, detection):
        self.detections.append(detection)

    def getDetections(self):
        return self.detections

    def clearDetections(self):
        self.detections = []

    def detectionsCount(self):
        return len(self.detections)

    def inferencesCount(self):
        length = 0
        addedFrames = []
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    addedFrames.append(self.detections[i].frame)
        return len(addedFrames)

    def getAveragePrecision(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].precision
            avg = sum/len(self.detections)
        return  avg.item() if torch.is_tensor(avg) else avg

    def getMaxPrecision(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].precision:
                max=self.detections[i].precision
        return max.item() if torch.is_tensor(max) else max

    def getMinPrecision(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].precision:
                min=self.detections[i].precision
        return 0.0 if min==9999999.99 else min.item() if torch.is_tensor(min) else min

    def getAverageSpeed(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].speed
            avg = sum/len(self.detections)
        return avg

    def getMaxSpeed(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].speed:
                max=self.detections[i].speed
        return max

    def getMinSpeed(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].speed:
                min=self.detections[i].speed
        return 0.0 if min==9999999.99 else min

    def getAverageInferenceSpeed(self):
        sum=0.0
        avg=0.0
        addedFrames=[]
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    sum+=self.detections[i].speed
                    addedFrames.append(self.detections[i].frame)
            avg = sum/len(addedFrames) if len(addedFrames)>0 else 0.0
        return avg

    def getMaxInferenceSpeed(self):
        max=0.0
        addedFrames=[]
        for i in range(len(self.detections)):
            if max<self.detections[i].speed and self.detections[i].frame not in addedFrames:
                max=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return max

    def getMinInferenceSpeed(self):
        min=9999999.99
        addedFrames=[]
        for i in range(len(self.detections)):
            if min>self.detections[i].speed and self.detections[i].frame not in addedFrames:
                min=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return 0.0 if min==9999999.99 else min

    def filterPrecisionsBelow(self, minPrecision):
        newDetections = []
        for i in range(len(self.detections)):
            if self.detections[i].precision>=minPrecision:
                newDetections.append(self.detections[i])
        self.detections = newDetections

    def printResults(self):
        print("Model:", self.reference,
              "\nBest Precision:", self.getMaxPrecision(),
              "%\nAverage Precision:", self.getAveragePrecision(),
              "%\nWorst Precision:", self.getMinPrecision(),
              "%\nBest Speed:", self.getMinSpeed(),
              "seconds\nAverage Speed:", self.getAverageSpeed(),
              "seconds\nWorst Speed:", self.getMaxSpeed(),
              "seconds\nBest Inference Speed:", self.getMinInferenceSpeed(),
              "seconds\nAverage Inference Speed:", self.getAverageInferenceSpeed(),
              "seconds\nWorst Inference Speed:", self.getMaxInferenceSpeed(),
              "seconds\nTotal Detections:", self.detectionsCount(),
              "\nTotal Inferences:", self.inferencesCount(),
              "\n\n")

class DetectionResult():
    def __init__(self, model, label, precision, speed, device=None, source=None, frame=None, boundingBox=None, details=None):
        self.model = model
        self.label = label
        self.precision = precision.item() if torch.is_tensor(precision) else precision
        self.speed = speed.item() if torch.is_tensor(speed) else speed
        self.device = device
        self.source = source
        self.frame = frame
        self.boundingBox = boundingBox
        self.details = details

target_classes = ["truck", "bus", "car", "train", "bicycle"]


VEHICLE_DETECTION_MODELS = [
    DetectionModel("FCOS", "resnet50_fpn", fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights, "fcos_resnet", (255,0,0)),
    DetectionModel("RetinaNet", "resnet50_fpn", retinanet_resnet50_fpn, RetinaNet_ResNet50_FPN_Weights, "retinanet_resnet", (255,255,0)),
    DetectionModel("RetinaNet", "resnet50_fpn_v2", retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights, "retinanet_resnet_v2", (0,234,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn", fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights, "faster_rcnn_resnet", (170,0,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn_v2", fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights, "faster_rcnn_resnet_v2", (255,127,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_fpn", fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights, "faster_rcnn_mobilenet_v3", (191,255,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_320_fpn", fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights, "faster_rcnn_mobilenet_v3_320", (0,149,255)),
    DetectionModel("MaskRCNN", "resnet50_fpn", maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights, "mask_rcnn_resnet", (255,0,170)),
    DetectionModel("MaskRCNN", "resnet50_fpn_v2", maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights, "mask_rcnn_resnet_v2", (255,212,0)),
    DetectionModel("SSD300", "vgg16", ssd300_vgg16, SSD300_VGG16_Weights, "ssd_vgg16", (106,255,0)),
    DetectionModel("SSDLite320", "mobilenet_v3_large", ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights, "ssd_mobilenet_v3", (0,64,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5nu.pt', "yolov5nu", (237,185,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5su.pt', "yolov5su", (185,215,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5mu.pt', "yolov5mu", (231,233,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5lu.pt', "yolov5lu", (220,185,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5xu.pt', "yolov5xu", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5n6u.pt', "yolov5n6u", (143,35,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5s6u.pt', "yolov5s6u", (35,98,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5m6u.pt', "yolov5m6u", (143,106,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5l6u.pt', "yolov5l6u", (107,35,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5x6u.pt', "yolov5x6u", (79,143,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8n.pt', "yolov8n", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8s.pt', "yolov8s", (115,115,115)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8m.pt', "yolov8m", (204,204,204)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8l.pt', "yolov8l", (255,0,0)), # cycle restarts
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8x.pt', "yolov8x", (255,255,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9c.pt', "yolov9c", (0,234,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9e.pt', "yolov9e", (170,0,255))#,
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10n.pt', "yolov10n", (185,237,224)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10s.pt', "yolov10s", (115,115,115)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10m.pt', "yolov10m", (204,204,204)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10b.pt', "yolov10b", (204,204,204)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10l.pt', "yolov10l", (255,0,0)), # cycle restarts
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10x.pt', "yolov10x", (255,255,0))
]

def process(frame, frameIndex=-1, source=None):
    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        processed_frame = frame.copy()

        if currentModel.concept=="YOLO":
            start_time = time.time()
            results = currentModel.model(processed_frame, agnostic_nms=True, verbose=False, device=device)[0]
            processing_time = time.time() - start_time
            for result in results:
                detection_count = result.boxes.shape[0]
                for j in range(detection_count):
                    cls = int(result.boxes.cls[j].item())
                    name = result.names[cls]
                    confidence = float(result.boxes.conf[j].item())
                    bounding_box = result.boxes.xyxy[j].cpu().numpy()
                    if name in target_classes and confidence>0.5:
                        x1, y1, x2, y2 = [int(x) for x in bounding_box]
                        # Ensure x1 < x2 and y1 < y2
                        x1, x2 = min(x1, x2), max(x1, x2)
                        y1, y2 = min(y1, y2), max(y1, y2)
                        label = '{} ({:.2f}%) ({:.3f}s) {}'.format(name.upper(), confidence*100, processing_time, currentModel.reference)
                        print(label)
                        VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, name, confidence*100, processing_time, device, source, frameIndex, bounding_box, label))
                        cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                        cv2.putText(processed_frame, label, (x1+10, y1+25), 0, 0.8, currentModel.color, 2)
        else:
            # Convert frame to RGB and PIL Image, then apply transformation
            frame_rgb = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            image = transform(pil_image).unsqueeze(0).to(device)

            # Object detection
            start_time = time.time()
            with torch.no_grad():
                predictions = currentModel.model(image)

            # Processing time
            processing_time = time.time() - start_time

            # Visualization
            scores = predictions[0]['scores']
            boxes = predictions[0]['boxes']
            labels = predictions[0]['labels']
            confidence_threshold = 0.5

            for score, box, label in zip(scores, boxes, labels):
                # and score > confidence_threshold
                if currentModel.weights.meta["categories"][label.item()] in target_classes and score > confidence_threshold:
                    x1, y1, x2, y2 = map(int, box)
                    label_text = f'{currentModel.weights.meta["categories"][label.item()].upper()} ({score*100:.2f}%) ({processing_time:.3f}s) {currentModel.reference}'
                    print(label_text)
                    VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, currentModel.weights.meta["categories"][label.item()], float(score.item())*100, processing_time, device, source, frameIndex, box, label_text))
                    cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                    cv2.putText(processed_frame, label_text, (x1, y1+20), 0, 0.8, currentModel.color, 2)

        # img = Image.fromarray(processed_frame, 'RGB')
        # display(img)
        clear_output(wait=True)
    return frame

def handleFrame(frame, frameIndex=-1, source=None):
    processed_frame = process(frame,frameIndex,source)

def handleRelease():
    cam.release()
    print("Source released.")

    print(device)
    print(source)

    BestPrecision=None
    BestAvgPrecision=None
    WorstPrecision=None
    BestSpeed=None
    BestAvgSpeed=None
    WorstSpeed=None

    Models=[]
    BestPrecisions=[]
    AvgPrecisions=[]
    WorstPrecisions=[]
    BestSpeeds=[]
    AvgSpeeds=[]
    WorstSpeeds=[]
    BestInferenceSpeeds=[]
    AvgInferenceSpeeds=[]
    WorstInferenceSpeeds=[]
    TotalDetectionsArr=[]
    TotalInferencesArr=[]

    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        # currentModel.printResults()

        Models.append(currentModel.reference)
        BestPrecisions.append(currentModel.getMaxPrecision())
        AvgPrecisions.append(currentModel.getAveragePrecision())
        WorstPrecisions.append(currentModel.getMinPrecision())
        BestSpeeds.append(currentModel.getMinSpeed())
        AvgSpeeds.append(currentModel.getAverageSpeed())
        WorstSpeeds.append(currentModel.getMaxSpeed())
        BestInferenceSpeeds.append(currentModel.getMinInferenceSpeed())
        AvgInferenceSpeeds.append(currentModel.getAverageInferenceSpeed())
        WorstInferenceSpeeds.append(currentModel.getMaxInferenceSpeed())
        TotalDetectionsArr.append(currentModel.detectionsCount())
        TotalInferencesArr.append(currentModel.inferencesCount())

        if BestPrecision is None or BestPrecision.getMaxPrecision()<=currentModel.getMaxPrecision():
            BestPrecision = currentModel
        if BestAvgPrecision is None or BestAvgPrecision.getAveragePrecision()<=currentModel.getAveragePrecision():
            BestAvgPrecision = currentModel
        if WorstPrecision is None or WorstPrecision.getMinPrecision()>=currentModel.getMinPrecision():
            WorstPrecision = currentModel
        if BestSpeed is None or BestSpeed.getMinSpeed()>=currentModel.getMinSpeed():
            BestSpeed = currentModel
        if BestAvgSpeed is None or BestAvgSpeed.getAverageSpeed()>=currentModel.getAverageSpeed():
            BestAvgSpeed = currentModel
        if WorstSpeed is None or WorstSpeed.getMaxSpeed()<=currentModel.getMaxSpeed():
            WorstSpeed = currentModel

    if BestPrecision is not None:
        print("Best Precision:")
        BestPrecision.printResults()
    if BestAvgPrecision is not None:
        print("Best Average Precision:")
        BestAvgPrecision.printResults()
    if WorstPrecision is not None:
        print("Worst Precision:")
        WorstPrecision.printResults()
    if BestSpeed is not None:
        print("Best Speed:")
        BestSpeed.printResults()
    if BestAvgSpeed is not None:
        print("Best Average Speed:")
        BestAvgSpeed.printResults()
    if WorstSpeed is not None:
        print("Worst Speed:")
        WorstSpeed.printResults()

    print("models=",Models)
    print("best_precisions=",BestPrecisions)
    print("avg_precisions=",AvgPrecisions)
    print("worst_precisions=",WorstPrecisions)
    print("best_speeds=",BestSpeeds)
    print("avg_speeds=",AvgSpeeds)
    print("worst_speeds=",WorstSpeeds)
    print("best_inference_speeds=",BestInferenceSpeeds)
    print("avg_inference_speeds=",AvgInferenceSpeeds)
    print("worst_inference_speeds=",WorstInferenceSpeeds)
    print("total_detections=",TotalDetectionsArr)
    print("total_inferences=",TotalInferencesArr)

inputType="video"
source = "/data/video/MyDrive/2034115-hd_1920_1080_30fps.mp4" if inputType=="video" else 0
cam = cv2.VideoCapture(source)
display_handle=display(None, display_id=True)
every = 30

start = 0 if source!=0 else -1
end = int(cam.get(cv2.CAP_PROP_FRAME_COUNT)) if source!=0 else -1
if source!=0: cam.set(1, start)

frameCount=start
while_safety=0
saved_count=0

if not cam.isOpened(): print("Error: Could not open source.")
else:
    try:
        while True if end<0 else frameCount<end:
            _, frame = cam.read()
            if frame is None:
                if source!=0:
                    if while_safety > 2000: break
                    while_safety += 1
                    continue
                else:
                    print("Error: Could not capture frame.")
                    cam.release()
                    break

            if every>0:
                if (frameCount+1)%math.floor(every) == 0:
                    while_safety = 0
                    handleFrame(frame, frameCount, source)
            else:
                while_safety=0
                handleFrame(frame)
            frameCount += 1
        handleRelease()
    except KeyboardInterrupt:
        handleRelease()
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Source could not be released.")
        except:
            pass
        traceback.print_exc()


models= ['fcos_resnet', 'retinanet_resnet', 'retinanet_resnet_v2', 'faster_rcnn_resnet', 'faster_rcnn_resnet_v2', 'faster_rcnn_mobilenet_v3', 'faster_rcnn_mobilenet_v3_320', 'mask_rcnn_resnet', 'mask_rcnn_resnet_v2', 'ssd_vgg16', 'ssd_mobilenet_v3', 'yolov5nu', 'yolov5su', 'yolov5mu', 'yolov5lu', 'yolov5xu', 'yolov5n6u', 'yolov5s6u', 'yolov5m6u', 'yolov5l6u', 'yolov5x6u', 'yolov8n', 'yolov8s', 'yolov8m', 'yolov8l', 'yolov8x', 'yolov9c', 'yolov9e']
best_precisions= [84.83719825744629, 91.69005751609802, 95.44172286987305, 99.92283582687378, 99.9826729297638, 99.82041120529175, 99.8579740524292, 99.87071752548218, 99.93306398391724, 99.47911500930786, 99.57059025764465, 93.70632767677307, 94.43681836128235, 95.35425901412964, 95.55730819702148, 95.68663239479065, 96.31909132003784, 94.22449469566345, 95.38388252258301, 96.37545347213745, 96.30266427993774, 94.4012463092804, 95.29010653495789, 95.23689150810242, 95.42948007583618, 96.0139274597168, 96.24937772750854, 96.66422009468079]
avg_precisions= [65.88353116597447, 71.22543070449338, 73.83463682156604, 86.81564299159226, 88.95875359403676, 88.03691740693718, 85.20919003430203, 87.22850633754793, 89.61977848930964, 84.4685799545712, 83.99542532068618, 76.06018336725907, 77.5317103912433, 78.89031929020979, 81.0616362649341, 82.0430493196555, 79.49501279564132, 82.74618758749, 83.24965186965414, 83.5373806904574, 84.09532246230536, 76.09085207400115, 81.14048236608505, 80.57317768511751, 81.92332851457166, 83.38619881385081, 83.3097627086024, 83.26056412363474]
worst_precisions= [50.05142688751221, 50.09543299674988, 50.03858208656311, 50.51295757293701, 50.544363260269165, 50.14258027076721, 50.13910531997681, 50.08808374404907, 51.55285596847534, 51.73056721687317, 50.462132692337036, 50.02012252807617, 50.22692680358887, 50.904226303100586, 50.430405139923096, 50.85350275039673, 50.22304058074951, 50.04777908325195, 52.48023271560669, 51.146399974823, 50.04284381866455, 51.3719916343689, 51.98100805282593, 50.360554456710815, 50.063395500183105, 50.028371810913086, 52.067744731903076, 50.01170635223389]
best_speeds= [0.028139352798461914, 0.02793741226196289, 0.026671648025512695, 0.02678203582763672, 0.029085159301757812, 0.023817062377929688, 0.021103382110595703, 0.08254361152648926, 0.08128786087036133, 0.02750706672668457, 0.03693556785583496, 0.013080358505249023, 0.01242208480834961, 0.014293193817138672, 0.016450166702270508, 0.01877617835998535, 0.019731760025024414, 0.019443750381469727, 0.022606611251831055, 0.024990558624267578, 0.028153657913208008, 0.011769294738769531, 0.011424064636230469, 0.013297796249389648, 0.01572275161743164, 0.01611018180847168, 0.020073652267456055, 0.03289008140563965]
avg_speeds= [0.030792944771902902, 0.029292076954002544, 0.028318104585765096, 0.02886300182661484, 0.03123413059031328, 0.02576814129434783, 0.02242687750144823, 0.08974299368718548, 0.09169944581531343, 0.02925159955265546, 0.05648271834596674, 0.018707769017823984, 0.019829779863357544, 0.03103177766410672, 0.041884237112000934, 0.05792619591265653, 0.02879849643934341, 0.031218556545240462, 0.043611033138258634, 0.06006063203342625, 0.08457339957169409, 0.01735735531919491, 0.021718995571136476, 0.027438315575432883, 0.03602433312046635, 0.04453092115419405, 0.038911155841317595, 0.06352777185693251]
worst_speeds= [0.04837989807128906, 0.033477067947387695, 0.033692359924316406, 0.03216052055358887, 0.03963828086853027, 0.04023432731628418, 0.02700519561767578, 0.09676146507263184, 0.10283088684082031, 0.031893014907836914, 0.06261897087097168, 0.14199042320251465, 0.20679640769958496, 0.3692808151245117, 0.642627477645874, 1.030693769454956, 0.19211077690124512, 0.296842098236084, 0.5695502758026123, 1.0269184112548828, 1.6599688529968262, 0.12665724754333496, 0.20059609413146973, 0.3542494773864746, 0.5346677303314209, 0.7629315853118896, 0.465618371963501, 0.9071543216705322]
best_inference_speeds= [0.028139352798461914, 0.02793741226196289, 0.026671648025512695, 0.02678203582763672, 0.029085159301757812, 0.023817062377929688, 0.021103382110595703, 0.08254361152648926, 0.08128786087036133, 0.02750706672668457, 0.03693556785583496, 0.013080358505249023, 0.01242208480834961, 0.014293193817138672, 0.016450166702270508, 0.01877617835998535, 0.019731760025024414, 0.019443750381469727, 0.022606611251831055, 0.024990558624267578, 0.028153657913208008, 0.011769294738769531, 0.011424064636230469, 0.013297796249389648, 0.01572275161743164, 0.01611018180847168, 0.020073652267456055, 0.03289008140563965]
avg_inference_speeds= [0.03095675366265433, 0.0292507495198931, 0.028286235673086985, 0.028882827077593123, 0.031283940587724955, 0.025713239397321428, 0.02241040127617972, 0.08988874299185616, 0.09153856549944196, 0.02925097942352295, 0.05628053631101336, 0.018832453659602573, 0.020679388727460588, 0.029326226030077254, 0.04096086536134992, 0.05827803271157401, 0.027785701411111013, 0.03118344715663365, 0.044170847960880825, 0.06298927749906268, 0.08825049230030604, 0.017195097037724087, 0.019990384578704834, 0.02738920279911586, 0.03581484726497105, 0.044286515031542094, 0.038396639483315606, 0.06770493303026472]
worst_inference_speeds= [0.04837989807128906, 0.033477067947387695, 0.033692359924316406, 0.03216052055358887, 0.03963828086853027, 0.04023432731628418, 0.02700519561767578, 0.09676146507263184, 0.10283088684082031, 0.031893014907836914, 0.06261897087097168, 0.14199042320251465, 0.20679640769958496, 0.3692808151245117, 0.642627477645874, 1.030693769454956, 0.19211077690124512, 0.296842098236084, 0.5695502758026123, 1.0269184112548828, 1.6599688529968262, 0.12665724754333496, 0.20059609413146973, 0.3542494773864746, 0.5346677303314209, 0.7629315853118896, 0.465618371963501, 0.9071543216705322]
total_detections= [280, 233, 211, 299, 319, 232, 169, 307, 315, 99, 94, 142, 192, 196, 215, 226, 168, 223, 231, 244, 239, 161, 200, 223, 222, 222, 217, 226]
total_inferences= [28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28]


import cv2
import torch
from torchvision.models.detection import *
from torchvision.io.image import read_image
from torchvision.utils import draw_bounding_boxes
from torchvision.transforms.functional import to_pil_image
import torchvision.transforms as transforms
from PIL import Image
import IPython
from IPython.display import display, clear_output
import time
import traceback
import math
from ultralytics import YOLO

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

transform = transforms.Compose([
    #transforms.Resize((300, 300)),
    transforms.ToTensor(),
])

class DetectionModel():
    def __init__(self, concept, backbone, model, weights, reference, color=None):
        self.concept = concept
        self.backbone = backbone
        self.model = model(weights) if concept=="YOLO" else model(weights=weights.DEFAULT)
        if concept!="YOLO":
            self.model.to(device)
            self.model.eval()
        self.weights = weights if concept=="YOLO" else weights.DEFAULT
        self.reference = reference
        self.color = color
        self.detections = []

    def addDetection(self, detection):
        self.detections.append(detection)

    def getDetections(self):
        return self.detections

    def clearDetections(self):
        self.detections = []

    def detectionsCount(self):
        return len(self.detections)

    def inferencesCount(self):
        length = 0
        addedFrames = []
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    addedFrames.append(self.detections[i].frame)
        return len(addedFrames)

    def getAveragePrecision(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].precision
            avg = sum/len(self.detections)
        return  avg.item() if torch.is_tensor(avg) else avg

    def getMaxPrecision(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].precision:
                max=self.detections[i].precision
        return max.item() if torch.is_tensor(max) else max

    def getMinPrecision(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].precision:
                min=self.detections[i].precision
        return 0.0 if min==9999999.99 else min.item() if torch.is_tensor(min) else min

    def getAverageSpeed(self):
        sum=0.0
        avg=0.0
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                sum+=self.detections[i].speed
            avg = sum/len(self.detections)
        return avg

    def getMaxSpeed(self):
        max=0.0
        for i in range(len(self.detections)):
            if max<self.detections[i].speed:
                max=self.detections[i].speed
        return max

    def getMinSpeed(self):
        min=9999999.99
        for i in range(len(self.detections)):
            if min>self.detections[i].speed:
                min=self.detections[i].speed
        return 0.0 if min==9999999.99 else min

    def getAverageInferenceSpeed(self):
        sum=0.0
        avg=0.0
        addedFrames=[]
        if len(self.detections)>0:
            for i in range(len(self.detections)):
                if self.detections[i].frame not in addedFrames:
                    sum+=self.detections[i].speed
                    addedFrames.append(self.detections[i].frame)
            avg = sum/len(addedFrames) if len(addedFrames)>0 else 0.0
        return avg

    def getMaxInferenceSpeed(self):
        max=0.0
        addedFrames=[]
        for i in range(len(self.detections)):
            if max<self.detections[i].speed and self.detections[i].frame not in addedFrames:
                max=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return max

    def getMinInferenceSpeed(self):
        min=9999999.99
        addedFrames=[]
        for i in range(len(self.detections)):
            if min>self.detections[i].speed and self.detections[i].frame not in addedFrames:
                min=self.detections[i].speed
                addedFrames.append(self.detections[i].frame)
        return 0.0 if min==9999999.99 else min

    def filterPrecisionsBelow(self, minPrecision):
        newDetections = []
        for i in range(len(self.detections)):
            if self.detections[i].precision>=minPrecision:
                newDetections.append(self.detections[i])
        self.detections = newDetections

    def printResults(self):
        print("Model:", self.reference,
              "\nBest Precision:", self.getMaxPrecision(),
              "%\nAverage Precision:", self.getAveragePrecision(),
              "%\nWorst Precision:", self.getMinPrecision(),
              "%\nBest Speed:", self.getMinSpeed(),
              "seconds\nAverage Speed:", self.getAverageSpeed(),
              "seconds\nWorst Speed:", self.getMaxSpeed(),
              "seconds\nBest Inference Speed:", self.getMinInferenceSpeed(),
              "seconds\nAverage Inference Speed:", self.getAverageInferenceSpeed(),
              "seconds\nWorst Inference Speed:", self.getMaxInferenceSpeed(),
              "seconds\nTotal Detections:", self.detectionsCount(),
              "\nTotal Inferences:", self.inferencesCount(),
              "\n\n")

class DetectionResult():
    def __init__(self, model, label, precision, speed, device=None, source=None, frame=None, boundingBox=None, details=None):
        self.model = model
        self.label = label
        self.precision = precision.item() if torch.is_tensor(precision) else precision
        self.speed = speed.item() if torch.is_tensor(speed) else speed
        self.device = device
        self.source = source
        self.frame = frame
        self.boundingBox = boundingBox
        self.details = details

target_classes = ["truck", "bus", "car", "train", "bicycle"]


VEHICLE_DETECTION_MODELS = [
    DetectionModel("FCOS", "resnet50_fpn", fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights, "fcos_resnet", (255,0,0)),
    DetectionModel("RetinaNet", "resnet50_fpn", retinanet_resnet50_fpn, RetinaNet_ResNet50_FPN_Weights, "retinanet_resnet", (255,255,0)),
    DetectionModel("RetinaNet", "resnet50_fpn_v2", retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights, "retinanet_resnet_v2", (0,234,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn", fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights, "faster_rcnn_resnet", (170,0,255)),
    DetectionModel("FasterRCNN", "resnet50_fpn_v2", fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights, "faster_rcnn_resnet_v2", (255,127,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_fpn", fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights, "faster_rcnn_mobilenet_v3", (191,255,0)),
    DetectionModel("FasterRCNN", "mobilenet_v3_large_320_fpn", fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights, "faster_rcnn_mobilenet_v3_320", (0,149,255)),
    DetectionModel("MaskRCNN", "resnet50_fpn", maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights, "mask_rcnn_resnet", (255,0,170)),
    DetectionModel("MaskRCNN", "resnet50_fpn_v2", maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights, "mask_rcnn_resnet_v2", (255,212,0)),
    DetectionModel("SSD300", "vgg16", ssd300_vgg16, SSD300_VGG16_Weights, "ssd_vgg16", (106,255,0)),
    DetectionModel("SSDLite320", "mobilenet_v3_large", ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights, "ssd_mobilenet_v3", (0,64,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5nu.pt', "yolov5nu", (237,185,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5su.pt', "yolov5su", (185,215,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5mu.pt', "yolov5mu", (231,233,185)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5lu.pt', "yolov5lu", (220,185,237)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5xu.pt', "yolov5xu", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5n6u.pt', "yolov5n6u", (143,35,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5s6u.pt', "yolov5s6u", (35,98,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5m6u.pt', "yolov5m6u", (143,106,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5l6u.pt', "yolov5l6u", (107,35,143)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov5x6u.pt', "yolov5x6u", (79,143,35)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8n.pt', "yolov8n", (185,237,224)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8s.pt', "yolov8s", (115,115,115)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8m.pt', "yolov8m", (204,204,204)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8l.pt', "yolov8l", (255,0,0)), # cycle restarts
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov8x.pt', "yolov8x", (255,255,0)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9c.pt', "yolov9c", (0,234,255)),
    DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov9e.pt', "yolov9e", (170,0,255))#,
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10n.pt', "yolov10n", (185,237,224)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10s.pt', "yolov10s", (115,115,115)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10m.pt', "yolov10m", (204,204,204)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10b.pt', "yolov10b", (204,204,204)),
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10l.pt', "yolov10l", (255,0,0)), # cycle restarts
    # DetectionModel("YOLO", "csp_darknet53", YOLO, 'yolov10x.pt', "yolov10x", (255,255,0))
]

def process(frame, frameIndex=-1, source=None):
    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        processed_frame = frame.copy()

        if currentModel.concept=="YOLO":
            start_time = time.time()
            results = currentModel.model(processed_frame, agnostic_nms=True, verbose=False, device=device)[0]
            processing_time = time.time() - start_time
            for result in results:
                detection_count = result.boxes.shape[0]
                for j in range(detection_count):
                    cls = int(result.boxes.cls[j].item())
                    name = result.names[cls]
                    confidence = float(result.boxes.conf[j].item())
                    bounding_box = result.boxes.xyxy[j].cpu().numpy()
                    if name in target_classes and confidence>0.5:
                        x1, y1, x2, y2 = [int(x) for x in bounding_box]
                        # Ensure x1 < x2 and y1 < y2
                        x1, x2 = min(x1, x2), max(x1, x2)
                        y1, y2 = min(y1, y2), max(y1, y2)
                        label = '{} ({:.2f}%) ({:.3f}s) {}'.format(name.upper(), confidence*100, processing_time, currentModel.reference)
                        print(label)
                        VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, name, confidence*100, processing_time, device, source, frameIndex, bounding_box, label))
                        cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                        cv2.putText(processed_frame, label, (x1+10, y1+25), 0, 0.8, currentModel.color, 2)
        else:
            # Convert frame to RGB and PIL Image, then apply transformation
            frame_rgb = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            image = transform(pil_image).unsqueeze(0).to(device)

            # Object detection
            start_time = time.time()
            with torch.no_grad():
                predictions = currentModel.model(image)

            # Processing time
            processing_time = time.time() - start_time

            # Visualization
            scores = predictions[0]['scores']
            boxes = predictions[0]['boxes']
            labels = predictions[0]['labels']
            confidence_threshold = 0.5

            for score, box, label in zip(scores, boxes, labels):
                # and score > confidence_threshold
                if currentModel.weights.meta["categories"][label.item()] in target_classes and score > confidence_threshold:
                    x1, y1, x2, y2 = map(int, box)
                    label_text = f'{currentModel.weights.meta["categories"][label.item()].upper()} ({score*100:.2f}%) ({processing_time:.3f}s) {currentModel.reference}'
                    print(label_text)
                    VEHICLE_DETECTION_MODELS[i].addDetection(DetectionResult(currentModel.reference, currentModel.weights.meta["categories"][label.item()], float(score.item())*100, processing_time, device, source, frameIndex, box, label_text))
                    cv2.rectangle(processed_frame, (x1, y1), (x2, y2), currentModel.color, 2)
                    cv2.putText(processed_frame, label_text, (x1, y1+20), 0, 0.8, currentModel.color, 2)

        # img = Image.fromarray(processed_frame, 'RGB')
        # display(img)
        clear_output(wait=True)
    return frame

def handleFrame(frame, frameIndex=-1, source=None):
    processed_frame = process(frame,frameIndex,source)

def handleRelease():
    cam.release()
    print("Source released.")

    print(device)
    print(source)

    BestPrecision=None
    BestAvgPrecision=None
    WorstPrecision=None
    BestSpeed=None
    BestAvgSpeed=None
    WorstSpeed=None

    Models=[]
    BestPrecisions=[]
    AvgPrecisions=[]
    WorstPrecisions=[]
    BestSpeeds=[]
    AvgSpeeds=[]
    WorstSpeeds=[]
    BestInferenceSpeeds=[]
    AvgInferenceSpeeds=[]
    WorstInferenceSpeeds=[]
    TotalDetectionsArr=[]
    TotalInferencesArr=[]

    for i in range(len(VEHICLE_DETECTION_MODELS)):
        currentModel = VEHICLE_DETECTION_MODELS[i]
        # currentModel.printResults()

        Models.append(currentModel.reference)
        BestPrecisions.append(currentModel.getMaxPrecision())
        AvgPrecisions.append(currentModel.getAveragePrecision())
        WorstPrecisions.append(currentModel.getMinPrecision())
        BestSpeeds.append(currentModel.getMinSpeed())
        AvgSpeeds.append(currentModel.getAverageSpeed())
        WorstSpeeds.append(currentModel.getMaxSpeed())
        BestInferenceSpeeds.append(currentModel.getMinInferenceSpeed())
        AvgInferenceSpeeds.append(currentModel.getAverageInferenceSpeed())
        WorstInferenceSpeeds.append(currentModel.getMaxInferenceSpeed())
        TotalDetectionsArr.append(currentModel.detectionsCount())
        TotalInferencesArr.append(currentModel.inferencesCount())

        if BestPrecision is None or BestPrecision.getMaxPrecision()<=currentModel.getMaxPrecision():
            BestPrecision = currentModel
        if BestAvgPrecision is None or BestAvgPrecision.getAveragePrecision()<=currentModel.getAveragePrecision():
            BestAvgPrecision = currentModel
        if WorstPrecision is None or WorstPrecision.getMinPrecision()>=currentModel.getMinPrecision():
            WorstPrecision = currentModel
        if BestSpeed is None or BestSpeed.getMinSpeed()>=currentModel.getMinSpeed():
            BestSpeed = currentModel
        if BestAvgSpeed is None or BestAvgSpeed.getAverageSpeed()>=currentModel.getAverageSpeed():
            BestAvgSpeed = currentModel
        if WorstSpeed is None or WorstSpeed.getMaxSpeed()<=currentModel.getMaxSpeed():
            WorstSpeed = currentModel

    if BestPrecision is not None:
        print("Best Precision:")
        BestPrecision.printResults()
    if BestAvgPrecision is not None:
        print("Best Average Precision:")
        BestAvgPrecision.printResults()
    if WorstPrecision is not None:
        print("Worst Precision:")
        WorstPrecision.printResults()
    if BestSpeed is not None:
        print("Best Speed:")
        BestSpeed.printResults()
    if BestAvgSpeed is not None:
        print("Best Average Speed:")
        BestAvgSpeed.printResults()
    if WorstSpeed is not None:
        print("Worst Speed:")
        WorstSpeed.printResults()

    print("models=",Models)
    print("best_precisions=",BestPrecisions)
    print("avg_precisions=",AvgPrecisions)
    print("worst_precisions=",WorstPrecisions)
    print("best_speeds=",BestSpeeds)
    print("avg_speeds=",AvgSpeeds)
    print("worst_speeds=",WorstSpeeds)
    print("best_inference_speeds=",BestInferenceSpeeds)
    print("avg_inference_speeds=",AvgInferenceSpeeds)
    print("worst_inference_speeds=",WorstInferenceSpeeds)
    print("total_detections=",TotalDetectionsArr)
    print("total_inferences=",TotalInferencesArr)

inputType="video"
source = "/data/video/MyDrive/2034115-hd_1920_1080_30fps.mp4" if inputType=="video" else 0
cam = cv2.VideoCapture(source)
display_handle=display(None, display_id=True)
every = 60

start = 0 if source!=0 else -1
end = int(cam.get(cv2.CAP_PROP_FRAME_COUNT)) if source!=0 else -1
if source!=0: cam.set(1, start)

frameCount=start
while_safety=0
saved_count=0

if not cam.isOpened(): print("Error: Could not open source.")
else:
    try:
        while True if end<0 else frameCount<end:
            _, frame = cam.read()
            if frame is None:
                if source!=0:
                    if while_safety > 2000: break
                    while_safety += 1
                    continue
                else:
                    print("Error: Could not capture frame.")
                    cam.release()
                    break

            if every>0:
                if (frameCount+1)%math.floor(every) == 0:
                    while_safety = 0
                    handleFrame(frame, frameCount, source)
            else:
                while_safety=0
                handleFrame(frame)
            frameCount += 1
        handleRelease()
    except KeyboardInterrupt:
        handleRelease()
    except:
        print("Unknown error.")
        try:
            cam.release()
            raise TypeError("Error: Source could not be released.")
        except:
            pass
        traceback.print_exc()


models= ['fcos_resnet', 'retinanet_resnet', 'retinanet_resnet_v2', 'faster_rcnn_resnet', 'faster_rcnn_resnet_v2', 'faster_rcnn_mobilenet_v3', 'faster_rcnn_mobilenet_v3_320', 'mask_rcnn_resnet', 'mask_rcnn_resnet_v2', 'ssd_vgg16', 'ssd_mobilenet_v3', 'yolov5nu', 'yolov5su', 'yolov5mu', 'yolov5lu', 'yolov5xu', 'yolov5n6u', 'yolov5s6u', 'yolov5m6u', 'yolov5l6u', 'yolov5x6u', 'yolov8n', 'yolov8s', 'yolov8m', 'yolov8l', 'yolov8x', 'yolov9c', 'yolov9e']
best_precisions= [84.58108305931091, 91.69005751609802, 93.27404499053955, 99.92283582687378, 99.9826729297638, 99.746835231781, 99.8579740524292, 99.87071752548218, 99.93306398391724, 99.46763515472412, 99.57059025764465, 93.70632767677307, 94.43681836128235, 95.35425901412964, 95.45791745185852, 95.68663239479065, 96.31909132003784, 93.82559657096863, 95.38388252258301, 96.12937569618225, 96.30266427993774, 93.96243095397949, 95.29010653495789, 95.23689150810242, 95.31776905059814, 95.91649174690247, 96.24937772750854, 96.36507630348206]
avg_precisions= [65.94888042520594, 70.84219549013221, 73.63541686888969, 86.41327571548871, 89.50482164659807, 88.75137303782775, 85.42400418993938, 86.61803800564307, 89.51267269766255, 83.99975249107848, 85.83257946857186, 76.09982723775117, 76.87920746050384, 77.95007419340389, 81.48921184169436, 82.06955736333674, 79.4478557727955, 82.03686941753735, 82.69370680269988, 82.6965537227568, 83.2526321709156, 75.91727044847276, 79.98378987264151, 79.56331414835793, 82.28059504871015, 83.48321663999111, 83.05485491399412, 82.00701574484508]
worst_precisions= [50.05142688751221, 50.09543299674988, 50.03858208656311, 50.51295757293701, 50.88812708854675, 51.23204588890076, 50.13910531997681, 50.08808374404907, 52.36402750015259, 57.175421714782715, 51.42931342124939, 50.02012252807617, 50.22692680358887, 50.904226303100586, 50.430405139923096, 51.39314532279968, 50.22304058074951, 50.04777908325195, 52.77990698814392, 51.146399974823, 50.374335050582886, 51.3719916343689, 51.98100805282593, 50.82685947418213, 50.063395500183105, 50.349944829940796, 52.29911804199219, 50.39564371109009]
best_speeds= [0.028989076614379883, 0.027878522872924805, 0.02714705467224121, 0.027615785598754883, 0.029932260513305664, 0.023841142654418945, 0.0207369327545166, 0.07994198799133301, 0.08062243461608887, 0.02773594856262207, 0.03734111785888672, 0.012668132781982422, 0.012316703796386719, 0.014595985412597656, 0.0164639949798584, 0.0189816951751709, 0.019505739212036133, 0.019174814224243164, 0.022186994552612305, 0.024609088897705078, 0.028297901153564453, 0.011658430099487305, 0.011602640151977539, 0.013902425765991211, 0.015676498413085938, 0.015913963317871094, 0.020662784576416016, 0.033616065979003906]
avg_speeds= [0.02994090539437753, 0.028718127375063688, 0.02800295140483592, 0.028435567881436957, 0.030915841748637537, 0.026555369385575827, 0.021694769342261624, 0.08835809106950636, 0.08866258335720961, 0.028828509310458567, 0.056814304617948314, 0.024805842966273212, 0.027498435974121094, 0.040749866937853624, 0.05968838062101198, 0.09474650729786266, 0.03433092435201009, 0.03873439702120694, 0.05736940010734226, 0.09185296590210962, 0.13708267410596212, 0.021272153030207127, 0.02584852112664117, 0.03920389924730573, 0.05105270500536318, 0.07351381087971624, 0.0544803606139289, 0.10284267392074853]
worst_speeds= [0.03316545486450195, 0.030076026916503906, 0.029192209243774414, 0.030620813369750977, 0.03323030471801758, 0.04766201972961426, 0.023673295974731445, 0.0943453311920166, 0.09342241287231445, 0.030319690704345703, 0.06337404251098633, 0.1431872844696045, 0.20314836502075195, 0.36160778999328613, 0.6324999332427979, 1.0465197563171387, 0.19774580001831055, 0.3012254238128662, 0.5736570358276367, 1.021592378616333, 1.6408741474151611, 0.1276383399963379, 0.1976017951965332, 0.35574913024902344, 0.5399158000946045, 0.7690916061401367, 0.4608473777770996, 0.8885152339935303]
best_inference_speeds= [0.028989076614379883, 0.027878522872924805, 0.02714705467224121, 0.027615785598754883, 0.029932260513305664, 0.023841142654418945, 0.0207369327545166, 0.07994198799133301, 0.08062243461608887, 0.02773594856262207, 0.03734111785888672, 0.012668132781982422, 0.012316703796386719, 0.014595985412597656, 0.0164639949798584, 0.0189816951751709, 0.019505739212036133, 0.019174814224243164, 0.022186994552612305, 0.024609088897705078, 0.028297901153564453, 0.011658430099487305, 0.011602640151977539, 0.013902425765991211, 0.015676498413085938, 0.015913963317871094, 0.020662784576416016, 0.033616065979003906]
avg_inference_speeds= [0.03007432392665318, 0.028750709124973843, 0.027958972113473073, 0.02846508366721017, 0.03094135011945452, 0.02640465327671596, 0.02175108024052211, 0.08840204988207136, 0.08869186469486781, 0.0288250276020595, 0.05719876289367676, 0.02278123583112444, 0.027144057410103933, 0.04050593716757638, 0.06185223375047956, 0.09346190520695277, 0.033852338790893555, 0.0409167834690639, 0.06313211577279228, 0.09760088579995292, 0.14475216184343612, 0.02097947256905692, 0.025952390262058804, 0.0391876186643328, 0.05445521218436105, 0.0710125480379377, 0.05330819743020194, 0.09640421186174665]
worst_inference_speeds= [0.03316545486450195, 0.030076026916503906, 0.029192209243774414, 0.030620813369750977, 0.03323030471801758, 0.04766201972961426, 0.023673295974731445, 0.0943453311920166, 0.09342241287231445, 0.030319690704345703, 0.06337404251098633, 0.1431872844696045, 0.20314836502075195, 0.36160778999328613, 0.6324999332427979, 1.0465197563171387, 0.19774580001831055, 0.3012254238128662, 0.5736570358276367, 1.021592378616333, 1.6408741474151611, 0.1276383399963379, 0.1976017951965332, 0.35574913024902344, 0.5399158000946045, 0.7690916061401367, 0.4608473777770996, 0.8885152339935303]
total_detections= [135, 115, 101, 149, 155, 113, 83, 154, 157, 47, 43, 69, 95, 97, 103, 110, 81, 110, 115, 122, 120, 81, 99, 112, 108, 107, 108, 114]
total_inferences= [14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]





mapper = []
for i in range(len(models)):
    mapper.append([models[i], f'{avg_precisions[i]:.5f}', f'{avg_inference_speeds[i]:.5f}'])
sorted_list = sorted(mapper,key=lambda l:l[1], reverse=True)
print(sorted_list)



